{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EFFkWznYT-J7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1188d2a-ba10-4488-a8c9-396c04665033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-arango in /usr/local/lib/python3.11/dist-packages (8.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from python-arango) (2.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from python-arango) (2.32.3)\n",
            "Requirement already satisfied: requests_toolbelt in /usr/local/lib/python3.11/dist-packages (from python-arango) (1.0.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.11/dist-packages (from python-arango) (2.10.1)\n",
            "Requirement already satisfied: setuptools>=42 in /usr/local/lib/python3.11/dist-packages (from python-arango) (75.1.0)\n",
            "Requirement already satisfied: importlib_metadata>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from python-arango) (8.6.1)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.11/dist-packages (from python-arango) (24.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.7.1->python-arango) (3.21.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->python-arango) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->python-arango) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->python-arango) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.39)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages correctly\n",
        "import sys\n",
        "!{sys.executable} -m pip install python-arango networkx pandas gdown\n",
        "!{sys.executable} -m pip install langchain langchain-openai langchain-community openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "import networkx as nx\n",
        "from arango import ArangoClient\n",
        "# Update these langchain imports\n",
        "from langchain_openai import OpenAI  # Or use this\n",
        "# Alternatively, you might need to use:\n",
        "# from langchain_community.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import pandas as pd\n",
        "import json\n",
        "import gdown"
      ],
      "metadata": {
        "id": "p7B8F1cPUhfC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def install_and_import(package):\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "        print(f\"✅ {package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"✅ Successfully installed {package}\")\n",
        "\n",
        "# Install required packages\n",
        "required_packages = [\"python-arango\", \"networkx\", \"langchain\", \"gdown\", \"pandas\", \"openai\"]\n",
        "for package in required_packages:\n",
        "    install_and_import(package)\n",
        "\n",
        "# Note about cugraph\n",
        "print(\"Note: cugraph requires CUDA. If you don't have a GPU, we'll use NetworkX instead.\")\n",
        "try:\n",
        "    install_and_import(\"cugraph\")\n",
        "    use_cugraph = True\n",
        "except:\n",
        "    print(\"⚠️ Could not install cugraph. Using NetworkX for all graph operations.\")\n",
        "    use_cugraph = False\n",
        "\n",
        "# Import required libraries after installation\n",
        "import networkx as nx\n",
        "from arango import ArangoClient\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "import pandas as pd\n",
        "import json\n",
        "import gdown\n",
        "\n",
        "print(\"All dependencies imported successfully!\")\n"
      ],
      "metadata": {
        "id": "7DqL4k38Ulpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a15fdd-6fee-4efa-edd0-181d1dac0f0a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing python-arango...\n",
            "✅ Successfully installed python-arango\n",
            "✅ networkx is already installed\n",
            "✅ langchain is already installed\n",
            "✅ gdown is already installed\n",
            "✅ pandas is already installed\n",
            "✅ openai is already installed\n",
            "Note: cugraph requires CUDA. If you don't have a GPU, we'll use NetworkX instead.\n",
            "Installing cugraph...\n",
            "⚠️ Could not install cugraph. Using NetworkX for all graph operations.\n",
            "All dependencies imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Setup & Dependencies\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "# Function to install and import a package\n",
        "def install_and_import(package):\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "        print(f\"✅ {package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"✅ Successfully installed {package}\")\n",
        "\n",
        "# Install required packages\n",
        "required_packages = [\"python-arango\", \"networkx\", \"pandas\", \"gdown\", \"requests\", \"tqdm\"]\n",
        "# Update langchain packages to use the newer structure\n",
        "langchain_packages = [\"langchain\", \"langchain-openai\", \"langchain-community\"]\n",
        "\n",
        "for package in required_packages:\n",
        "    install_and_import(package)\n",
        "for package in langchain_packages:\n",
        "    install_and_import(package)\n",
        "\n",
        "# Import required libraries after installation\n",
        "import networkx as nx\n",
        "from arango import ArangoClient\n",
        "# Updated imports for langchain\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import pandas as pd\n",
        "import json\n",
        "import gdown\n",
        "import requests\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import gzip\n",
        "import time\n",
        "\n",
        "print(\"All dependencies imported successfully!\")"
      ],
      "metadata": {
        "id": "Io26PYfBUoow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2522ac31-1f0b-4e7e-efff-66673bbf1222"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing python-arango...\n",
            "✅ Successfully installed python-arango\n",
            "✅ networkx is already installed\n",
            "✅ pandas is already installed\n",
            "✅ gdown is already installed\n",
            "✅ requests is already installed\n",
            "✅ tqdm is already installed\n",
            "✅ langchain is already installed\n",
            "Installing langchain-openai...\n",
            "✅ Successfully installed langchain-openai\n",
            "Installing langchain-community...\n",
            "✅ Successfully installed langchain-community\n",
            "All dependencies imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSetting up Amazon SNAP dataset downloads...\")\n",
        "\n",
        "def download_file(url, filename):\n",
        "    \"\"\"Download a file with progress bar\"\"\"\n",
        "    # This function is only called if the file is missing, so no need to check if it exists here\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx status codes)\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        block_size = 1024  # 1 Kibibyte\n",
        "\n",
        "        with open(filename, 'wb') as file, tqdm(\n",
        "                desc=filename,\n",
        "                total=total_size,\n",
        "                unit='iB',\n",
        "                unit_scale=True,\n",
        "                unit_divisor=1024,\n",
        "            ) as bar:\n",
        "            for data in response.iter_content(block_size):\n",
        "                size = file.write(data)\n",
        "                bar.update(size)\n",
        "\n",
        "        print(f\"✅ Downloaded {filename}\")\n",
        "        return filename\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"⚠️ Error downloading file: {e}\")\n",
        "        return None  # Indicate failure\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ An unexpected error occurred during download: {e}\")\n",
        "        return None # Indicate failure\n",
        "\n",
        "def parse_amazon_metadata(gz_file):\n",
        "    \"\"\"Parse Amazon metadata from gzipped file\"\"\"\n",
        "    print(f\"Parsing metadata from {gz_file}...\")\n",
        "    products = []\n",
        "    current_product = {}\n",
        "\n",
        "    try:\n",
        "        with gzip.open(gz_file, 'rt', encoding='latin1') as f:\n",
        "            lines = []\n",
        "            for i, line in enumerate(tqdm(f, desc=\"Reading lines\")):\n",
        "                line = line.strip()\n",
        "                if line == '':\n",
        "                    if current_product:\n",
        "                        products.append(current_product)\n",
        "                        current_product = {}\n",
        "                else:\n",
        "                    if ':' in line:\n",
        "                        key, value = line.split(':', 1)\n",
        "                        current_product[key.strip()] = value.strip()\n",
        "\n",
        "                # For testing, limit to a sample\n",
        "                if len(products) >= 10000:  # Adjust this number as needed\n",
        "                    break\n",
        "\n",
        "        print(f\"✅ Parsed {len(products)} products\")\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(products)\n",
        "\n",
        "        # Clean up and prepare for graph analysis\n",
        "        if 'ASIN' in df.columns:\n",
        "            df['ASIN'] = df['ASIN'].astype(str)\n",
        "\n",
        "        # Save a CSV version for easier reuse\n",
        "        csv_file = gz_file.replace('.gz', '.csv')\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"✅ Saved to {csv_file}\")\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error parsing metadata file: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_amazon_copurchase(gz_file):\n",
        "    \"\"\"Parse Amazon co-purchasing network from gzipped file\"\"\"\n",
        "    print(f\"Parsing co-purchase network from {gz_file}...\")\n",
        "    edges = []\n",
        "\n",
        "    try:\n",
        "        with gzip.open(gz_file, 'rt', encoding='latin1') as f:\n",
        "            for i, line in enumerate(tqdm(f, desc=\"Reading edges\")):\n",
        "                if not line.startswith('#'):\n",
        "                    source, target = line.strip().split()\n",
        "                    edges.append((source, target))\n",
        "\n",
        "                # For testing, limit to a sample\n",
        "                if len(edges) >= 100000:  # Adjust this number as needed\n",
        "                    break\n",
        "\n",
        "        print(f\"✅ Parsed {len(edges)} co-purchase edges\")\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(edges, columns=['source', 'target'])\n",
        "\n",
        "        # Save a CSV version for easier reuse\n",
        "        csv_file = gz_file.replace('.gz', '.csv')\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"✅ Saved to {csv_file}\")\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error parsing co-purchase network: {e}\")\n",
        "        return None\n",
        "\n",
        "# SNAP Amazon Dataset URLs - choose based on your needs\n",
        "amazon_datasets = {\n",
        "    \"metadata\": \"http://snap.stanford.edu/data/amazon/productGraph/metadata.json.gz\",\n",
        "    \"copurchase\": \"http://snap.stanford.edu/data/amazon0601.txt.gz\",\n",
        "    # For a smaller dataset, you can use category-specific ones:\n",
        "    \"books\": \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Books.csv\",\n",
        "    \"electronics\": \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Electronics.csv\"\n",
        "}\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "data_dir = \"amazon_data\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Download and process datasets\n",
        "print(\"\\nDownloading and processing Amazon SNAP datasets...\")\n",
        "datasets = {}\n",
        "\n",
        "# Load metadata.csv directly if it exists, otherwise, load from gz and create a CSV.\n",
        "metadata_csv_file = os.path.join(data_dir, \"metadata.csv\")\n",
        "metadata_gz_file = os.path.join(data_dir, \"metadata.json.gz\")\n",
        "\n",
        "if os.path.exists(metadata_csv_file):\n",
        "    print(\"✅ Using existing metadata CSV file.\")\n",
        "    try:\n",
        "        datasets[\"metadata\"] = pd.read_csv(metadata_csv_file)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"⚠️ The metadata CSV file is empty!\")\n",
        "        datasets[\"metadata\"] = pd.DataFrame()  # Create an empty dataframe\n",
        "else:\n",
        "    print(\"Metadata CSV file not found. Attempting to parse from gz file...\")\n",
        "    if not os.path.exists(metadata_gz_file):\n",
        "        download_file(amazon_datasets[\"metadata\"], metadata_gz_file)\n",
        "    metadata_df = parse_amazon_metadata(metadata_gz_file)\n",
        "    if metadata_df is not None:\n",
        "        datasets[\"metadata\"] = metadata_df\n",
        "    else:\n",
        "        print(\"⚠️ Could not parse metadata from gz file. Metadata will be unavailable.\")\n",
        "        datasets[\"metadata\"] = pd.DataFrame() # or some other default if needed\n",
        "\n",
        "\n",
        "# Load copurchase data. Load from CSV if available, otherwise load from gz and create.\n",
        "copurchase_csv_file = os.path.join(data_dir, \"amazon0601.txt.csv\")\n",
        "copurchase_gz_file = os.path.join(data_dir, \"amazon0601.txt.gz\")\n",
        "\n",
        "if os.path.exists(copurchase_csv_file):\n",
        "    print(\"✅ Using existing copurchase CSV file.\")\n",
        "    try:\n",
        "        datasets[\"copurchase\"] = pd.read_csv(copurchase_csv_file)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"⚠️ The copurchase CSV file is empty!\")\n",
        "        datasets[\"copurchase\"] = pd.DataFrame(columns=['source', 'target']) # or some other default if needed\n",
        "else:\n",
        "    print(\"Co-purchase CSV file not found. Attempting to parse from gz file...\")\n",
        "    if not os.path.exists(copurchase_gz_file):\n",
        "        download_file(amazon_datasets[\"copurchase\"], copurchase_gz_file)\n",
        "    copurchase_df = parse_amazon_copurchase(copurchase_gz_file)\n",
        "    if copurchase_df is not None:\n",
        "        datasets[\"copurchase\"] = copurchase_df\n",
        "    else:\n",
        "        print(\"⚠️ Could not parse copurchase data from gz file. Co-purchase data will be unavailable.\")\n",
        "        datasets[\"copurchase\"] = pd.DataFrame(columns=['source', 'target']) # or some other default if needed"
      ],
      "metadata": {
        "id": "ve5mVAdtU51O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebee7ccb-6b3c-44cb-8093-fe1e4851e0a8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Setting up Amazon SNAP dataset downloads...\n",
            "\n",
            "Downloading and processing Amazon SNAP datasets...\n",
            "Metadata CSV file not found. Attempting to parse from gz file...\n",
            "Parsing metadata from amazon_data/metadata.json.gz...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading lines: 9430088it [02:15, 69358.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Parsed 0 products\n",
            "✅ Saved to amazon_data/metadata.json.csv\n",
            "✅ Using existing copurchase CSV file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPreparing Amazon product graph...\")\n",
        "\n",
        "def prepare_amazon_graph(copurchase_df, metadata_df=None):\n",
        "    \"\"\"Transform Amazon dataset into a graph structure\"\"\"\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add edges from co-purchase data\n",
        "    for _, row in tqdm(copurchase_df.iterrows(), total=len(copurchase_df), desc=\"Adding edges\"):\n",
        "        G.add_edge(str(row['source']), str(row['target']))\n",
        "\n",
        "    # Add node attributes from metadata if available\n",
        "    if metadata_df is not None and 'ASIN' in metadata_df.columns:\n",
        "        print(\"Adding product metadata to nodes...\")\n",
        "        for _, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc=\"Adding metadata\"):\n",
        "            asin = str(row['ASIN'])\n",
        "            if asin in G:\n",
        "                # Add attributes from metadata\n",
        "                for col in metadata_df.columns:\n",
        "                    if col != 'ASIN' and pd.notna(row[col]):\n",
        "                        G.nodes[asin][col] = row[col]\n",
        "\n",
        "    print(f\"✅ Created Amazon product graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
        "    return G\n",
        "\n",
        "# Create the Amazon product graph\n",
        "amazon_graph = prepare_amazon_graph(\n",
        "    datasets[\"copurchase\"],\n",
        "    datasets.get(\"metadata\") if \"metadata\" in datasets else None\n",
        ")\n"
      ],
      "metadata": {
        "id": "rTlHtVHfU9xD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb01eba2-c771-450e-92be-6efe128ab6b2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing Amazon product graph...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Adding edges: 100%|██████████| 100000/100000 [00:04<00:00, 22755.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created Amazon product graph with 26520 nodes and 100000 edges\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPerforming basic graph analysis...\")\n",
        "\n",
        "def analyze_graph(G):\n",
        "    \"\"\"Perform basic analysis on the graph\"\"\"\n",
        "    analysis = {}\n",
        "\n",
        "    # Basic statistics\n",
        "    analysis[\"num_nodes\"] = G.number_of_nodes()\n",
        "    analysis[\"num_edges\"] = G.number_of_edges()\n",
        "\n",
        "    # Compute degree statistics (this can be slow for large graphs)\n",
        "    print(\"Computing degree statistics...\")\n",
        "    degrees = [d for n, d in G.degree()]\n",
        "    analysis[\"avg_degree\"] = sum(degrees) / len(degrees)\n",
        "    analysis[\"max_degree\"] = max(degrees)\n",
        "\n",
        "    # Identify top nodes by degree (potential influential products)\n",
        "    print(\"Finding most connected products...\")\n",
        "    degree_dict = dict(G.degree())\n",
        "    top_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    analysis[\"top_nodes_by_degree\"] = top_nodes\n",
        "\n",
        "    # Extract largest connected component\n",
        "    print(\"Finding largest connected component...\")\n",
        "    largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
        "    analysis[\"largest_cc_size\"] = len(largest_cc)\n",
        "    analysis[\"largest_cc_percentage\"] = len(largest_cc) / G.number_of_nodes() * 100\n",
        "\n",
        "    # Sample a small subgraph for visualization and detailed analysis\n",
        "    print(\"Creating sample subgraph for detailed analysis...\")\n",
        "    seed_node = top_nodes[0][0]  # Use the highest degree node as seed\n",
        "    sample_nodes = set([seed_node])\n",
        "    frontier = set([seed_node])\n",
        "\n",
        "    # BFS to get a neighborhood around the seed\n",
        "    while len(sample_nodes) < 100 and frontier:\n",
        "        new_frontier = set()\n",
        "        for node in frontier:\n",
        "            neighbors = set(G.neighbors(node))\n",
        "            new_nodes = neighbors - sample_nodes\n",
        "            sample_nodes.update(list(new_nodes)[:5])  # Limit to 5 new neighbors per node\n",
        "            new_frontier.update(list(new_nodes)[:5])\n",
        "            if len(sample_nodes) >= 100:\n",
        "                break\n",
        "        frontier = new_frontier\n",
        "\n",
        "    sample_subgraph = G.subgraph(sample_nodes)\n",
        "    analysis[\"sample_subgraph\"] = sample_subgraph\n",
        "    analysis[\"sample_subgraph_size\"] = sample_subgraph.number_of_nodes()\n",
        "\n",
        "    print(f\"✅ Completed basic graph analysis\")\n",
        "    return analysis\n",
        "\n",
        "# Run the analysis\n",
        "graph_analysis = analyze_graph(amazon_graph)\n",
        "\n",
        "# Print some findings\n",
        "print(\"\\nAmazon Product Network Analysis Results:\")\n",
        "print(f\"Total products (nodes): {graph_analysis['num_nodes']:,}\")\n",
        "print(f\"Total co-purchase links (edges): {graph_analysis['num_edges']:,}\")\n",
        "print(f\"Average connections per product: {graph_analysis['avg_degree']:.2f}\")\n",
        "print(f\"Maximum connections for a product: {graph_analysis['max_degree']}\")\n",
        "print(f\"Largest connected component contains {graph_analysis['largest_cc_percentage']:.2f}% of products\")\n",
        "\n",
        "print(\"\\nTop 10 most connected products (potential influencers):\")\n",
        "for i, (node, degree) in enumerate(graph_analysis['top_nodes_by_degree'], 1):\n",
        "    print(f\"{i}. Product {node}: {degree} connections\")"
      ],
      "metadata": {
        "id": "pzEKbBL0WXsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4410ddd8-aaa3-41de-bd54-3709fc4101df"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing basic graph analysis...\n",
            "Computing degree statistics...\n",
            "Finding most connected products...\n",
            "Finding largest connected component...\n",
            "Creating sample subgraph for detailed analysis...\n",
            "✅ Completed basic graph analysis\n",
            "\n",
            "Amazon Product Network Analysis Results:\n",
            "Total products (nodes): 26,520\n",
            "Total co-purchase links (edges): 100,000\n",
            "Average connections per product: 7.54\n",
            "Maximum connections for a product: 139\n",
            "Largest connected component contains 100.00% of products\n",
            "\n",
            "Top 10 most connected products (potential influencers):\n",
            "1. Product 36: 139 connections\n",
            "2. Product 5: 134 connections\n",
            "3. Product 89: 121 connections\n",
            "4. Product 41: 115 connections\n",
            "5. Product 44: 103 connections\n",
            "6. Product 50: 100 connections\n",
            "7. Product 48: 99 connections\n",
            "8. Product 406: 97 connections\n",
            "9. Product 1862: 94 connections\n",
            "10. Product 90: 92 connections\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPerforming community detection...\")\n",
        "\n",
        "def detect_communities(G, max_nodes=5000):\n",
        "    \"\"\"Detect communities in the graph\"\"\"\n",
        "    # For large graphs, sample a subgraph\n",
        "    if G.number_of_nodes() > max_nodes:\n",
        "        print(f\"Graph is large ({G.number_of_nodes()} nodes), sampling {max_nodes} nodes for community detection...\")\n",
        "        # Use the sample subgraph from analysis\n",
        "        subgraph = graph_analysis[\"sample_subgraph\"]\n",
        "    else:\n",
        "        subgraph = G\n",
        "\n",
        "    # Convert to undirected for community detection algorithms\n",
        "    undirected_G = subgraph.to_undirected()\n",
        "\n",
        "    print(\"Running Louvain community detection...\")\n",
        "    try:\n",
        "        # Try to import community module (python-louvain)\n",
        "        import community as community_louvain\n",
        "        partition = community_louvain.best_partition(undirected_G)\n",
        "\n",
        "        # Count communities and their sizes\n",
        "        communities = {}\n",
        "        for node, community_id in partition.items():\n",
        "            if community_id not in communities:\n",
        "                communities[community_id] = []\n",
        "            communities[community_id].append(node)\n",
        "\n",
        "        # Sort by community size\n",
        "        sorted_communities = sorted(communities.items(), key=lambda x: len(x[1]), reverse=True)\n",
        "\n",
        "        print(f\"✅ Detected {len(communities)} communities\")\n",
        "        return {\n",
        "            \"algorithm\": \"louvain\",\n",
        "            \"num_communities\": len(communities),\n",
        "            \"community_sizes\": [len(comm) for _, comm in sorted_communities[:10]],\n",
        "            \"top_communities\": sorted_communities[:5],\n",
        "            \"node_communities\": partition\n",
        "        }\n",
        "    except ImportError:\n",
        "        print(\"Louvain algorithm not available, using connected components instead...\")\n",
        "        # Fallback to connected components\n",
        "        components = list(nx.connected_components(undirected_G))\n",
        "        sorted_components = sorted(components, key=len, reverse=True)\n",
        "\n",
        "        print(f\"✅ Detected {len(components)} connected components\")\n",
        "        return {\n",
        "            \"algorithm\": \"connected_components\",\n",
        "            \"num_communities\": len(components),\n",
        "            \"community_sizes\": [len(comp) for comp in sorted_components[:10]],\n",
        "            \"top_communities\": [(i, list(comp)) for i, comp in enumerate(sorted_components[:5])],\n",
        "            \"node_communities\": {node: i for i, comp in enumerate(components) for node in comp}\n",
        "        }\n",
        "\n",
        "# Try to install the community detection library\n",
        "try:\n",
        "    install_and_import(\"python-louvain\")\n",
        "except:\n",
        "    print(\"Could not install python-louvain. Will use connected components instead.\")\n",
        "\n",
        "# Run community detection\n",
        "community_analysis = detect_communities(amazon_graph)\n",
        "\n",
        "# Print community findings\n",
        "print(\"\\nCommunity Detection Results:\")\n",
        "print(f\"Algorithm used: {community_analysis['algorithm']}\")\n",
        "print(f\"Number of communities/clusters detected: {community_analysis['num_communities']}\")\n",
        "print(f\"Top 5 community sizes: {community_analysis['community_sizes'][:5]}\")"
      ],
      "metadata": {
        "id": "4RyFHpgaWZwe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f02843-c201-4e7d-a747-da2d5fd38321"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing community detection...\n",
            "Installing python-louvain...\n",
            "✅ Successfully installed python-louvain\n",
            "Graph is large (26520 nodes), sampling 5000 nodes for community detection...\n",
            "Running Louvain community detection...\n",
            "✅ Detected 4 communities\n",
            "\n",
            "Community Detection Results:\n",
            "Algorithm used: louvain\n",
            "Number of communities/clusters detected: 4\n",
            "Top 5 community sizes: [34, 24, 24, 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nChecking ArangoDB connection...\")\n",
        "\n",
        "def setup_arangodb():\n",
        "    \"\"\"Setup ArangoDB connection safely\"\"\"\n",
        "    try:\n",
        "        client = ArangoClient(hosts=\"http://localhost:8529\")\n",
        "        try:\n",
        "            db = client.db('amazon_db', username='root', password='password')\n",
        "            print(\"✅ Connected to ArangoDB with provided credentials\")\n",
        "        except:\n",
        "            # If default credentials fail, create the database\n",
        "            sys_db = client.db('_system', username='root', password='password')\n",
        "            if not sys_db.has_database('amazon_db'):\n",
        "                sys_db.create_database('amazon_db')\n",
        "                print(\"✅ Created amazon_db database\")\n",
        "            db = client.db('amazon_db', username='root', password='password')\n",
        "        return db\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error connecting to ArangoDB: {e}\")\n",
        "        print(\"⚠️ Graph will not be persisted to database\")\n",
        "        return None\n",
        "\n",
        "db = setup_arangodb()\n",
        "\n",
        "def persist_amazon_graph(G, db):\n",
        "    \"\"\"Save Amazon graph into ArangoDB\"\"\"\n",
        "    if db is None:\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Create collections\n",
        "        nodes_collection = \"amazon_products\"\n",
        "        edges_collection = \"amazon_copurchase\"\n",
        "\n",
        "        if not db.has_collection(nodes_collection):\n",
        "            db.create_collection(nodes_collection)\n",
        "            print(f\"✅ Created collection: {nodes_collection}\")\n",
        "\n",
        "        if not db.has_collection(edges_collection):\n",
        "            db.create_collection(edges_collection, edge=True)\n",
        "            print(f\"✅ Created edge collection: {edges_collection}\")\n",
        "\n",
        "        # Insert nodes in batches\n",
        "        products_collection = db.collection(nodes_collection)\n",
        "        batch_size = 1000\n",
        "        total_nodes = G.number_of_nodes()\n",
        "\n",
        "        print(f\"Inserting {total_nodes} nodes in batches of {batch_size}...\")\n",
        "        nodes_list = list(G.nodes(data=True))\n",
        "\n",
        "        for i in tqdm(range(0, total_nodes, batch_size), desc=\"Inserting node batches\"):\n",
        "            batch = nodes_list[i:i+batch_size]\n",
        "            nodes_batch = []\n",
        "\n",
        "            for node, attrs in batch:\n",
        "                node_doc = {\"_key\": str(node).replace(\"/\", \"_\")}\n",
        "                node_doc.update(attrs)\n",
        "                nodes_batch.append(node_doc)\n",
        "\n",
        "            # Import batch\n",
        "            products_collection.import_bulk(nodes_batch, on_duplicate=\"update\")\n",
        "\n",
        "        # Insert edges in batches\n",
        "        copurchase_collection = db.collection(edges_collection)\n",
        "        total_edges = G.number_of_edges()\n",
        "\n",
        "        print(f\"Inserting {total_edges} edges in batches of {batch_size}...\")\n",
        "        edges_list = list(G.edges())\n",
        "\n",
        "        for i in tqdm(range(0, total_edges, batch_size), desc=\"Inserting edge batches\"):\n",
        "            batch = edges_list[i:i+batch_size]\n",
        "            edges_batch = []\n",
        "\n",
        "            for source, target in batch:\n",
        "                source_key = str(source).replace(\"/\", \"_\")\n",
        "                target_key = str(target).replace(\"/\", \"_\")\n",
        "                edges_batch.append({\n",
        "                    \"_from\": f\"{nodes_collection}/{source_key}\",\n",
        "                    \"_to\": f\"{nodes_collection}/{target_key}\"\n",
        "                })\n",
        "\n",
        "            # Import batch\n",
        "            copurchase_collection.import_bulk(edges_batch, on_duplicate=\"update\")\n",
        "\n",
        "        print(f\"✅ Successfully persisted Amazon graph to ArangoDB\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error persisting graph: {e}\")\n",
        "        return False\n",
        "\n",
        "# Only persist if database is available and user confirms\n",
        "if db is not None:\n",
        "    # Uncomment the line below to persist (can be slow for large graphs)\n",
        "    # persist_amazon_graph(amazon_graph, db)\n",
        "    print(\"ArangoDB persistence ready, but skipped for performance reasons.\")\n",
        "    print(\"You can uncomment the persistence code to enable it.\")"
      ],
      "metadata": {
        "id": "AybV5ZH2YlWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac060da1-87da-4cdf-d467-496032f5af48"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking ArangoDB connection...\n",
            "✅ Connected to ArangoDB with provided credentials\n",
            "ArangoDB persistence ready, but skipped for performance reasons.\n",
            "You can uncomment the persistence code to enable it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSetting up LangChain for graph insights with Cohere...\")\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import Cohere\n",
        "import os\n",
        "\n",
        "def setup_langchain_cohere(graph_analysis, community_analysis):\n",
        "    \"\"\"Sets up LangChain with the Cohere API.\"\"\"\n",
        "    try:\n",
        "        if 'COHERE_API_KEY' in os.environ:\n",
        "            print(\"✅ Using Cohere\")\n",
        "            query_template = PromptTemplate(\n",
        "                template=\"\"\"\n",
        "            Based on the network analysis:\n",
        "\n",
        "            Graph has {num_nodes} nodes and {num_edges} edges\n",
        "            Average degree: {avg_degree:.2f}\n",
        "            Max degree: {max_degree}\n",
        "            Communities detected: {num_communities}\n",
        "            Community Sizes: {community_sizes}\n",
        "            Top Products by Degree: {top_nodes_by_degree}\n",
        "\n",
        "            Query: {query}\n",
        "\n",
        "            Answer:\n",
        "            \"\"\",\n",
        "                input_variables=[\"query\", \"num_nodes\", \"num_edges\", \"avg_degree\", \"max_degree\", \"num_communities\", \"community_sizes\", \"top_nodes_by_degree\"]\n",
        "            )\n",
        "            cohere_llm = Cohere() # Initialize Cohere LLM\n",
        "            return LLMChain(llm=cohere_llm, prompt=query_template)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"COHERE_API_KEY environment variable not set.\") # Raise error if API key is missing\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error setting up LangChain: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def agentic_query(query, llm_chain, graph_analysis, community_analysis):\n",
        "    \"\"\"Processes queries about the graph using the LLM chain.\"\"\"\n",
        "    if llm_chain is None:\n",
        "        return \"LLM chain not available. Please check setup.\"\n",
        "    return llm_chain.run({\"query\": query, **graph_analysis, **community_analysis})\n",
        "\n",
        "\n",
        "# Example graph and community analysis data (replace with your actual data)\n",
        "graph_analysis = {\n",
        "    \"num_nodes\": 100,\n",
        "    \"num_edges\": 200,\n",
        "    \"avg_degree\": 2.5,\n",
        "    \"max_degree\": 10,\n",
        "    \"top_nodes_by_degree\": [(1, 10), (2, 9)],\n",
        "}\n",
        "community_analysis = {\n",
        "    \"num_communities\": 5,\n",
        "    \"community_sizes\": [20, 15, 12, 34, 19]\n",
        "}\n",
        "\n",
        "try:\n",
        "  #setup langchain with Cohere\n",
        "  llm_chain = setup_langchain_cohere(graph_analysis, community_analysis)\n",
        "\n",
        "  # Example queries\n",
        "  queries = [\n",
        "      \"What is the most influential product?\",\n",
        "      \"How many communities are there?\",\n",
        "      \"What is the structure of the network?\",\n",
        "      \"Give me a product recommendation.\"\n",
        "  ]\n",
        "\n",
        "  for query in queries:\n",
        "      response = agentic_query(query, llm_chain, graph_analysis, community_analysis)\n",
        "      print(f\"Query: {query}\\nAnswer: {response}\\n\")\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\") #specifically handle missing API key error"
      ],
      "metadata": {
        "id": "sgj1VoqIY4cY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d15391be-37cc-4711-dd69-70773a13d2a5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Setting up LangChain for graph insights with Cohere...\n",
            "✅ Using Cohere\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-43667655b039>:29: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  return LLMChain(llm=cohere_llm, prompt=query_template)\n",
            "<ipython-input-33-43667655b039>:44: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  return llm_chain.run({\"query\": query, **graph_analysis, **community_analysis})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the most influential product?\n",
            "Answer: The product (1,10) is the most influential product in the graph according to the provided network analysis. Product (1,10) has a high degree and is central to the network, meaning it connects many communities and is a top product in terms of connectivity. This could indicate that Product (1,10) is popular, widely used, or referenced, making it influential in driving connections within the network. \n",
            "\n",
            "Query: How many communities are there?\n",
            "Answer: The graph has been detected to have 5 communities. \n",
            "\n",
            "Query: What is the structure of the network?\n",
            "Answer: The network has a clear community structure, a graph theorem which refers to clusters of nodes and the edge paths within them. In this case, it has been detected as having 5 communities, with varying sizes and degrees. \n",
            "\n",
            "One node, labeled as 1, is the single hub in its community, while another node, labeled as 2, is the authority in its respective community. Authority nodes tend to connect to other nodes while hub nodes tend to be connected to by other nodes. \n",
            "\n",
            "These communities may be interconnected, with some nodes acting as bridges between communities, facilitating communication and interaction between distinct clusters. \n",
            "\n",
            "In conjunction with the average coefficient of 2.5, it suggests a combination of high cohesion within each community and significant interconnectedness between communities. \n",
            "\n",
            "Query: Give me a product recommendation.\n",
            "Answer: With a maximum degree of only 10, it seems this product has a relatively small number of customer advocates with a large number of connections (i.e., a relatively small number of mega-influencers). Therefore, it's likely that one of the products in this graph has a very high number of endorsements from these mega-influencers. \n",
            "\n",
            "In terms of the top-recommended products by degree, the product (1,10) seems like it might be a popular product that is endorsed by a relatively small group of highly connected customers. Since this group is relatively small, these customers might be interested in new products that are related to this one (and could endorse it to others). \n",
            "\n",
            "Based on these factors, I would recommend targeting these mega-influencers with a personalized offer for a new, related product to the one they already endorse. Since this product has a very high degree, even a small number of additional endorsements could have a large impact on its overall popularity. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"COHERE_API_KEY\"] = \"WcXnR3lxNWGwnoJmI2hq8CnCmPfAr8fRbFFacCsT\"\n",
        "!pip install cohere"
      ],
      "metadata": {
        "id": "MFUiGR35Y8Hq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe2e7ff9-3a3a-40a2-edd4-1c09b8430495"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.13.12-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.4.0)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.10.6)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.21.0)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
            "Downloading cohere-5.13.12-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: types-requests, fastavro, cohere\n",
            "Successfully installed cohere-5.13.12 fastavro-1.10.0 types-requests-2.32.0.20241016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import Cohere\n",
        "\n",
        "def setup_langchain_cohere(graph_analysis, community_analysis):\n",
        "    \"\"\"Sets up LangChain with the Cohere API.\"\"\"\n",
        "    try:\n",
        "        if 'COHERE_API_KEY' in os.environ:\n",
        "            print(\"✅ Using Cohere\")\n",
        "            query_template = PromptTemplate(\n",
        "                template=\"\"\"\n",
        "            Based on the network analysis:\n",
        "\n",
        "            Graph has {num_nodes} nodes and {num_edges} edges\n",
        "            Average degree: {avg_degree:.2f}\n",
        "            Max degree: {max_degree}\n",
        "            Communities detected: {num_communities}\n",
        "            Community Sizes: {community_sizes}\n",
        "            Top Products by Degree: {top_nodes_by_degree}\n",
        "\n",
        "            Query: {query}\n",
        "\n",
        "            Answer:\n",
        "            \"\"\",\n",
        "                input_variables=[\"query\", \"num_nodes\", \"num_edges\", \"avg_degree\", \"max_degree\", \"num_communities\", \"community_sizes\", \"top_nodes_by_degree\"]\n",
        "            )\n",
        "            cohere_llm = Cohere()\n",
        "            return LLMChain(llm=cohere_llm, prompt=query_template)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"COHERE_API_KEY environment variable not set.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error setting up LangChain: {e}\")\n",
        "        return None\n",
        "\n",
        "def agentic_query(query, llm_chain, graph_analysis, community_analysis):\n",
        "    \"\"\"Processes queries about the graph using the LLM chain.\"\"\"\n",
        "    if llm_chain is None:\n",
        "        return \"LLM chain not available. Please check setup.\"\n",
        "    return llm_chain.run({\"query\": query, **graph_analysis, **community_analysis})\n",
        "\n",
        "# Example graph and community analysis data (replace with your actual data)\n",
        "graph_analysis = {\n",
        "    \"num_nodes\": 100,\n",
        "    \"num_edges\": 200,\n",
        "    \"avg_degree\": 2.5,\n",
        "    \"max_degree\": 10,\n",
        "    \"top_nodes_by_degree\": [(1, 10), (2, 9)],\n",
        "}\n",
        "community_analysis = {\n",
        "    \"num_communities\": 5,\n",
        "    \"community_sizes\": [20, 15, 12, 34, 19]\n",
        "}\n",
        "\n",
        "try:\n",
        "  #setup langchain with Cohere\n",
        "  llm_chain = setup_langchain_cohere(graph_analysis, community_analysis)\n",
        "\n",
        "  print(\"\\nTesting agentic queries on Amazon graph...\")\n",
        "  examples = [\n",
        "      \"What are the most influential products in the Amazon network?\",\n",
        "      \"What insights can we gain from the community structure?\",\n",
        "      \"How can this graph be used for product recommendations?\",\n",
        "      \"What does the network structure tell us about Amazon's marketplace?\"\n",
        "  ]\n",
        "\n",
        "  for example in examples:\n",
        "      print(f\"\\nQuery: {example}\")\n",
        "      time.sleep(1)\n",
        "      result = agentic_query(example, llm_chain, graph_analysis, community_analysis) #fixed by providing the other three variables\n",
        "      print(f\"Result: {result}\")\n",
        "\n",
        "  print(\"\\n✅ Amazon SNAP Graph Analysis complete!\")\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "J_WsV7zLY6fU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27b3ff63-013a-4e5f-d14c-41166d3fbd97"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using Cohere\n",
            "\n",
            "Testing agentic queries on Amazon graph...\n",
            "\n",
            "Query: What are the most influential products in the Amazon network?\n",
            "Result: Based on the provided network analysis of Amazon's product network, the top two most influential products (by degree) are product IDs 1 and 2, with 9 and 10 reviews respectively. These products have the highest degree centrality among all items in the network, meaning they have been reviewed the most often. This could suggest that these products are among the most popular and well-reviewed items on Amazon, generating a significant amount of discussion and feedback from customers. \n",
            "\n",
            "It's important to note that degree centrality is just one aspect of network analysis, and other metrics such as betweenness and eigenvector centrality can also influence the overall influence and importance of a node within the network. However, based solely on the information provided, Products 1 and 2 stand out as the most prominently reviewed items. \n",
            "\n",
            "It would be beneficial to examine additional insights and consider other factors to understand the entire landscape of Amazon's product network thoroughly. For example, analyzing products with high betweenness or eigenvector centrality may provide information on items that have a significant impact on connecting different parts of the network or driving customer attention and purchases despite potentially having fewer direct reviews. \n",
            "\n",
            "Furthermore, other attributes such as sales volume, product category, brand popularity, and pricing strategies can also influence a product's success and\n",
            "\n",
            "Query: What insights can we gain from the community structure?\n",
            "Result: The community structure of a graph can provide several insights into its behavior and properties. Here are some key insights that can be gained from the community structure of the given network:\n",
            "\n",
            "1. **Identifying Central Topics or Assets**: In a community, entities tend to cluster around specific topics, features, or assets. In the context of a product ecosystem, communities can represent clusters of products or services that are related to one another. These communities can help identify the primary themes or highend topics that are prevalent across the graph. \n",
            "\n",
            "2. **Understanding Cross-communication**: The presence of edges between communities can indicate cross-pollination, where products or entities from different communities interact with each other. Analyzing these connections helps identify key bridging entities that link different communities, highlighting influential products or assets that are influential in driving cross-community interaction. \n",
            "\n",
            "3. **Detecting Authority Figures**: Identifying the most central entities within each community can help pinpoint authority figures or popular products within specific clusters. These entities often have a significant influence on their respective communities and can be critical in driving change or behavior within the network. \n",
            "\n",
            "4. **Understanding Community Size & Density**: The size and density of the detected communities can provide insights into the breadth and tightness of the communities. Larger\n",
            "\n",
            "Query: How can this graph be used for product recommendations?\n",
            "Result: The graph can be used for product recommendations in the following ways: \n",
            "\n",
            "1. Identifying Popular Products: The top products by degree indicate the products with the highest connectivity in the graph. In this case, product (1) has a degree of 10, indicating it is popular among the customers and is connected to many other products. Product (2) also has a high degree of 9, making it another popular item. These products can be recommended to customers seeking widely popular and versatile items. \n",
            "\n",
            "2. Recommending Products Based on Similarity: \n",
            "\n",
            "The community detection analysis suggests that there are groups of products with shared interests and preferences. The recommendation algorithm could suggest products from the same community. \n",
            "\n",
            "To offer more personalised recommendations, products within a customer's expressed interest or purchasing history could be identified and their neighbouring products in the graph recommended to the customer. This adopts a behaviour where customers may also enjoy these correlated products. \n",
            "\n",
            "3. Understanding Balanced Versus Unpopular Products: \n",
            "\n",
            "Averaging 2.5 degrees suggests that most products are within reach or accessible to others. Products with degrees less than this average could be identified as potentially inactive or less popular. Future recommendations could incentivise or promote these to balance sales. \n",
            "\n",
            "This all hinges on\n",
            "\n",
            "Query: What does the network structure tell us about Amazon's marketplace?\n",
            "Result: The network analysis of Amazon's marketplace indicates a complex and interconnected marketplace with a significant number of products and brands. Here's what the structure of this network tells us: \n",
            "\n",
            "1. **Large Number of Nodes (Products and Brands)**: The graph comprises 100 nodes, which translates to a substantial number of products or brands available in Amazon's marketplace. This indicates the diversity of choices available to customers, ranging from numerous brands and product categories. \n",
            "\n",
            "2. **Density and Connectivity**: With an average degree of 2.50, the graph implies a reasonably dense and interconnected network of products and brands. This suggests that in many cases, products are connected and recommended to users based on their interactions with other products, creating a interconnected web of suggested items. \n",
            "\n",
            "3. **Variation in Brand and Product Popularity**: The variation in the number of edges for each node indicates that some products or brands are much more popular (with either higher sales volume or more attention from reviewers) than others. The popularity gap exists, suggesting diversified trends in consumer preferences and markets within the vast Amazon marketplace. \n",
            "\n",
            "4. **Community Detection**: The detection of five distinct communities suggests that there are groups or clusters of products or brands with stronger connectivity within the dataset. These communities might represent niche\n",
            "\n",
            "✅ Amazon SNAP Graph Analysis complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import os\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_cohere import ChatCohere"
      ],
      "metadata": {
        "id": "bISraOqJZxMW"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_cohere streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Osaq5tbcg1n",
        "outputId": "26b7ef3f-dd4c-4285-f18f-df9f82685449"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_cohere in /usr/local/lib/python3.11/dist-packages (0.4.2)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.42.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: cohere<6.0,>=5.12.0 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (5.13.12)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (0.3.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (0.3.39)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (2.10.6)\n",
            "Requirement already satisfied: types-pyyaml<7.0.0.0,>=6.0.12.20240917 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (6.0.12.20241230)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.27.1)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (2.27.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (0.21.0)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (2.32.0.20241016)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.19)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (2.0.38)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.11.12)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (2.8.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_cohere) (1.33)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.9.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain_cohere) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain_cohere) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain_cohere) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_cohere) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.6)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain_cohere) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain_cohere) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain_cohere) (2024.10.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain_cohere) (4.67.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain_cohere) (1.3.1)\n",
            "Downloading streamlit-1.42.2-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.42.2 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "import networkx as nx\n",
        "from arango import ArangoClient\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import pandas as pd\n",
        "import json\n",
        "import gdown\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import gzip\n",
        "import time\n",
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt  # Import matplotlib for visualizations\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1. Dependency Installation (Using Jupyter-Friendly Method)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def install_and_import(package):\n",
        "    installed = True #boolean to store whether the required packages are installed\n",
        "\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "        print(f\"✅ {package} is already installed\")\n",
        "    except ImportError:\n",
        "        try:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "            print(f\"✅ Successfully installed {package}\")\n",
        "            importlib.import_module(package) # Check if it can be imported after installation\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error installing {package}: {e}\")\n",
        "            print(\"Skipping this package.\")\n",
        "            installed = False\n",
        "\n",
        "    return installed\n",
        "\n",
        "#required packages\n",
        "required_packages = [\"python-arango\", \"networkx\", \"pandas\", \"gdown\", \"requests\", \"tqdm\", \"streamlit\", \"matplotlib\"]\n",
        "langchain_packages = [\"langchain\", \"langchain-openai\", \"langchain-community\"]\n",
        "all_packages = required_packages + langchain_packages\n",
        "installed_all_packages = True\n",
        "missing_packages = []\n",
        "\n",
        "for package in all_packages:\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "        print(f\"✅ {package} is already installed\")\n",
        "    except ImportError:\n",
        "        missing_packages.append(package)\n",
        "        installed_all_packages = False # We don't know the results yet.\n",
        "\n",
        "if not installed_all_packages:\n",
        "    print(\"Installing all missing packages in one go...\")\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + missing_packages)\n",
        "        print(\"✅ Successfully installed all missing packages\")\n",
        "        for package in missing_packages:\n",
        "            try:\n",
        "                 importlib.import_module(package)  # Double check if it can be imported now.\n",
        "            except:\n",
        "                 print(f\"Failed to import {package}, there might be a dependency error.\")\n",
        "                 installed_all_packages = False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error installing packages: {e}\")\n",
        "        print(\"Skipping graph analysis and web interface setup.\")\n",
        "        installed_all_packages = False # Something failed to install so we return false.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMezTj_RZxvQ",
        "outputId": "649f72c7-fe63-499c-b789-13fc42575eb1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ networkx is already installed\n",
            "✅ pandas is already installed\n",
            "✅ gdown is already installed\n",
            "✅ requests is already installed\n",
            "✅ tqdm is already installed\n",
            "✅ streamlit is already installed\n",
            "✅ matplotlib is already installed\n",
            "✅ langchain is already installed\n",
            "Installing all missing packages in one go...\n",
            "✅ Successfully installed all missing packages\n",
            "Failed to import python-arango, there might be a dependency error.\n",
            "Failed to import langchain-openai, there might be a dependency error.\n",
            "Failed to import langchain-community, there might be a dependency error.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    import streamlit as st\n",
        "    import networkx as nx\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    import time\n",
        "    import os\n",
        "    from langchain import PromptTemplate, LLMChain\n",
        "    from langchain.llms import Cohere\n",
        "\n",
        "    try: # To catch missing variables if any setup issues occur\n",
        "      # Load your datasets and process the graph as previously done\n",
        "      amazon_graph = nx.DiGraph([(i, (i+1) % 100) for i in range(100)])\n",
        "    except Exception as e:\n",
        "      print(f\"Error creating test graph: {e}\")\n",
        "      exit() # Exit due to essential setup failure\n",
        "\n",
        "    from itertools import islice # for graph sampling\n",
        "\n",
        "    # Functions (Copied from previous responses, please note these are just examples)\n",
        "    def analyze_graph(G):\n",
        "        \"\"\"Analyzes a graph and returns various metrics.\"\"\"\n",
        "        analysis = {}\n",
        "        analysis[\"num_nodes\"] = G.number_of_nodes()\n",
        "        analysis[\"num_edges\"] = G.number_of_edges()\n",
        "        degrees = [d for n, d in G.degree()]\n",
        "        analysis[\"avg_degree\"] = sum(degrees) / len(degrees) if degrees else 0\n",
        "        analysis[\"max_degree\"] = max(degrees) if degrees else 0\n",
        "        degree_dict = dict(G.degree())\n",
        "        top_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "        analysis[\"top_nodes_by_degree\"] = top_nodes\n",
        "\n",
        "        # Find largest weakly connected component\n",
        "        connected_components = list(nx.weakly_connected_components(G))\n",
        "        if connected_components:\n",
        "            largest_cc = max(connected_components, key=len)\n",
        "            analysis[\"largest_cc_size\"] = len(largest_cc)\n",
        "            analysis[\"largest_cc_percentage\"] = len(largest_cc) / G.number_of_nodes() * 100\n",
        "        else:\n",
        "            analysis[\"largest_cc_size\"] = 0\n",
        "            analysis[\"largest_cc_percentage\"] = 0\n",
        "\n",
        "        # Sample a small subgraph for visualization and detailed analysis\n",
        "        if top_nodes:\n",
        "            seed_node = top_nodes[0][0]\n",
        "            sample_nodes = set([seed_node])\n",
        "            frontier = set([seed_node])\n",
        "            while len(sample_nodes) < 100 and frontier:\n",
        "                new_frontier = set()\n",
        "                for node in frontier:\n",
        "                    neighbors = set(G.neighbors(node))\n",
        "                    new_nodes = neighbors - sample_nodes\n",
        "                    sample_nodes.update(list(new_nodes)[:5])\n",
        "                    new_frontier.update(list(new_nodes)[:5])\n",
        "                    if len(sample_nodes) >= 100:\n",
        "                        break\n",
        "                frontier = new_frontier\n",
        "            sample_subgraph = G.subgraph(sample_nodes)\n",
        "            analysis[\"sample_subgraph_nodes\"] = list(sample_nodes)  # Store nodes instead of subgraph\n",
        "            analysis[\"sample_subgraph_size\"] = sample_subgraph.number_of_nodes()\n",
        "        else:\n",
        "            analysis[\"sample_subgraph_nodes\"] = []\n",
        "            analysis[\"sample_subgraph_size\"] = 0\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def detect_communities(G, graph_analysis=None, max_nodes=5000):\n",
        "        \"\"\"Detects communities within the graph using Louvain or connected components.\"\"\"\n",
        "        # Handle large graphs by sampling\n",
        "        try: #this could give an error, if no data is there for example\n",
        "            if G.number_of_nodes() > max_nodes:\n",
        "                print(f\"Graph is large ({G.number_of_nodes()} nodes), sampling {max_nodes} nodes for community detection...\")\n",
        "                if graph_analysis and \"sample_subgraph_nodes\" in graph_analysis:\n",
        "                    subgraph = G.subgraph(graph_analysis[\"sample_subgraph_nodes\"])\n",
        "                else:\n",
        "                    # Sample nodes if no sample subgraph is available\n",
        "                    subgraph = G.subgraph(list(G.nodes())[:max_nodes])\n",
        "            else:\n",
        "                subgraph = G\n",
        "\n",
        "            # Convert to undirected for community detection\n",
        "            undirected_G = subgraph.to_undirected()\n",
        "\n",
        "            try:\n",
        "                # Try using Louvain algorithm\n",
        "                import community as community_louvain\n",
        "                partition = community_louvain.best_partition(undirected_G)\n",
        "                communities = {}\n",
        "                for node, community_id in partition.items():\n",
        "                    if community_id not in communities:\n",
        "                        communities[community_id] = []\n",
        "                    communities[community_id].append(node)\n",
        "                sorted_communities = sorted(communities.items(), key=lambda x: len(x[1]), reverse=True)\n",
        "                return {\n",
        "                    \"algorithm\": \"louvain\",\n",
        "                    \"num_communities\": len(communities),\n",
        "                    \"community_sizes\": [len(comm) for _, comm in sorted_communities[:10]],\n",
        "                    \"top_communities\": sorted_communities[:5],\n",
        "                    \"node_communities\": partition,\n",
        "                }\n",
        "            except ImportError:\n",
        "                print(\"Louvain algorithm not available, using connected components instead...\")\n",
        "                # Fallback to connected components\n",
        "                components = list(nx.connected_components(undirected_G))\n",
        "                sorted_components = sorted(components, key=len, reverse=True)\n",
        "                return {\n",
        "                    \"algorithm\": \"connected_components\",\n",
        "                    \"num_communities\": len(components),\n",
        "                    \"community_sizes\": [len(comp) for comp in sorted_components[:10]],\n",
        "                    \"top_communities\": [(i, list(comp)) for i, comp in enumerate(sorted_components[:5])],\n",
        "                    \"node_communities\": {node: i for i, comp in enumerate(components) for node in comp}\n",
        "                }\n",
        "        except:\n",
        "            print(\"Can't do community setup, maybe there is no information in this set?\")\n",
        "            return{}\n",
        "\n",
        "    def setup_langchain_cohere(graph_analysis, community_analysis):\n",
        "        \"\"\"Sets up LangChain with the Cohere API.\"\"\"\n",
        "        try:\n",
        "            # Check if Cohere API key is set\n",
        "            cohere_api_key = os.environ.get('COHERE_API_KEY')\n",
        "            if cohere_api_key:\n",
        "                print(\"✅ Using Cohere\")\n",
        "                query_template = PromptTemplate(\n",
        "                    template=\"\"\"\n",
        "                Based on the network analysis:\n",
        "                Graph has {num_nodes} nodes and {num_edges} edges\n",
        "                Average degree: {avg_degree:.2f}\n",
        "                Max degree: {max_degree}\n",
        "                Communities detected: {num_communities}\n",
        "                Community Sizes: {community_sizes}\n",
        "                Top Products by Degree: {top_nodes_by_degree}\n",
        "                Query: {query}\n",
        "                Answer:\n",
        "                \"\"\",\n",
        "                    input_variables=[\"query\", \"num_nodes\", \"num_edges\", \"avg_degree\", \"max_degree\",\n",
        "                                    \"num_communities\", \"community_sizes\", \"top_nodes_by_degree\"]\n",
        "                )\n",
        "                cohere_llm = Cohere(cohere_api_key=cohere_api_key)\n",
        "                return LLMChain(llm=cohere_llm, prompt=query_template)\n",
        "            else:\n",
        "                # Raise error if API key is not set\n",
        "                raise ValueError(\"COHERE_API_KEY environment variable not set.\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error setting up LangChain: {e}\")\n",
        "            return None\n",
        "\n",
        "    def agentic_query(query, llm_chain, graph_analysis, community_analysis):\n",
        "        \"\"\"Processes queries about the graph using the LLM chain.\"\"\"\n",
        "        if llm_chain is None:\n",
        "            return \"LLM chain not available. Please check setup.\"\n",
        "\n",
        "        # Create a new dictionary with all parameters\n",
        "        params = {\n",
        "            \"query\": query\n",
        "        }\n",
        "        # Add graph analysis parameters\n",
        "        for key, value in graph_analysis.items():\n",
        "            if isinstance(value, (str, int, float, list, tuple, dict)) and key != \"sample_subgraph_nodes\":\n",
        "                params[key] = value\n",
        "\n",
        "        # Add community analysis parameters\n",
        "        for key, value in community_analysis.items():\n",
        "            if isinstance(value, (str, int, float, list, tuple, dict)) and key != \"node_communities\":\n",
        "                params[key] = value\n",
        "\n",
        "        # Run the chain with the prepared parameters\n",
        "        return llm_chain.run(**params)\n",
        "\n",
        "    graph_analysis = analyze_graph(amazon_graph)\n",
        "    community_analysis = detect_communities(amazon_graph, graph_analysis)\n",
        "    print(graph_analysis)\n",
        "    print(community_analysis)\n",
        "    # ----------------------------------------------------------------------------\n",
        "    # 4. Streamlit Application\n",
        "    # ----------------------------------------------------------------------------\n",
        "\n",
        "    st.title(\"Amazon Product Network Analysis\")\n",
        "\n",
        "    st.sidebar.header(\"Graph Statistics\")\n",
        "    st.sidebar.write(f\"Total Products (Nodes): {graph_analysis.get('num_nodes'):,}\")\n",
        "    st.sidebar.write(f\"Total Co-Purchase Links (Edges): {graph_analysis.get('num_edges'):,}\")\n",
        "    st.sidebar.write(f\"Average Connections per Product: {graph_analysis.get('avg_degree'):.2f}\")\n",
        "    st.sidebar.write(f\"Maximum Connections for a Product: {graph_analysis.get('max_degree')}\")\n",
        "    st.sidebar.write(f\"Largest Connected Component: {graph_analysis.get('largest_cc_percentage'):.2f}%\")\n",
        "\n",
        "    st.sidebar.header(\"Community Statistics\")\n",
        "    st.sidebar.write(f\"Number of Communities: {community_analysis.get('num_communities', 'N/A')}\")\n",
        "    st.sidebar.write(\"Top 5 Community Sizes:\")\n",
        "    if \"community_sizes\" in community_analysis:\n",
        "        for i, size in enumerate(community_analysis['community_sizes'][:5]):\n",
        "            st.sidebar.write(f\"{i+1}: \" + str(size))\n",
        "\n",
        "    # Visualization - using matplotlib for simplicity\n",
        "    st.header(\"Graph Visualization\")\n",
        "    st.write(\"Displaying a sample subgraph for visualization\")\n",
        "    if 'sample_subgraph' in graph_analysis and graph_analysis['sample_subgraph']:\n",
        "        fig, ax = plt.subplots()\n",
        "        nx.draw(graph_analysis[\"sample_subgraph\"], with_labels=True, ax=ax)\n",
        "        st.pyplot(fig)  # st.pyplot for matplotlib plots. If using plotly or other library you will use different command.\n",
        "    else:\n",
        "        st.write(\"No sample subgraph available.\")\n",
        "\n",
        "            # LLM-powered Insights Section\n",
        "    st.header(\"LLM-Powered Insights\")\n",
        "\n",
        "    llm_chain = setup_langchain_cohere(graph_analysis, community_analysis)\n",
        "\n",
        "    if llm_chain:\n",
        "        query = st.text_input(\"Enter your query about the Amazon network:\")\n",
        "        if query:\n",
        "            result = agentic_query(query, llm_chain, graph_analysis, community_analysis)\n",
        "            st.write(\"LLM Answer:\", result)\n",
        "    else:\n",
        "        st.error(\"Failed to set up the LLM Chain. Check your API key and settings.\")\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzAq0w1lZ2iW",
        "outputId": "2f24bd25-aada-4b54-f41a-396ddc9aca52"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-25 09:32:44.630 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_nodes': 100, 'num_edges': 100, 'avg_degree': 2.0, 'max_degree': 2, 'top_nodes_by_degree': [(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2), (8, 2), (9, 2)], 'largest_cc_size': 100, 'largest_cc_percentage': 100.0, 'sample_subgraph_nodes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], 'sample_subgraph_size': 100}\n",
            "{'algorithm': 'louvain', 'num_communities': 11, 'community_sizes': [12, 11, 10, 10, 9, 9, 9, 9, 8, 7], 'top_communities': [(8, [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]), (9, [87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]), (5, [23, 24, 25, 26, 27, 28, 29, 30, 31, 32]), (10, [45, 46, 47, 48, 49, 50, 51, 52, 53, 54]), (1, [5, 6, 7, 8, 9, 10, 11, 12, 13])], 'node_communities': {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 4, 15: 4, 16: 4, 17: 4, 18: 4, 19: 4, 20: 4, 21: 4, 22: 4, 23: 5, 24: 5, 25: 5, 26: 5, 27: 5, 28: 5, 29: 5, 30: 5, 31: 5, 32: 5, 33: 8, 34: 8, 35: 8, 36: 8, 37: 8, 38: 8, 39: 8, 40: 8, 41: 8, 42: 8, 43: 8, 44: 8, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 2, 56: 2, 57: 2, 58: 2, 59: 2, 60: 2, 61: 2, 62: 2, 63: 3, 64: 3, 65: 3, 66: 3, 67: 3, 68: 3, 69: 3, 70: 3, 71: 3, 72: 6, 73: 6, 74: 6, 75: 6, 76: 6, 77: 6, 78: 7, 79: 7, 80: 7, 81: 7, 82: 7, 83: 7, 84: 7, 85: 7, 86: 7, 87: 9, 88: 9, 89: 9, 90: 9, 91: 9, 92: 9, 93: 9, 94: 9, 95: 9, 96: 9, 97: 9, 98: 0, 99: 0}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-25 09:32:45.033 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-02-25 09:32:45.037 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.041 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.044 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.048 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.051 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.055 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.058 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.061 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.065 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.069 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.071 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.074 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.077 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.079 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.082 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.085 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.087 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.090 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.092 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.095 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.098 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.100 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.101 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.103 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.107 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.112 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.114 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.122 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.122 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.149 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.158 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.161 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.166 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.171 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.172 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.179 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.180 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.181 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.182 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.184 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.189 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.193 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.196 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.198 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.199 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.200 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.203 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.204 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.204 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.205 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.206 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.207 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.208 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.209 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.209 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using Cohere\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-25 09:32:45.850 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.851 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.856 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.858 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.861 Session state does not function when running a script without `streamlit run`\n",
            "2025-02-25 09:32:45.865 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-25 09:32:45.873 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import base64\n",
        "import os\n",
        "import requests\n",
        "import gzip\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "@dataclass\n",
        "class GraphAnalysis:\n",
        "    num_nodes: int\n",
        "    num_edges: int\n",
        "    avg_degree: float\n",
        "    max_degree: int\n",
        "    top_nodes_by_degree: List[Tuple[str, int]]\n",
        "    largest_cc_size: int\n",
        "    largest_cc_percentage: float\n",
        "    sample_subgraph: Optional[nx.DiGraph]\n",
        "    sample_subgraph_size: int\n",
        "    image: str\n",
        "\n",
        "def visualize_graph(graph: Optional[nx.DiGraph]) -> str:\n",
        "    \"\"\"Visualizes the graph (or a sample) and returns a base64 encoded image.\"\"\"\n",
        "    if graph is None:\n",
        "        return \"\"\n",
        "\n",
        "    # Always sample a subgraph for visualization to keep it simple and fast\n",
        "    if graph.number_of_nodes() > 100:\n",
        "        top_nodes = sorted(graph.degree(), key=lambda x: x[1], reverse=True)[:10]  #Top 10\n",
        "        seed_node = top_nodes[0][0]\n",
        "        sample_nodes = {seed_node}\n",
        "        frontier = {seed_node}\n",
        "        while len(sample_nodes) < 100 and frontier:\n",
        "            new_frontier = set()\n",
        "            for node in frontier:\n",
        "                neighbors = set(graph.neighbors(node))\n",
        "                new_nodes = (neighbors - sample_nodes)\n",
        "                selected_nodes = list(new_nodes)[:5]  # Limit to 5 neighbors\n",
        "                sample_nodes.update(selected_nodes)\n",
        "                new_frontier.update(selected_nodes)\n",
        "                if len(sample_nodes) >= 100:\n",
        "                    break\n",
        "            frontier = new_frontier\n",
        "        graph = graph.subgraph(list(sample_nodes))\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12, 6))  # Adjust figure size as needed\n",
        "    nx.draw(graph, with_labels=True, font_weight='bold', node_size=400, font_size=9, alpha=0.7) #Keep small\n",
        "    plt.title(\"Generated Graph (Sample)\")  # Clarify it's a sample\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close()\n",
        "    return base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "\n",
        "def analyze_graph(graph: Optional[nx.DiGraph]) -> Optional[GraphAnalysis]:\n",
        "    \"\"\"Analyzes the graph and returns metrics, including a sampled image.\"\"\"\n",
        "    if graph is None:\n",
        "        return None\n",
        "\n",
        "    analysis = {\n",
        "        \"num_nodes\": graph.number_of_nodes(),\n",
        "        \"num_edges\": graph.number_of_edges(),\n",
        "    }\n",
        "    degrees = [d for _, d in graph.degree()]\n",
        "    analysis[\"avg_degree\"] = sum(degrees) / len(degrees) if degrees else 0.0\n",
        "    analysis[\"max_degree\"] = max(degrees) if degrees else 0\n",
        "    top_nodes = sorted(graph.degree(), key=lambda x: x[1], reverse=True)[:10] #Top 10\n",
        "    analysis[\"top_nodes_by_degree\"] = top_nodes\n",
        "\n",
        "    connected_components = list(nx.weakly_connected_components(graph))\n",
        "    if connected_components:\n",
        "        largest_cc = max(connected_components, key=len)\n",
        "        analysis[\"largest_cc_size\"] = len(largest_cc)\n",
        "        analysis[\"largest_cc_percentage\"] = (len(largest_cc) / graph.number_of_nodes()) * 100\n",
        "    else:\n",
        "        analysis[\"largest_cc_size\"] = 0\n",
        "        analysis[\"largest_cc_percentage\"] = 0.0\n",
        "\n",
        "    analysis[\"sample_subgraph\"] = None  # We'll *always* create the sample now\n",
        "    analysis[\"sample_subgraph_size\"] = 0\n",
        "    analysis[\"image\"] = visualize_graph(graph)  # Get sampled image\n",
        "\n",
        "    return GraphAnalysis(**analysis)\n",
        "\n",
        "\n",
        "def download_file(url: str, filename: str) -> Optional[str]:\n",
        "    \"\"\"Downloads a file with a progress bar.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        with open(filename, 'wb') as file, tqdm(\n",
        "            desc=filename, total=total_size, unit='iB', unit_scale=True, unit_divisor=1024\n",
        "        ) as bar:\n",
        "            for data in response.iter_content(1024):\n",
        "                file.write(data)\n",
        "                bar.update(len(data))\n",
        "        print(f\"✅ Downloaded {filename}\")\n",
        "        return filename\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"⚠️ Error downloading {filename}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def parse_amazon_copurchase(gz_file: str) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Parse Amazon co-purchasing network data.\"\"\"\n",
        "    print(f\"Parsing co-purchase network from {gz_file}...\")\n",
        "    edges = []\n",
        "    try:\n",
        "        with gzip.open(gz_file, 'rt', encoding='latin1') as f:\n",
        "            for line in tqdm(f, desc=\"Reading edges\"):\n",
        "                if not line.startswith('#'):\n",
        "                    source, target = line.strip().split()\n",
        "                    edges.append((source, target))\n",
        "        print(f\"✅ Parsed {len(edges)} co-purchase edges\")\n",
        "        df = pd.DataFrame(edges, columns=['source', 'target'])\n",
        "        csv_file = gz_file.replace('.gz', '.csv')\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"✅ Saved to {csv_file}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error parsing co-purchase data: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_graph(copurchase_df: Optional[pd.DataFrame]) -> Optional[nx.DiGraph]:\n",
        "    \"\"\"Loads the graph from DataFrames.\"\"\"\n",
        "    if copurchase_df is None:\n",
        "        print(\"⚠️ Copurchase DataFrame is None.\")\n",
        "        return None\n",
        "    try:\n",
        "        graph = nx.DiGraph()\n",
        "        with tqdm(total=len(copurchase_df), desc=\"Adding edges\") as pbar:\n",
        "            for _, row in copurchase_df.iterrows():\n",
        "                graph.add_edge(str(row['source']), str(row['target']))\n",
        "                pbar.update(1)\n",
        "        print(f\"✅ Created graph: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
        "        return graph\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error loading graph: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_data(max_nodes_to_display: int = 1000) -> Dict[str, str]:\n",
        "    \"\"\"Downloads, parses, and analyzes the graph, returning results for Gradio.\"\"\"\n",
        "\n",
        "    amazon_datasets = {\n",
        "        \"copurchase\": \"http://snap.stanford.edu/data/amazon0601.txt.gz\",\n",
        "    }\n",
        "    data_dir = \"amazon_data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    copurchase_file = os.path.join(data_dir, \"amazon0601.txt.gz\")\n",
        "    copurchase_csv_file = copurchase_file.replace('.gz', '.csv')\n",
        "    if os.path.exists(copurchase_csv_file):\n",
        "        print(\"Using existing copurchase CSV.\")\n",
        "        copurchase_df = pd.read_csv(copurchase_csv_file)\n",
        "    else:\n",
        "        if not os.path.exists(copurchase_file):\n",
        "            download_file(amazon_datasets[\"copurchase\"], copurchase_file)\n",
        "        copurchase_df = parse_amazon_copurchase(copurchase_file)\n",
        "        if copurchase_df is None:\n",
        "            return {\n",
        "                \"graph_summary\": \"Error: Could not load co-purchase data.\",\n",
        "                \"graph_visualization\": \"\",\n",
        "                \"status\": \"Data loading error.\"\n",
        "            }\n",
        "\n",
        "    amazon_graph = load_graph(copurchase_df)\n",
        "    if amazon_graph is None:\n",
        "        return {\n",
        "            \"graph_summary\": \"Error: Could not create graph.\",\n",
        "            \"graph_visualization\": \"\",\n",
        "            \"status\": \"Graph creation error.\"\n",
        "        }\n",
        "\n",
        "    graph_analysis = analyze_graph(amazon_graph)\n",
        "    if graph_analysis is None:\n",
        "        return {\n",
        "            \"graph_summary\": \"Error: Graph analysis failed.\",\n",
        "            \"graph_visualization\": \"\",\n",
        "            \"status\": \"Graph analysis error.\"\n",
        "        }\n",
        "\n",
        "    # Create a concise summary for the text output\n",
        "    summary = (\n",
        "        f\"The graph has {graph_analysis.num_nodes} nodes and {graph_analysis.num_edges} edges.\\n\"\n",
        "        f\"Average degree: {graph_analysis.avg_degree:.2f}, Max degree: {graph_analysis.max_degree}.\\n\"\n",
        "        f\"Largest connected component size: {graph_analysis.largest_cc_size} \"\n",
        "        f\"({graph_analysis.largest_cc_percentage:.2f}% of nodes).\\n\"\n",
        "        f\"Top nodes by degree: {graph_analysis.top_nodes_by_degree[:5]}\" # Top 5\n",
        "\n",
        "    )\n",
        "\n",
        "    # Limit the graph displayed\n",
        "    if graph_analysis.num_nodes > max_nodes_to_display:\n",
        "      summary += f\"\\n\\nDisplaying a sample of up to {max_nodes_to_display} nodes.\"\n",
        "\n",
        "    return {\n",
        "        \"graph_summary\": summary,\n",
        "        \"graph_visualization\": graph_analysis.image,  # Always a sampled/limited image\n",
        "        \"status\": \"Graph analysis complete!\",\n",
        "    }\n",
        "\n",
        "# --- Gradio Interface Setup ---\n",
        "inputs = [\n",
        "    gr.Slider(minimum=100, maximum=10000, value=1000, step=100, label=\"Max Nodes to Display\", key=\"max_nodes_to_display\")\n",
        "]\n",
        "outputs = [\n",
        "    gr.Textbox(label=\"Graph Summary\", key=\"graph_summary\"),\n",
        "    gr.HTML(label=\"Graph Visualization\", key=\"graph_visualization\"),\n",
        "    gr.Textbox(label=\"Status\", key=\"status\"),\n",
        "]\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_data,\n",
        "    inputs=inputs,\n",
        "    outputs=outputs,\n",
        "    title=\"Amazon Graph Analysis (Simplified)\",\n",
        "    description=\"Analyzes the Amazon product co-purchasing network and displays a simplified graph.\",\n",
        "    allow_flagging=\"never\",  # Prevent flagging, since we don't have user input\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(debug=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "E1rwLFhmdpiJ",
        "outputId": "5694c37b-be76-46bd-e9c3-704f89999d9a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8bd2aa29efd996b207.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8bd2aa29efd996b207.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rb8ITqDIdrBb",
        "outputId": "66d26e9b-c1f4-410a-c493-ac267df67d72"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.18.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.18.0-py3-none-any.whl (62.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 fastapi-0.115.8 ffmpy-0.5.0 gradio-5.18.0 gradio-client-1.7.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.7 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "markupsafe"
                ]
              },
              "id": "2968e0a771164aa69c30ea98ce25ff75"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}
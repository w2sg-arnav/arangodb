{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFFkWznYT-J7",
        "outputId": "336900a0-2669-4e67-db5d-5b282b0b73ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-arango\n",
            "  Downloading python_arango-8.1.5-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from python-arango) (2.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from python-arango) (2.32.3)\n",
            "Collecting requests_toolbelt (from python-arango)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from python-arango) (2.3.0)\n",
            "Requirement already satisfied: setuptools>=42 in /usr/local/lib/python3.11/dist-packages (from python-arango) (75.1.0)\n",
            "Collecting importlib_metadata>=4.7.1 (from python-arango)\n",
            "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.11/dist-packages (from python-arango) (24.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.7.1->python-arango) (3.21.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->python-arango) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->python-arango) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->python-arango) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading python_arango-8.1.5-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: importlib_metadata, requests_toolbelt, python-arango\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib-metadata 4.6.4\n",
            "    Uninstalling importlib-metadata-4.6.4:\n",
            "      Successfully uninstalled importlib-metadata-4.6.4\n",
            "Successfully installed importlib_metadata-8.6.1 python-arango-8.1.5 requests_toolbelt-1.0.0\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.7-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.64.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.35 (from langchain)\n",
            "  Downloading langchain_core-0.3.40-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.3.11-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading SQLAlchemy-2.0.38-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.11.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.35->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
            "  Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
            "  Downloading greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain-0.3.19-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.7-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.64.0-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.3/472.3 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.6/345.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.40-py3-none-any.whl (414 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.3/414.3 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
            "Downloading langsmith-0.3.11-py3-none-any.whl (335 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.3/335.3 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.0-py3-none-any.whl (30 kB)\n",
            "Downloading SQLAlchemy-2.0.38-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (602 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m602.4/602.4 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.3/231.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: zstandard, python-dotenv, propcache, orjson, mypy-extensions, marshmallow, jsonpointer, jiter, httpx-sse, h11, greenlet, frozenlist, aiohappyeyeballs, yarl, typing-inspect, tiktoken, SQLAlchemy, jsonpatch, httpcore, aiosignal, pydantic-settings, httpx, dataclasses-json, aiohttp, openai, langsmith, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
            "Successfully installed SQLAlchemy-2.0.38 aiohappyeyeballs-2.4.6 aiohttp-3.11.13 aiosignal-1.3.2 dataclasses-json-0.6.7 frozenlist-1.5.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 httpx-sse-0.4.0 jiter-0.8.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.19 langchain-community-0.3.18 langchain-core-0.3.40 langchain-openai-0.3.7 langchain-text-splitters-0.3.6 langsmith-0.3.11 marshmallow-3.26.1 mypy-extensions-1.0.0 openai-1.64.0 orjson-3.10.15 propcache-0.3.0 pydantic-settings-2.8.0 python-dotenv-1.0.1 tiktoken-0.9.0 typing-inspect-0.9.0 yarl-1.18.3 zstandard-0.23.0\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages correctly\n",
        "import sys\n",
        "!{sys.executable} -m pip install python-arango networkx pandas gdown\n",
        "!{sys.executable} -m pip install langchain langchain-openai langchain-community openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p7B8F1cPUhfC"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "import networkx as nx\n",
        "from arango import ArangoClient\n",
        "# Update these langchain imports\n",
        "from langchain_openai import OpenAI  # Or use this\n",
        "# Alternatively, you might need to use:\n",
        "# from langchain_community.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import pandas as pd\n",
        "import json\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DqL4k38Ulpk",
        "outputId": "bf04d903-4b96-4134-b41a-f5c0d8063fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing python-arango...\n",
            "✅ Successfully installed python-arango\n",
            "✅ networkx is already installed\n",
            "✅ langchain is already installed\n",
            "✅ gdown is already installed\n",
            "✅ pandas is already installed\n",
            "✅ openai is already installed\n",
            "Note: cugraph requires CUDA. If you don't have a GPU, we'll use NetworkX instead.\n",
            "Installing cugraph...\n",
            "⚠️ Could not install cugraph. Using NetworkX for all graph operations.\n",
            "All dependencies imported successfully!\n"
          ]
        }
      ],
      "source": [
        "def install_and_import(package):\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "        print(f\"✅ {package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"✅ Successfully installed {package}\")\n",
        "\n",
        "# Install required packages\n",
        "required_packages = [\"python-arango\", \"networkx\", \"langchain\", \"gdown\", \"pandas\", \"openai\"]\n",
        "for package in required_packages:\n",
        "    install_and_import(package)\n",
        "\n",
        "# Note about cugraph\n",
        "print(\"Note: cugraph requires CUDA. If you don't have a GPU, we'll use NetworkX instead.\")\n",
        "try:\n",
        "    install_and_import(\"cugraph\")\n",
        "    use_cugraph = True\n",
        "except:\n",
        "    print(\"⚠️ Could not install cugraph. Using NetworkX for all graph operations.\")\n",
        "    use_cugraph = False\n",
        "\n",
        "# Import required libraries after installation\n",
        "import networkx as nx\n",
        "from arango import ArangoClient\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "import pandas as pd\n",
        "import json\n",
        "import gdown\n",
        "\n",
        "print(\"All dependencies imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io26PYfBUoow",
        "outputId": "49793055-2569-49a3-b33c-3b3bd93371be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing python-arango...\n",
            "✅ Successfully installed python-arango\n",
            "✅ networkx is already installed\n",
            "✅ pandas is already installed\n",
            "✅ gdown is already installed\n",
            "✅ requests is already installed\n",
            "✅ tqdm is already installed\n",
            "✅ langchain is already installed\n",
            "Installing langchain-openai...\n",
            "✅ Successfully installed langchain-openai\n",
            "Installing langchain-community...\n",
            "✅ Successfully installed langchain-community\n",
            "All dependencies imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Setup & Dependencies\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "# Function to install and import a package\n",
        "def install_and_import(package):\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "        print(f\"✅ {package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"✅ Successfully installed {package}\")\n",
        "\n",
        "# Install required packages\n",
        "required_packages = [\"python-arango\", \"networkx\", \"pandas\", \"gdown\", \"requests\", \"tqdm\"]\n",
        "# Update langchain packages to use the newer structure\n",
        "langchain_packages = [\"langchain\", \"langchain-openai\", \"langchain-community\"]\n",
        "\n",
        "for package in required_packages:\n",
        "    install_and_import(package)\n",
        "for package in langchain_packages:\n",
        "    install_and_import(package)\n",
        "\n",
        "# Import required libraries after installation\n",
        "import networkx as nx\n",
        "from arango import ArangoClient\n",
        "# Updated imports for langchain\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import pandas as pd\n",
        "import json\n",
        "import gdown\n",
        "import requests\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import gzip\n",
        "import time\n",
        "\n",
        "print(\"All dependencies imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve5mVAdtU51O",
        "outputId": "8c551a06-9c7e-458b-eede-a5944c0956d6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Setting up Amazon SNAP dataset downloads...\n",
            "\n",
            "Downloading and processing Amazon SNAP datasets...\n",
            "Processing dataset: metadata\n",
            "CSV file for metadata not found. Downloading and parsing...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "amazon_data/metadata.json.gz: 100%|██████████| 3.13G/3.13G [03:45<00:00, 14.9MiB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Downloaded amazon_data/metadata.json.gz\n",
            "Parsing metadata from amazon_data/metadata.json.gz...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading lines: 9430088it [01:17, 121536.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1) at end of file:\n",
            "✅ Parsed 0 products\n",
            "⚠️ Warning: 'ASIN' column not found in metadata.\n",
            "✅ Saved to amazon_data/metadata.json.csv\n",
            "Processing dataset: amazon0302\n",
            "CSV file for amazon0302 not found. Downloading and parsing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "amazon_data/amazon0302.txt.gz: 100%|██████████| 4.45M/4.45M [00:01<00:00, 3.04MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Downloaded amazon_data/amazon0302.txt.gz\n",
            "Parsing co-purchase network from amazon_data/amazon0302.txt.gz...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading edges: 1234881it [00:00, 1356632.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Parsed 1234877 co-purchase edges\n",
            "✅ Saved to amazon_data/amazon0302.txt.csv\n",
            "Processing dataset: amazon0312\n",
            "CSV file for amazon0312 not found. Downloading and parsing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "amazon_data/amazon0312.txt.gz: 100%|██████████| 10.8M/10.8M [00:01<00:00, 8.21MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Downloaded amazon_data/amazon0312.txt.gz\n",
            "Parsing co-purchase network from amazon_data/amazon0312.txt.gz...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading edges: 3200444it [00:02, 1346056.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Parsed 3200440 co-purchase edges\n",
            "✅ Saved to amazon_data/amazon0312.txt.csv\n",
            "Processing dataset: amazon0505\n",
            "CSV file for amazon0505 not found. Downloading and parsing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "amazon_data/amazon0505.txt.gz: 100%|██████████| 11.2M/11.2M [00:01<00:00, 6.39MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Downloaded amazon_data/amazon0505.txt.gz\n",
            "Parsing co-purchase network from amazon_data/amazon0505.txt.gz...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading edges: 3356828it [00:02, 1335174.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Parsed 3356824 co-purchase edges\n",
            "✅ Saved to amazon_data/amazon0505.txt.csv\n",
            "Processing dataset: amazon0601\n",
            "CSV file for amazon0601 not found. Downloading and parsing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "amazon_data/amazon0601.txt.gz: 100%|██████████| 11.3M/11.3M [00:01<00:00, 6.76MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Downloaded amazon_data/amazon0601.txt.gz\n",
            "Parsing co-purchase network from amazon_data/amazon0601.txt.gz...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading edges: 3387392it [00:02, 1307101.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Parsed 3387388 co-purchase edges\n",
            "✅ Saved to amazon_data/amazon0601.txt.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import gzip\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "print(\"\\nSetting up Amazon SNAP dataset downloads...\")\n",
        "\n",
        "def download_file(url, filename):\n",
        "    \"\"\"Download a file with progress bar\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        block_size = 1024\n",
        "\n",
        "        with open(filename, 'wb') as file, tqdm(\n",
        "                desc=filename,\n",
        "                total=total_size,\n",
        "                unit='iB',\n",
        "                unit_scale=True,\n",
        "                unit_divisor=1024,\n",
        "            ) as bar:\n",
        "            for data in response.iter_content(block_size):\n",
        "                size = file.write(data)\n",
        "                bar.update(size)\n",
        "\n",
        "        print(f\"✅ Downloaded {filename}\")\n",
        "        return filename\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"⚠️ Error downloading file: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ An unexpected error occurred during download: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_amazon_metadata(gz_file):\n",
        "    \"\"\"Parse Amazon metadata from gzipped file using json.loads, handling multi-line JSON objects.\"\"\"\n",
        "    print(f\"Parsing metadata from {gz_file}...\")\n",
        "    products = []\n",
        "    current_product_lines = []  # Accumulate lines for the current product\n",
        "\n",
        "    try:\n",
        "        with gzip.open(gz_file, 'rt', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(tqdm(f, desc=\"Reading lines\")):\n",
        "                line = line.strip()\n",
        "                if line == \"\":  # Empty line signals end of a product entry\n",
        "                    if current_product_lines:  # If we have lines accumulated\n",
        "                        try:\n",
        "                            # Join accumulated lines and parse as JSON\n",
        "                            product_json = json.loads(\"\".join(current_product_lines))\n",
        "                            products.append(product_json)\n",
        "                        except json.JSONDecodeError as e:\n",
        "                            print(f\"⚠️ JSONDecodeError: {e} on lines ending at {i + 1}:\")\n",
        "                            #for problematic_line in current_product_lines: #show all the problematic lines.\n",
        "                                #print(problematic_line)\n",
        "                        current_product_lines = []  # Reset for the next product\n",
        "                else:\n",
        "                    current_product_lines.append(line)  # Accumulate non-empty lines\n",
        "\n",
        "            # Handle any remaining lines after the loop (last product)\n",
        "            if current_product_lines:\n",
        "                try:\n",
        "                    product_json = json.loads(\"\".join(current_product_lines))\n",
        "                    products.append(product_json)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"⚠️ JSONDecodeError: {e} at end of file:\")\n",
        "                    #for problematic_line in current_product_lines:\n",
        "                        #print(problematic_line)\n",
        "\n",
        "        print(f\"✅ Parsed {len(products)} products\")\n",
        "        df = pd.DataFrame(products)\n",
        "\n",
        "        if 'ASIN' in df.columns:\n",
        "            df['ASIN'] = df['ASIN'].astype(str)\n",
        "        else:\n",
        "            print(\"⚠️ Warning: 'ASIN' column not found in metadata.\")\n",
        "            df['ASIN'] = ''\n",
        "\n",
        "        csv_file = gz_file.replace('.gz', '.csv')\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"✅ Saved to {csv_file}\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error parsing metadata file: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def parse_amazon_copurchase(gz_file, max_edges=None):\n",
        "    \"\"\"Parse Amazon co-purchasing network, with optional edge limit.\"\"\"\n",
        "    print(f\"Parsing co-purchase network from {gz_file}...\")\n",
        "    edges = []\n",
        "    try:\n",
        "        with gzip.open(gz_file, 'rt', encoding='latin1') as f:\n",
        "            for i, line in enumerate(tqdm(f, desc=\"Reading edges\")):\n",
        "                if not line.startswith('#'):\n",
        "                    source, target = line.strip().split()\n",
        "                    edges.append((source, target))\n",
        "                    if max_edges is not None and i >= max_edges:\n",
        "                        break\n",
        "\n",
        "        print(f\"✅ Parsed {len(edges)} co-purchase edges\")\n",
        "        df = pd.DataFrame(edges, columns=['source', 'target'])\n",
        "        csv_file = gz_file.replace('.gz', '.csv')\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"✅ Saved to {csv_file}\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error parsing co-purchase network: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# SNAP Amazon Dataset URLs - now a dictionary with dataset names as keys\n",
        "amazon_datasets = {\n",
        "    \"metadata\": {\n",
        "        \"url\": \"http://snap.stanford.edu/data/amazon/productGraph/metadata.json.gz\",\n",
        "        \"type\": \"metadata\"\n",
        "    },\n",
        "    \"amazon0302\": {\n",
        "        \"url\": \"http://snap.stanford.edu/data/amazon0302.txt.gz\",\n",
        "        \"type\": \"copurchase\"\n",
        "    },\n",
        "    \"amazon0312\": {\n",
        "        \"url\": \"http://snap.stanford.edu/data/amazon0312.txt.gz\",\n",
        "         \"type\": \"copurchase\"\n",
        "    },\n",
        "    \"amazon0505\": {\n",
        "        \"url\": \"http://snap.stanford.edu/data/amazon0505.txt.gz\",\n",
        "        \"type\": \"copurchase\"\n",
        "    },\n",
        "    \"amazon0601\": {\n",
        "        \"url\": \"http://snap.stanford.edu/data/amazon0601.txt.gz\",\n",
        "        \"type\": \"copurchase\"\n",
        "    },\n",
        "    # \"books\": { # Removed books and electronics\n",
        "    #     \"url\": \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Books.csv\",\n",
        "    #     \"type\": \"ratings_csv\" #different parsing logic needed\n",
        "    # },\n",
        "    # \"electronics\": {\n",
        "    #     \"url\": \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Electronics.csv\",\n",
        "    #      \"type\": \"ratings_csv\"\n",
        "    # }\n",
        "}\n",
        "\n",
        "# Create data directory\n",
        "data_dir = \"amazon_data\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Download and process datasets\n",
        "print(\"\\nDownloading and processing Amazon SNAP datasets...\")\n",
        "datasets = {} # This will hold all datasets (DataFrames)\n",
        "\n",
        "for dataset_name, dataset_info in amazon_datasets.items():\n",
        "    print(f\"Processing dataset: {dataset_name}\")\n",
        "    gz_file = os.path.join(data_dir, os.path.basename(dataset_info[\"url\"]))\n",
        "    csv_file = gz_file.replace('.gz', '.csv')\n",
        "    datasets[dataset_name] = {\"data\": None, \"graph\": None} # Initialize entry\n",
        "\n",
        "    if os.path.exists(csv_file):\n",
        "        print(f\"✅ Using existing CSV file for {dataset_name}.\")\n",
        "        try:\n",
        "            datasets[dataset_name][\"data\"] = pd.read_csv(csv_file)\n",
        "        except pd.errors.EmptyDataError:\n",
        "            print(f\"⚠️ CSV file for {dataset_name} is empty. Reprocessing.\")\n",
        "            if not os.path.exists(gz_file):\n",
        "                download_file(dataset_info[\"url\"], gz_file)\n",
        "            if dataset_info[\"type\"] == \"metadata\":\n",
        "                df = parse_amazon_metadata(gz_file)\n",
        "            elif dataset_info[\"type\"] == \"copurchase\":\n",
        "                df = parse_amazon_copurchase(gz_file)  # No max_edges here\n",
        "            else:\n",
        "                print(f\"⚠️ Unknown dataset type: {dataset_info['type']}\")\n",
        "                df = None\n",
        "\n",
        "            if df is not None:\n",
        "                datasets[dataset_name][\"data\"] = df\n",
        "\n",
        "    else:\n",
        "        print(f\"CSV file for {dataset_name} not found. Downloading and parsing...\")\n",
        "        if not os.path.exists(gz_file):\n",
        "            download_file(dataset_info[\"url\"], gz_file)\n",
        "        if dataset_info[\"type\"] == \"metadata\":\n",
        "            df = parse_amazon_metadata(gz_file)\n",
        "        elif dataset_info[\"type\"] == \"copurchase\":\n",
        "            df = parse_amazon_copurchase(gz_file)  # No max_edges here\n",
        "        else:\n",
        "            print(f\"⚠️ Unknown dataset type: {dataset_info['type']}\")\n",
        "            df = None\n",
        "\n",
        "        if df is not None:\n",
        "            datasets[dataset_name][\"data\"] = df\n",
        "        else:\n",
        "             datasets[dataset_name][\"data\"] = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rTlHtVHfU9xD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb501583-a5bf-4dcb-bd1d-5d54c8a8dd8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing Amazon product graph...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Adding edges: 100%|██████████| 3387388/3387388 [02:14<00:00, 25127.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata DataFrame is empty, no attributes added.\n",
            "✅ Created Amazon product graph with 403394 nodes and 3387388 edges\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nPreparing Amazon product graph...\")\n",
        "\n",
        "def prepare_amazon_graph(copurchase_df, metadata_df=None):\n",
        "    \"\"\"Transform Amazon dataset into a graph structure.\"\"\"\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add edges from co-purchase data. Handle the case where copurchase_df is None or empty.\n",
        "    if copurchase_df is not None and not copurchase_df.empty:\n",
        "        for _, row in tqdm(copurchase_df.iterrows(), total=len(copurchase_df), desc=\"Adding edges\"):\n",
        "            try:\n",
        "                G.add_edge(str(row['source']), str(row['target']))\n",
        "            except KeyError as e:\n",
        "                print(f\"⚠️ KeyError: {e}.  Missing 'source' or 'target' in row: {row}\")\n",
        "                # Decide how to handle.  Skip?  Log? For now, skip.\n",
        "                continue\n",
        "    else:\n",
        "        print(\"⚠️ No co-purchase data available.  Graph will have no edges.\")\n",
        "\n",
        "    # Add node attributes from metadata if available and valid.\n",
        "    if metadata_df is not None and 'ASIN' in metadata_df.columns and not metadata_df.empty:\n",
        "        print(\"Adding product metadata to nodes...\")\n",
        "        for _, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc=\"Adding metadata\"):\n",
        "            asin = str(row['ASIN'])  # Ensure ASIN is a string\n",
        "            if asin: #check that asin is not empty\n",
        "              if asin in G:  # Only add metadata if the node exists in the graph\n",
        "                  for col in metadata_df.columns:\n",
        "                      if col != 'ASIN' and pd.notna(row[col]):\n",
        "                          G.nodes[asin][col] = row[col]\n",
        "              #else:  # Optional: Verbose logging for missing nodes.\n",
        "              #    print(f\"Node {asin} not found in graph. Skipping metadata.\")\n",
        "            else:\n",
        "                print(\"Skipping metadata entry with empty or missing ASIN.\")\n",
        "    elif metadata_df is not None and metadata_df.empty:\n",
        "        print(\"Metadata DataFrame is empty, no attributes added.\")\n",
        "    else:\n",
        "      print(\"Metadata is not available so no attributes will be added.\")\n",
        "\n",
        "\n",
        "    print(f\"✅ Created Amazon product graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
        "    return G\n",
        "\n",
        "# --- Usage (Modified for Multiple Datasets) ---\n",
        "\n",
        "# Select the dataset you want to use to create the graph.\n",
        "# For this example, let's use 'amazon0601'.  You'll make this selectable in Gradio later.\n",
        "dataset_name = \"amazon0601\"\n",
        "\n",
        "if dataset_name in datasets and datasets[dataset_name][\"data\"] is not None:\n",
        "    copurchase_df = datasets[dataset_name][\"data\"]\n",
        "    # Only use metadata if it's available *and* we're using amazon0601 (or another copurchase dataset).\n",
        "    #  The metadata is *not* tied to a specific copurchase dataset date.\n",
        "    metadata_df = datasets.get(\"metadata\", {}).get(\"data\") if \"metadata\" in datasets else None\n",
        "\n",
        "    amazon_graph = prepare_amazon_graph(copurchase_df, metadata_df)\n",
        "\n",
        "    # Store the graph in the datasets dictionary for later use.\n",
        "    datasets[dataset_name][\"graph\"] = amazon_graph\n",
        "else:\n",
        "    print(f\"⚠️ Error: Could not create graph. Dataset '{dataset_name}' is missing or has no data.\")\n",
        "    amazon_graph = None  # Set to None to indicate failure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pzEKbBL0WXsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e3b01cd-12d9-4fc1-ed00-2bceca8c5df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing basic graph analysis...\n",
            "Computing degree statistics...\n",
            "Finding most connected products...\n",
            "Finding largest connected component...\n",
            "Creating sample subgraph for detailed analysis...\n",
            "✅ Completed basic graph analysis\n",
            "\n",
            "Amazon Product Network Analysis Results:\n",
            "Total products (nodes): 403,394\n",
            "Total co-purchase links (edges): 3,387,388\n",
            "Average connections per product: 16.79\n",
            "Maximum connections for a product: 2761\n",
            "Largest connected component contains 99.99% of products\n",
            "\n",
            "Top 10 most connected products (potential influencers):\n",
            "1. Product 1041: 2761 connections\n",
            "2. Product 45: 2497 connections\n",
            "3. Product 50: 2291 connections\n",
            "4. Product 529: 1522 connections\n",
            "5. Product 783: 1184 connections\n",
            "6. Product 10030: 866 connections\n",
            "7. Product 89: 813 connections\n",
            "8. Product 1862: 796 connections\n",
            "9. Product 12245: 731 connections\n",
            "10. Product 52: 719 connections\n"
          ]
        }
      ],
      "source": [
        "from typing import Optional  # Import Optional\n",
        "\n",
        "print(\"\\nPerforming basic graph analysis...\")\n",
        "\n",
        "def analyze_graph(G: Optional[nx.DiGraph]) -> Optional[dict]:\n",
        "    \"\"\"Perform basic analysis on the graph, handling None input.\"\"\"\n",
        "    if G is None:\n",
        "        print(\"⚠️ Input graph is None. Cannot perform analysis.\")\n",
        "        return None  # Or return a dictionary with default values\n",
        "\n",
        "    analysis = {}\n",
        "\n",
        "    try:\n",
        "        # Basic statistics\n",
        "        analysis[\"num_nodes\"] = G.number_of_nodes()\n",
        "        analysis[\"num_edges\"] = G.number_of_edges()\n",
        "\n",
        "        # Compute degree statistics\n",
        "        print(\"Computing degree statistics...\")\n",
        "        degrees = [d for n, d in G.degree()]\n",
        "        analysis[\"avg_degree\"] = sum(degrees) / len(degrees) if degrees else 0.0  # Handle empty graph\n",
        "        analysis[\"max_degree\"] = max(degrees) if degrees else 0\n",
        "\n",
        "        # Identify top nodes by degree\n",
        "        print(\"Finding most connected products...\")\n",
        "        degree_dict = dict(G.degree())\n",
        "        top_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "        analysis[\"top_nodes_by_degree\"] = top_nodes\n",
        "\n",
        "        # Extract largest connected component\n",
        "        print(\"Finding largest connected component...\")\n",
        "        try:\n",
        "            largest_cc = max(nx.weakly_connected_components(G), key=len)\n",
        "            analysis[\"largest_cc_size\"] = len(largest_cc)\n",
        "            analysis[\"largest_cc_percentage\"] = (len(largest_cc) / G.number_of_nodes()) * 100\n",
        "        except ValueError: #raised if G is empty\n",
        "            analysis[\"largest_cc_size\"] = 0\n",
        "            analysis[\"largest_cc_percentage\"] = 0.0\n",
        "\n",
        "        # Sample a small subgraph for visualization\n",
        "        print(\"Creating sample subgraph for detailed analysis...\")\n",
        "        if top_nodes: #check if top_nodes is not empty\n",
        "            seed_node = top_nodes[0][0]\n",
        "            sample_nodes = set([seed_node])\n",
        "            frontier = set([seed_node])\n",
        "\n",
        "            while len(sample_nodes) < 100 and frontier:\n",
        "                new_frontier = set()\n",
        "                for node in frontier:\n",
        "                    neighbors = set(G.neighbors(node))\n",
        "                    new_nodes = neighbors - sample_nodes\n",
        "                    sample_nodes.update(list(new_nodes)[:5])\n",
        "                    new_frontier.update(list(new_nodes)[:5])\n",
        "                    if len(sample_nodes) >= 100:\n",
        "                        break\n",
        "                frontier = new_frontier\n",
        "\n",
        "            sample_subgraph = G.subgraph(sample_nodes)\n",
        "            analysis[\"sample_subgraph\"] = sample_subgraph #Keep for visualization\n",
        "            analysis[\"sample_subgraph_size\"] = sample_subgraph.number_of_nodes()\n",
        "        else:\n",
        "            analysis[\"sample_subgraph\"] = None\n",
        "            analysis[\"sample_subgraph_size\"] = 0\n",
        "\n",
        "\n",
        "        print(f\"✅ Completed basic graph analysis\")\n",
        "        return analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error during graph analysis: {e}\")\n",
        "        return None  # Or a dictionary with default/error values\n",
        "\n",
        "\n",
        "# Run the analysis (check if amazon_graph exists)\n",
        "if amazon_graph is not None:\n",
        "    graph_analysis = analyze_graph(amazon_graph)\n",
        "\n",
        "    # Print some findings (only if analysis was successful)\n",
        "    if graph_analysis:\n",
        "        print(\"\\nAmazon Product Network Analysis Results:\")\n",
        "        print(f\"Total products (nodes): {graph_analysis['num_nodes']:,}\")\n",
        "        print(f\"Total co-purchase links (edges): {graph_analysis['num_edges']:,}\")\n",
        "        print(f\"Average connections per product: {graph_analysis['avg_degree']:.2f}\")\n",
        "        print(f\"Maximum connections for a product: {graph_analysis['max_degree']}\")\n",
        "        print(f\"Largest connected component contains {graph_analysis['largest_cc_percentage']:.2f}% of products\")\n",
        "\n",
        "        print(\"\\nTop 10 most connected products (potential influencers):\")\n",
        "        for i, (node, degree) in enumerate(graph_analysis['top_nodes_by_degree'], 1):\n",
        "            print(f\"{i}. Product {node}: {degree} connections\")\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ Graph analysis skipped because the graph could not be created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6Osaq5tbcg1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc4db46-22d1-4c73-b406-a22c428a5309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_cohere in /usr/local/lib/python3.11/dist-packages (0.4.2)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.42.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: cohere<6.0,>=5.12.0 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (5.13.12)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (0.3.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (0.3.40)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (2.10.6)\n",
            "Requirement already satisfied: types-pyyaml<7.0.0.0,>=6.0.12.20240917 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (6.0.12.20241230)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (19.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.27.1)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (2.27.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (0.21.0)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (2.32.0.20241016)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.19)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (2.0.38)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.11.13)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (2.8.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.11)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_cohere) (1.33)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.9.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain_cohere) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain_cohere) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain_cohere) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_cohere) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.6)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain_cohere) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain_cohere) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain_cohere) (2025.2.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain_cohere) (4.67.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain_cohere) (1.3.1)\n",
            "Downloading streamlit-1.42.2-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.12 gitpython-3.1.44 pydeck-0.9.1 smmap-5.0.2 streamlit-1.42.2 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_cohere streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4RyFHpgaWZwe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b029e91c-e6ae-4ff0-8ebd-bac8708dbc73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing community detection...\n",
            "Could not install python-louvain. Will use connected components instead.\n",
            "Graph is large (403394 nodes), sampling for community detection...\n",
            "⚠️ python-louvain (community) not installed. Falling back to connected components.\n",
            "\n",
            "Community Detection Results:\n",
            "Algorithm used: connected_components\n",
            "Number of communities/clusters detected: 1\n",
            "Top 5 community sizes: [104]\n"
          ]
        }
      ],
      "source": [
        "from typing import Optional  # Import Optional\n",
        "\n",
        "print(\"\\nPerforming community detection...\")\n",
        "\n",
        "def detect_communities(G: Optional[nx.DiGraph], graph_analysis: Optional[dict] = None, max_nodes: int = 5000) -> Optional[dict]:\n",
        "    \"\"\"Detect communities, handling None graph and large graphs.\"\"\"\n",
        "\n",
        "    if G is None:\n",
        "        print(\"⚠️ Input graph is None. Cannot detect communities.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        if G.number_of_nodes() > max_nodes:\n",
        "            print(f\"Graph is large ({G.number_of_nodes()} nodes), sampling for community detection...\")\n",
        "            if graph_analysis and \"sample_subgraph\" in graph_analysis and graph_analysis[\"sample_subgraph\"] is not None:\n",
        "                subgraph = graph_analysis[\"sample_subgraph\"]\n",
        "            else:\n",
        "                # Fallback: random sample if no sample_subgraph is available\n",
        "                print(\"Sampling nodes for community detection (no previous sample)...\")\n",
        "                sampled_nodes = list(G.nodes())[:max_nodes]\n",
        "                subgraph = G.subgraph(sampled_nodes)\n",
        "        else:\n",
        "            subgraph = G\n",
        "\n",
        "        undirected_G = subgraph.to_undirected()  # Create undirected version\n",
        "\n",
        "        try:\n",
        "            import community as community_louvain  # Import here\n",
        "            print(\"Running Louvain community detection...\")\n",
        "            partition = community_louvain.best_partition(undirected_G)\n",
        "            communities = {}\n",
        "            for node, community_id in partition.items():\n",
        "                communities.setdefault(community_id, []).append(node)\n",
        "            sorted_communities = sorted(communities.items(), key=lambda x: len(x[1]), reverse=True)\n",
        "\n",
        "            return {\n",
        "                \"algorithm\": \"louvain\",\n",
        "                \"num_communities\": len(communities),\n",
        "                \"community_sizes\": [len(comm) for _, comm in sorted_communities[:10]],\n",
        "                \"top_communities\": sorted_communities[:5],\n",
        "                \"node_communities\": partition\n",
        "            }\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"⚠️ python-louvain (community) not installed. Falling back to connected components.\")\n",
        "            components = list(nx.connected_components(undirected_G))\n",
        "            sorted_components = sorted(components, key=len, reverse=True)\n",
        "            return {\n",
        "                \"algorithm\": \"connected_components\",\n",
        "                \"num_communities\": len(components),\n",
        "                \"community_sizes\": [len(comp) for comp in sorted_components[:10]],\n",
        "                \"top_communities\": [(i, list(comp)) for i, comp in enumerate(sorted_components[:5])],\n",
        "                \"node_communities\": {node: i for i, comp in enumerate(components) for node in comp}\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error during community detection: {e}\")\n",
        "        return None  # Or a dictionary with default/error values\n",
        "\n",
        "\n",
        "# Try to install the community detection library\n",
        "try:\n",
        "    install_package(\"python-louvain\")  # Use the installation function.\n",
        "except:\n",
        "    print(\"Could not install python-louvain. Will use connected components instead.\")\n",
        "\n",
        "\n",
        "# Run community detection (check if amazon_graph exists)\n",
        "if amazon_graph is not None:\n",
        "    community_analysis = detect_communities(amazon_graph, graph_analysis) #now we send the graph analysis\n",
        "\n",
        "    # Print community findings (only if analysis was successful)\n",
        "    if community_analysis:\n",
        "      print(\"\\nCommunity Detection Results:\")\n",
        "      print(f\"Algorithm used: {community_analysis['algorithm']}\")\n",
        "      print(f\"Number of communities/clusters detected: {community_analysis['num_communities']}\")\n",
        "      print(f\"Top 5 community sizes: {community_analysis['community_sizes'][:5]}\")\n",
        "else:\n",
        "    print(\"⚠️ Community detection skipped because the graph could not be created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AybV5ZH2YlWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ec92d0-f677-4276-da9b-fa6f594b7761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking ArangoDB connection...\n",
            "Creating database: arnav\n",
            "✅ Created database: arnav\n",
            "✅ Connected to ArangoDB: arnav\n",
            "Database contains 8 collections\n",
            "Created new graph: amazon0601\n",
            "✅ Created vertex collection: amazon0601_products\n",
            "✅ Created edge definition: amazon0601_copurchase\n",
            "Inserting 403394 nodes in batches of 1000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inserting nodes: 100%|██████████| 404/404 [00:10<00:00, 39.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserting 3387388 edges in batches of 1000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inserting edges: 100%|██████████| 3388/3388 [05:00<00:00, 11.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully persisted Amazon graph 'amazon0601' to ArangoDB\n",
            "✅ ArangoDB persistence complete for dataset: amazon0601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from typing import Optional\n",
        "import time\n",
        "from arango import ArangoClient, ServerConnectionError\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"\\nChecking ArangoDB connection...\")\n",
        "\n",
        "def setup_arangodb(retries=5, delay=5) -> Optional[ArangoClient.db]:\n",
        "    \"\"\"Setup ArangoDB connection safely with retries.\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            client = ArangoClient(hosts=ARANGO_HOST)\n",
        "            # Try connecting with user credentials first\n",
        "            try:\n",
        "                # First check if the database exists in the system database\n",
        "                sys_db = client.db('_system', username=ARANGO_USERNAME, password=ARANGO_PASSWORD)\n",
        "\n",
        "                if not sys_db.has_database(ARANGO_DB):\n",
        "                    print(f\"Creating database: {ARANGO_DB}\")\n",
        "                    sys_db.create_database(ARANGO_DB)\n",
        "                    print(f\"✅ Created database: {ARANGO_DB}\")\n",
        "\n",
        "                # Now connect to the specific database\n",
        "                db = client.db(ARANGO_DB, username=ARANGO_USERNAME, password=ARANGO_PASSWORD)\n",
        "                print(f\"✅ Connected to ArangoDB: {ARANGO_DB}\")\n",
        "                return db\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error while checking/creating database: {e}\")\n",
        "                return None\n",
        "\n",
        "        except ServerConnectionError as e:\n",
        "            print(f\"Attempt {attempt + 1}/{retries} failed: {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                print(f\"Retrying in {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "        except Exception as e:  # Catch other potential exceptions\n",
        "            print(f\"⚠️ Error connecting to ArangoDB: {e}\")\n",
        "            return None\n",
        "\n",
        "    print(f\"❌ Error connecting to ArangoDB after {retries} retries.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def persist_amazon_graph(db: ArangoClient.db, graph: nx.DiGraph, graph_name: str) -> bool:\n",
        "    \"\"\"Save Amazon graph into ArangoDB with a specified graph name.\"\"\"\n",
        "    if db is None or graph is None:\n",
        "        print(\"⚠️ Cannot persist graph: Database connection or graph is None.\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Verify we can perform operations on the database\n",
        "        try:\n",
        "            collections = db.collections()\n",
        "            print(f\"Database contains {len(collections)} collections\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Cannot access database collections: {e}\")\n",
        "            return False\n",
        "\n",
        "        # Use ArangoDB's graph module\n",
        "        if db.has_graph(graph_name):\n",
        "            arango_graph = db.graph(graph_name)\n",
        "            print(f\"Using existing graph: {graph_name}\")\n",
        "        else:\n",
        "            arango_graph = db.create_graph(graph_name)\n",
        "            print(f\"Created new graph: {graph_name}\")\n",
        "\n",
        "        nodes_collection_name = f\"{graph_name}_products\"  # Unique collection names\n",
        "        edges_collection_name = f\"{graph_name}_copurchase\"\n",
        "\n",
        "        # Vertex Collection\n",
        "        if not arango_graph.has_vertex_collection(nodes_collection_name):\n",
        "            products = arango_graph.create_vertex_collection(nodes_collection_name)\n",
        "            print(f\"✅ Created vertex collection: {nodes_collection_name}\")\n",
        "        else:\n",
        "            products = arango_graph.vertex_collection(nodes_collection_name)\n",
        "            print(f\"Using existing vertex collection: {nodes_collection_name}\")\n",
        "\n",
        "        # Edge Definition\n",
        "        if not arango_graph.has_edge_definition(edges_collection_name):\n",
        "            copurchase = arango_graph.create_edge_definition(\n",
        "                edge_collection=edges_collection_name,\n",
        "                from_vertex_collections=[nodes_collection_name],\n",
        "                to_vertex_collections=[nodes_collection_name]\n",
        "            )\n",
        "            print(f\"✅ Created edge definition: {edges_collection_name}\")\n",
        "        else:\n",
        "            copurchase = arango_graph.edge_collection(edges_collection_name)\n",
        "            print(f\"Using existing edge definition: {edges_collection_name}\")\n",
        "\n",
        "        # Insert nodes in batches\n",
        "        batch_size = 1000\n",
        "        nodes_list = list(graph.nodes(data=True))\n",
        "        print(f\"Inserting {len(nodes_list)} nodes in batches of {batch_size}...\")\n",
        "        for i in tqdm(range(0, len(nodes_list), batch_size), desc=\"Inserting nodes\"):\n",
        "            batch = nodes_list[i:i + batch_size]\n",
        "            nodes_batch = [\n",
        "                {\"_key\": str(node).replace('/', '_'), **data} for node, data in batch\n",
        "            ]\n",
        "            products.insert_many(nodes_batch, overwrite_mode='update')\n",
        "\n",
        "        # Insert edges in batches\n",
        "        edges_list = list(graph.edges(data=True))\n",
        "        print(f\"Inserting {len(edges_list)} edges in batches of {batch_size}...\")\n",
        "        for i in tqdm(range(0, len(edges_list), batch_size), desc=\"Inserting edges\"):\n",
        "            batch = edges_list[i:i + batch_size]\n",
        "            edges_batch = [\n",
        "                {\n",
        "                    \"_from\": f\"{nodes_collection_name}/{str(source).replace('/', '_')}\",\n",
        "                    \"_to\": f\"{nodes_collection_name}/{str(target).replace('/', '_')}\",\n",
        "                    **data,  # Include any edge attributes\n",
        "                }\n",
        "                for source, target, data in batch\n",
        "            ]\n",
        "            copurchase.insert_many(edges_batch, overwrite_mode='update')\n",
        "\n",
        "        print(f\"✅ Successfully persisted Amazon graph '{graph_name}' to ArangoDB\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error persisting graph: {e}\")\n",
        "        return False\n",
        "\n",
        "# --- Main Execution ---\n",
        "# Define these variables before running\n",
        "# amazon_graph = ...  # Your NetworkX graph\n",
        "# dataset_name = \"amazon0601\"  # Your dataset name\n",
        "\n",
        "# Get database connection\n",
        "db = setup_arangodb()\n",
        "\n",
        "if db is not None:\n",
        "    # We have a database connection, now try to add the graph if it exists\n",
        "    if 'amazon_graph' in globals() and amazon_graph is not None:\n",
        "        success = persist_amazon_graph(db, amazon_graph, dataset_name)\n",
        "        if success:\n",
        "            print(f\"✅ ArangoDB persistence complete for dataset: {dataset_name}\")\n",
        "        else:\n",
        "            print(f\"❌ ArangoDB persistence failed for dataset: {dataset_name}\")\n",
        "    else:\n",
        "        print(\"❌ ArangoDB persistence skipped: No graph to persist\")\n",
        "else:\n",
        "    print(\"❌ ArangoDB persistence skipped: No database connection.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sgj1VoqIY4cY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7a14d7-cc77-4ce2-947a-0e294cbbf195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Setting up LangChain for graph insights with Cohere...\n",
            "✅ Using Cohere\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-4637ae5d71fc>:43: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  return LLMChain(llm=cohere_llm, prompt=query_template)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the most influential product?\n",
            "Answer: Of the products in the top ten list, '1041' has the highest average degree at 2761. This means it is the most popular and influential product, since it is connected to the most edges in the graph. It is likely to be an important product in the market, generating a high level of interest and conversation among customers. It is generating a lot of buzz, and should be the center of attention in any marketing plan. \n",
            "\n",
            "I can go into more detail on what this figure means for your business, or any other network analysis insights you would like explained in clearer terms. I hope this helps clarify what the maximum degree product means in this case! \n",
            "\n",
            "Let me know if you have any other immediate marketing insights you would like to discuss pertaining to this data, or if you would like any of the other findings elaborated on.\n",
            "\n",
            "Query: How many communities are there?\n",
            "Answer: There is only one community detected in the analysis. The community size is of 104 nodes. In the network analysis, a community is a sub-group of nodes within a graph that are densely connected, meaning they have more edges than average. \n",
            "\n",
            "Is there anything else I can help you with regarding network analysis?\n",
            "\n",
            "Query: What is the structure of the network?\n",
            "Answer: The network analysis reveals a graph with over 403000 nodes and millions of edges. The average degree of the network is relatively high at 16.79, indicating substantial connectivity between nodes. This suggests a dense and interconnected structure to the network.\n",
            "\n",
            "One node, identified as 1041, stands out as it has the highest degree in the network with 2761 edges connected to it. This could indicate that node 1041 is a central entity in the network, potentially hosting important functions or serving as a hub for communication or interactions.\n",
            "\n",
            "The analysis also identifies several other nodes with higher degrees, such as nodes 45, 50, 529, 783, 10030, 89, 1862, and 12245, suggesting that they are also potentially important nodes in the network.\n",
            "\n",
            "The detection of a single community in the network indicates that the nodes can be grouped into a distinct cluster. The community size of 104 suggests that the network consists of a substantial number of nodes with connections mainly confined within this community. The exact meaning of this community will depend on the context of the network and the relationships between nodes within it. \n",
            "\n",
            "Overall, this network structure seems to emphasize connectivity and the importance of certain nodes in facilitating interactions within the broader network.\n",
            "\n",
            "Query: Give me a product recommendation.\n",
            "Answer: Based on the network analysis of the provided graph, I can provide some insights into product recommendations. Here are my findings:\n",
            "\n",
            "1. **Popular Products**: Products with the highest degrees in the graph that indicate their popularity are ('1041', '45', '50', '529', '783', '10030', '89', '1862', '12245', '52'). These products are used frequently and could be considered as popular items in the dataset. \n",
            "\n",
            "2. **Potential Recommendations**: Products with a high degree of connectivity to other popular products could be good candidates for recommendations. For example, product '45' with a degree of 2497 is connected to many other popular products. Recommendations based on products that are frequently bought or searched together with '45' could be an effective strategy. \n",
            "\n",
            "3. **Diversity of Products**: The graph reveals a diverse set of products with varying degrees. From highly popular products like '1041' with a max degree of 2761 to less popular yet still widely used products like '529' and '52'. This diversity suggests a broad range of customer preferences and interests. \n",
            "\n",
            "4. **Network Density**: The average degree of 16.79 indicates relatively dense regions in the graph. This means that many products are connected to each other, forming complex relationships in the dataset. \n",
            "\n",
            "5. **Community Detection**: The detection of a single community with a size of 104 suggests that further segmentation of customers or products into distinct groups could be beneficial for targeted marketing and personalized recommendations. \n",
            "\n",
            "Consider these insights when crafting your product recommendation strategy, such as leveraging popular products and leveraging connections between products to offer personalized recommendations to customers. Further analysis of the graph and exploration of additional features or trends can provide more actionable insights.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from typing import Optional\n",
        "\n",
        "print(\"\\nSetting up LangChain for graph insights with Cohere...\")\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "#from langchain.llms import Cohere  # Deprecated\n",
        "from langchain_cohere import ChatCohere  # Use ChatCohere\n",
        "import os\n",
        "\n",
        "def setup_langchain_cohere(graph_analysis: Optional[dict], community_analysis: Optional[dict]) -> Optional[LLMChain]:\n",
        "    \"\"\"Sets up LangChain with the Cohere API, handling None inputs.\"\"\"\n",
        "\n",
        "    if not COHERE_API_KEY:\n",
        "        print(\"⚠️ COHERE_API_KEY not set. Cannot initialize LLM.\")\n",
        "        return None\n",
        "\n",
        "    if graph_analysis is None or community_analysis is None:\n",
        "        print(\"⚠️ Graph or community analysis is None.  Cannot set up LLMChain.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        print(\"✅ Using Cohere\")\n",
        "        query_template = PromptTemplate(\n",
        "            template=\"\"\"\n",
        "Based on the network analysis:\n",
        "\n",
        "Graph has {num_nodes} nodes and {num_edges} edges.\n",
        "Average degree: {avg_degree:.2f}\n",
        "Max degree: {max_degree}\n",
        "Top Products by Degree: {top_nodes_by_degree}\n",
        "Communities detected: {num_communities}\n",
        "Community Sizes: {community_sizes}\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Answer:\n",
        "\"\"\",\n",
        "            input_variables=[\"query\", \"num_nodes\", \"num_edges\", \"avg_degree\", \"max_degree\",\n",
        "                             \"num_communities\", \"community_sizes\", \"top_nodes_by_degree\"]\n",
        "        )\n",
        "        # Use ChatCohere with the API key\n",
        "        cohere_llm = ChatCohere(cohere_api_key=COHERE_API_KEY, model=\"command\")\n",
        "        return LLMChain(llm=cohere_llm, prompt=query_template)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error setting up LangChain: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def agentic_query(query: str, llm_chain: Optional[LLMChain], graph_analysis: Optional[dict], community_analysis: Optional[dict]) -> str:\n",
        "    \"\"\"Processes queries about the graph using the LLM chain, handling None inputs.\"\"\"\n",
        "\n",
        "    if llm_chain is None:\n",
        "        return \"LLM chain not available.  Check Cohere API key and setup.\"\n",
        "\n",
        "    if graph_analysis is None or community_analysis is None:\n",
        "        return \"Graph or community analysis is missing. Cannot answer query.\"\n",
        "\n",
        "    try:\n",
        "         # Prepare the input dictionary, handling potential missing attributes, and avoiding\n",
        "         # passing the complete sample_subgraph to the prompt\n",
        "        input_data = {\n",
        "            \"query\": query,\n",
        "            \"num_nodes\": graph_analysis.get(\"num_nodes\", 0),  # Use .get() with defaults\n",
        "            \"num_edges\": graph_analysis.get(\"num_edges\", 0),\n",
        "            \"avg_degree\": graph_analysis.get(\"avg_degree\", 0.0),\n",
        "            \"max_degree\": graph_analysis.get(\"max_degree\", 0),\n",
        "            \"top_nodes_by_degree\": graph_analysis.get(\"top_nodes_by_degree\", []),\n",
        "            \"num_communities\": community_analysis.get(\"num_communities\", 0) if community_analysis else 0, #check that community analysis is not None\n",
        "            \"community_sizes\": community_analysis.get(\"community_sizes\", []) if community_analysis else [],\n",
        "        }\n",
        "\n",
        "\n",
        "        result = llm_chain.invoke(input_data)  # Use .invoke()\n",
        "        return result['text']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM query: {e}\")\n",
        "        return f\"An error occurred during the query: {e}\"\n",
        "\n",
        "# --- Example Usage (with checks for None) ---\n",
        "\n",
        "if graph_analysis is not None and community_analysis is not None:\n",
        "    try:\n",
        "        llm_chain = setup_langchain_cohere(graph_analysis, community_analysis)\n",
        "        if llm_chain:\n",
        "            queries = [\n",
        "                \"What is the most influential product?\",\n",
        "                \"How many communities are there?\",\n",
        "                \"What is the structure of the network?\",\n",
        "                \"Give me a product recommendation.\"\n",
        "            ]\n",
        "            for query in queries:\n",
        "                response = agentic_query(query, llm_chain, graph_analysis, community_analysis)\n",
        "                print(f\"Query: {query}\\nAnswer: {response}\\n\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "elif graph_analysis is None:\n",
        "    print(\"⚠️ Graph analysis failed, skipping agentic queries.\")\n",
        "elif community_analysis is None:\n",
        "    print(\"⚠️ Community analysis failed, skipping agentic queries.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MFUiGR35Y8Hq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b2ca23c-399c-40f1-a56c-8cbae6a622ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_cohere\n",
            "  Downloading langchain_cohere-0.4.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting cohere<6.0,>=5.12.0 (from langchain_cohere)\n",
            "  Downloading cohere-5.13.12-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (0.3.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (0.3.40)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (2.10.6)\n",
            "Collecting types-pyyaml<7.0.0.0,>=6.0.12.20240917 (from langchain_cohere)\n",
            "  Downloading types_PyYAML-6.0.12.20241230-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.12.0->langchain_cohere)\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (0.21.0)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.12.0->langchain_cohere)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (4.12.2)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.19)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (2.0.38)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (2.8.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.11)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.26.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_cohere) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_cohere) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_cohere) (0.7.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain_cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain_cohere) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain_cohere) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain_cohere) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain_cohere) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_cohere) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.6)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.12.0->langchain_cohere) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.12.0->langchain_cohere) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain_cohere) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain_cohere) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain_cohere) (2025.2.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.12.0->langchain_cohere) (4.67.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere<6.0,>=5.12.0->langchain_cohere) (1.3.1)\n",
            "Downloading langchain_cohere-0.4.2-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.13.12-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_PyYAML-6.0.12.20241230-py3-none-any.whl (20 kB)\n",
            "Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: types-requests, types-pyyaml, fastavro, cohere, langchain_cohere\n",
            "Successfully installed cohere-5.13.12 fastavro-1.10.0 langchain_cohere-0.4.2 types-pyyaml-6.0.12.20241230 types-requests-2.32.0.20241016\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.11/dist-packages (5.13.12)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.4.0)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.10.6)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.21.0)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.0.20241016)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2025.2.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_cohere\n",
        "import os\n",
        "os.environ[\"COHERE_API_KEY\"] = \"WcXnR3lxNWGwnoJmI2hq8CnCmPfAr8fRbFFacCsT\"\n",
        "!pip install cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "J_WsV7zLY6fU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a4d15c-8fc9-44c4-87dc-09811a21038a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using Cohere\n",
            "\n",
            "Testing agentic queries on Amazon graph...\n",
            "\n",
            "Query: What are the most influential products in the Amazon network?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-8983bb971772>:28: LangChainDeprecationWarning: The class `Cohere` was deprecated in LangChain 0.1.14 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import Cohere``.\n",
            "  cohere_llm = Cohere()\n",
            "<ipython-input-21-8983bb971772>:42: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  return llm_chain.run({\"query\": query, **graph_analysis, **community_analysis})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: The products with the highest degrees are Product 1 with a degree of 10 and Product 2 with a degree of 9. These products likely have significant influence over the network due to their high connectivity and popularity among customers. This could be because of the ratings the products have, their affordability, their uniqueness, or even just a result of the wide range of options available in this specific category on Amazon. \n",
            "\n",
            "It is important to note that the concept of influence is slightly subjective, and ultimate profitability can only be measured by the effectiveness of converting viewing customers into purchasing ones. \n",
            "\n",
            "Query: What insights can we gain from the community structure?\n",
            "Result: The community structure of a graph can provide several insights into its behavior and properties. \n",
            "\n",
            "1. **Identifying Groups** - Community structure can identify distinct groups or clusters within the graph. In the given network analysis, there are five detected communities, suggesting that the graph has five distinct subgroups. \n",
            "\n",
            "2. **Size and Shape** - Examining the size of communities can offer insights into their relative importance or significance within the graph. In this case, the community sizes vary significantly, ranging from 20 to 12 members. This could indicate different levels of activity, influence, or engagement among these groups.\n",
            "\n",
            "3. **Central Nodes** - Communities often have central nodes that act as connectors between different groups. Identifying these highly connected nodes within communities can help understand their pivotal role in information flow and maintaining connectivity within the graph. \n",
            "\n",
            "4. **Topical Interests** - In some cases, communities may represent groups of users with shared interests, preferences, or behaviors. By analyzing the content or relationships within communities, you can gain insights into the topographical focus and specialization of these groups. \n",
            "\n",
            "5. **Information Flow** - The detection of community structure within a graph can elucidate how information spreads and how communication patterns differ between groups. This is particularly useful\n",
            "\n",
            "Query: How can this graph be used for product recommendations?\n",
            "Result: The graph can be used for product recommendations in the following ways: \n",
            "\n",
            "1. Identifying Popular Products: The top products by degree suggest the most popular products which can be a good starting point for recommendations. For instance, if product (1, 10) appears in many recommendations, it could be a good option to recommend to customers. \n",
            "2. Seeding Recommendations: Products that are frequently co-purchased (high edge weight) can be used as seed nodes to seed recommendations. You can recommend the frequently co-purchased products to users who have shown interest in the related product categories or nodes based on the network structure. \n",
            "3. User-Based Recommendations: You could recommend products that are popular within a user's own community. This would help generate more relevant recommendations since users within the same community may have similar preferences. \n",
            "4. Recommendation Strategies: Strategy could be based on the characteristics of the products themselves. If a product has a high degree, it is already popular, so the strategy could focus on promoting it to similar users (based on community). Alternatively, if a product has a low degree, a strategy could focus on increasing its visibility by recommending it to a broader set of users, or in conjunction with other products. \n",
            "\n",
            "These\n",
            "\n",
            "Query: What does the network structure tell us about Amazon's marketplace?\n",
            "Result: The network analysis of Amazon's marketplace indicates a complex and densely connected ecosystem, characterized by a large number of nodes and edges, indicating extensive interactions among entities in the marketplace. \n",
            "\n",
            "The high average degree of 2.50 suggests that, on average, each entity is involved in multiple relationships or transactions, indicating a widespread connectivity pattern among sellers and products. \n",
            "\n",
            "The presence of a small number of entities with a high max degree of 10 indicates potential influencers or influential sellers/products that exert significant influence or demand in the marketplace. \n",
            "\n",
            "The detection of five distinct communities suggests that the marketplace can be segmented into distinct groups of sellers or products with intra-group interactions, which may reflect specialization, collaboration, or shared interests within these communities. \n",
            "\n",
            "The disclosure of the community sizes and the top products by degree provides specific insights into the scale and relative importance of these communities and identifies the most central products in terms of their degree within each community. \n",
            "\n",
            "Overall, this network analysis highlights the complex dynamics and interconnectedness of Amazon's marketplace, revealing insights into the structure, influential players, and potential communities or niches within the larger ecosystem. \n",
            "\n",
            "✅ Amazon SNAP Graph Analysis complete!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import os\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import Cohere\n",
        "\n",
        "def setup_langchain_cohere(graph_analysis, community_analysis):\n",
        "    \"\"\"Sets up LangChain with the Cohere API.\"\"\"\n",
        "    try:\n",
        "        if 'COHERE_API_KEY' in os.environ:\n",
        "            print(\"✅ Using Cohere\")\n",
        "            query_template = PromptTemplate(\n",
        "                template=\"\"\"\n",
        "            Based on the network analysis:\n",
        "\n",
        "            Graph has {num_nodes} nodes and {num_edges} edges\n",
        "            Average degree: {avg_degree:.2f}\n",
        "            Max degree: {max_degree}\n",
        "            Communities detected: {num_communities}\n",
        "            Community Sizes: {community_sizes}\n",
        "            Top Products by Degree: {top_nodes_by_degree}\n",
        "\n",
        "            Query: {query}\n",
        "\n",
        "            Answer:\n",
        "            \"\"\",\n",
        "                input_variables=[\"query\", \"num_nodes\", \"num_edges\", \"avg_degree\", \"max_degree\", \"num_communities\", \"community_sizes\", \"top_nodes_by_degree\"]\n",
        "            )\n",
        "            cohere_llm = Cohere()\n",
        "            return LLMChain(llm=cohere_llm, prompt=query_template)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"COHERE_API_KEY environment variable not set.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error setting up LangChain: {e}\")\n",
        "        return None\n",
        "\n",
        "def agentic_query(query, llm_chain, graph_analysis, community_analysis):\n",
        "    \"\"\"Processes queries about the graph using the LLM chain.\"\"\"\n",
        "    if llm_chain is None:\n",
        "        return \"LLM chain not available. Please check setup.\"\n",
        "    return llm_chain.run({\"query\": query, **graph_analysis, **community_analysis})\n",
        "\n",
        "# Example graph and community analysis data (replace with your actual data)\n",
        "graph_analysis = {\n",
        "    \"num_nodes\": 100,\n",
        "    \"num_edges\": 200,\n",
        "    \"avg_degree\": 2.5,\n",
        "    \"max_degree\": 10,\n",
        "    \"top_nodes_by_degree\": [(1, 10), (2, 9)],\n",
        "}\n",
        "community_analysis = {\n",
        "    \"num_communities\": 5,\n",
        "    \"community_sizes\": [20, 15, 12, 34, 19]\n",
        "}\n",
        "\n",
        "try:\n",
        "  #setup langchain with Cohere\n",
        "  llm_chain = setup_langchain_cohere(graph_analysis, community_analysis)\n",
        "\n",
        "  print(\"\\nTesting agentic queries on Amazon graph...\")\n",
        "  examples = [\n",
        "      \"What are the most influential products in the Amazon network?\",\n",
        "      \"What insights can we gain from the community structure?\",\n",
        "      \"How can this graph be used for product recommendations?\",\n",
        "      \"What does the network structure tell us about Amazon's marketplace?\"\n",
        "  ]\n",
        "\n",
        "  for example in examples:\n",
        "      print(f\"\\nQuery: {example}\")\n",
        "      time.sleep(1)\n",
        "      result = agentic_query(example, llm_chain, graph_analysis, community_analysis) #fixed by providing the other three variables\n",
        "      print(f\"Result: {result}\")\n",
        "\n",
        "  print(\"\\n✅ Amazon SNAP Graph Analysis complete!\")\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bISraOqJZxMW"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import os\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_cohere import ChatCohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mMezTj_RZxvQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eaed221-a611-4fac-d3e2-a1bb35dfafcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ networkx is already installed\n",
            "✅ pandas is already installed\n",
            "✅ gdown is already installed\n",
            "✅ requests is already installed\n",
            "✅ tqdm is already installed\n",
            "✅ streamlit is already installed\n",
            "✅ matplotlib is already installed\n",
            "✅ langchain is already installed\n",
            "Installing all missing packages in one go...\n",
            "✅ Successfully installed all missing packages\n",
            "Failed to import python-arango, there might be a dependency error.\n",
            "Failed to import langchain-openai, there might be a dependency error.\n",
            "Failed to import langchain-community, there might be a dependency error.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "import networkx as nx\n",
        "from arango import ArangoClient\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import pandas as pd\n",
        "import json\n",
        "import gdown\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import gzip\n",
        "import time\n",
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt  # Import matplotlib for visualizations\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# 1. Dependency Installation (Using Jupyter-Friendly Method)\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def install_and_import(package):\n",
        "    installed = True #boolean to store whether the required packages are installed\n",
        "\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "        print(f\"✅ {package} is already installed\")\n",
        "    except ImportError:\n",
        "        try:\n",
        "            print(f\"Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "            print(f\"✅ Successfully installed {package}\")\n",
        "            importlib.import_module(package) # Check if it can be imported after installation\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error installing {package}: {e}\")\n",
        "            print(\"Skipping this package.\")\n",
        "            installed = False\n",
        "\n",
        "    return installed\n",
        "\n",
        "#required packages\n",
        "required_packages = [\"python-arango\", \"networkx\", \"pandas\", \"gdown\", \"requests\", \"tqdm\", \"streamlit\", \"matplotlib\"]\n",
        "langchain_packages = [\"langchain\", \"langchain-openai\", \"langchain-community\"]\n",
        "all_packages = required_packages + langchain_packages\n",
        "installed_all_packages = True\n",
        "missing_packages = []\n",
        "\n",
        "for package in all_packages:\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "        print(f\"✅ {package} is already installed\")\n",
        "    except ImportError:\n",
        "        missing_packages.append(package)\n",
        "        installed_all_packages = False # We don't know the results yet.\n",
        "\n",
        "if not installed_all_packages:\n",
        "    print(\"Installing all missing packages in one go...\")\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + missing_packages)\n",
        "        print(\"✅ Successfully installed all missing packages\")\n",
        "        for package in missing_packages:\n",
        "            try:\n",
        "                 importlib.import_module(package)  # Double check if it can be imported now.\n",
        "            except:\n",
        "                 print(f\"Failed to import {package}, there might be a dependency error.\")\n",
        "                 installed_all_packages = False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error installing packages: {e}\")\n",
        "        print(\"Skipping graph analysis and web interface setup.\")\n",
        "        installed_all_packages = False # Something failed to install so we return false.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DzAq0w1lZ2iW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf980bae-4bb5-42da-fc32-da6143a809ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-26 06:25:04.104 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.208 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-02-26 06:25:04.209 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.209 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.210 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.210 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.210 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.211 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.211 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.212 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.212 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.212 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.213 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.213 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.213 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.214 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.214 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.215 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.215 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.215 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.216 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.216 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.216 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.217 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.217 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.217 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.218 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.218 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.218 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.219 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.219 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.219 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.220 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.220 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.221 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.221 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.221 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.221 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.222 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.222 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.223 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.223 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.223 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.224 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.224 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.224 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.225 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.225 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.225 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.225 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.226 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Louvain algorithm not available, using connected components instead...\n",
            "{'num_nodes': 100, 'num_edges': 100, 'avg_degree': 2.0, 'max_degree': 2, 'top_nodes_by_degree': [(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2), (8, 2), (9, 2)], 'largest_cc_size': 100, 'largest_cc_percentage': 100.0, 'sample_subgraph_nodes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], 'sample_subgraph_size': 100}\n",
            "{'algorithm': 'connected_components', 'num_communities': 1, 'community_sizes': [100], 'top_communities': [(0, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])], 'node_communities': {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0}}\n",
            "✅ Using Cohere\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-26 06:25:04.328 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.329 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.329 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.330 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.330 Session state does not function when running a script without `streamlit run`\n",
            "2025-02-26 06:25:04.331 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 06:25:04.331 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    import streamlit as st\n",
        "    import networkx as nx\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    import time\n",
        "    import os\n",
        "    from langchain import PromptTemplate, LLMChain\n",
        "    from langchain.llms import Cohere\n",
        "\n",
        "    try: # To catch missing variables if any setup issues occur\n",
        "      # Load your datasets and process the graph as previously done\n",
        "      amazon_graph = nx.DiGraph([(i, (i+1) % 100) for i in range(100)])\n",
        "    except Exception as e:\n",
        "      print(f\"Error creating test graph: {e}\")\n",
        "      exit() # Exit due to essential setup failure\n",
        "\n",
        "    from itertools import islice # for graph sampling\n",
        "\n",
        "    # Functions (Copied from previous responses, please note these are just examples)\n",
        "    def analyze_graph(G):\n",
        "        \"\"\"Analyzes a graph and returns various metrics.\"\"\"\n",
        "        analysis = {}\n",
        "        analysis[\"num_nodes\"] = G.number_of_nodes()\n",
        "        analysis[\"num_edges\"] = G.number_of_edges()\n",
        "        degrees = [d for n, d in G.degree()]\n",
        "        analysis[\"avg_degree\"] = sum(degrees) / len(degrees) if degrees else 0\n",
        "        analysis[\"max_degree\"] = max(degrees) if degrees else 0\n",
        "        degree_dict = dict(G.degree())\n",
        "        top_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "        analysis[\"top_nodes_by_degree\"] = top_nodes\n",
        "\n",
        "        # Find largest weakly connected component\n",
        "        connected_components = list(nx.weakly_connected_components(G))\n",
        "        if connected_components:\n",
        "            largest_cc = max(connected_components, key=len)\n",
        "            analysis[\"largest_cc_size\"] = len(largest_cc)\n",
        "            analysis[\"largest_cc_percentage\"] = len(largest_cc) / G.number_of_nodes() * 100\n",
        "        else:\n",
        "            analysis[\"largest_cc_size\"] = 0\n",
        "            analysis[\"largest_cc_percentage\"] = 0\n",
        "\n",
        "        # Sample a small subgraph for visualization and detailed analysis\n",
        "        if top_nodes:\n",
        "            seed_node = top_nodes[0][0]\n",
        "            sample_nodes = set([seed_node])\n",
        "            frontier = set([seed_node])\n",
        "            while len(sample_nodes) < 100 and frontier:\n",
        "                new_frontier = set()\n",
        "                for node in frontier:\n",
        "                    neighbors = set(G.neighbors(node))\n",
        "                    new_nodes = neighbors - sample_nodes\n",
        "                    sample_nodes.update(list(new_nodes)[:5])\n",
        "                    new_frontier.update(list(new_nodes)[:5])\n",
        "                    if len(sample_nodes) >= 100:\n",
        "                        break\n",
        "                frontier = new_frontier\n",
        "            sample_subgraph = G.subgraph(sample_nodes)\n",
        "            analysis[\"sample_subgraph_nodes\"] = list(sample_nodes)  # Store nodes instead of subgraph\n",
        "            analysis[\"sample_subgraph_size\"] = sample_subgraph.number_of_nodes()\n",
        "        else:\n",
        "            analysis[\"sample_subgraph_nodes\"] = []\n",
        "            analysis[\"sample_subgraph_size\"] = 0\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def detect_communities(G, graph_analysis=None, max_nodes=5000):\n",
        "        \"\"\"Detects communities within the graph using Louvain or connected components.\"\"\"\n",
        "        # Handle large graphs by sampling\n",
        "        try: #this could give an error, if no data is there for example\n",
        "            if G.number_of_nodes() > max_nodes:\n",
        "                print(f\"Graph is large ({G.number_of_nodes()} nodes), sampling {max_nodes} nodes for community detection...\")\n",
        "                if graph_analysis and \"sample_subgraph_nodes\" in graph_analysis:\n",
        "                    subgraph = G.subgraph(graph_analysis[\"sample_subgraph_nodes\"])\n",
        "                else:\n",
        "                    # Sample nodes if no sample subgraph is available\n",
        "                    subgraph = G.subgraph(list(G.nodes())[:max_nodes])\n",
        "            else:\n",
        "                subgraph = G\n",
        "\n",
        "            # Convert to undirected for community detection\n",
        "            undirected_G = subgraph.to_undirected()\n",
        "\n",
        "            try:\n",
        "                # Try using Louvain algorithm\n",
        "                import community as community_louvain\n",
        "                partition = community_louvain.best_partition(undirected_G)\n",
        "                communities = {}\n",
        "                for node, community_id in partition.items():\n",
        "                    if community_id not in communities:\n",
        "                        communities[community_id] = []\n",
        "                    communities[community_id].append(node)\n",
        "                sorted_communities = sorted(communities.items(), key=lambda x: len(x[1]), reverse=True)\n",
        "                return {\n",
        "                    \"algorithm\": \"louvain\",\n",
        "                    \"num_communities\": len(communities),\n",
        "                    \"community_sizes\": [len(comm) for _, comm in sorted_communities[:10]],\n",
        "                    \"top_communities\": sorted_communities[:5],\n",
        "                    \"node_communities\": partition,\n",
        "                }\n",
        "            except ImportError:\n",
        "                print(\"Louvain algorithm not available, using connected components instead...\")\n",
        "                # Fallback to connected components\n",
        "                components = list(nx.connected_components(undirected_G))\n",
        "                sorted_components = sorted(components, key=len, reverse=True)\n",
        "                return {\n",
        "                    \"algorithm\": \"connected_components\",\n",
        "                    \"num_communities\": len(components),\n",
        "                    \"community_sizes\": [len(comp) for comp in sorted_components[:10]],\n",
        "                    \"top_communities\": [(i, list(comp)) for i, comp in enumerate(sorted_components[:5])],\n",
        "                    \"node_communities\": {node: i for i, comp in enumerate(components) for node in comp}\n",
        "                }\n",
        "        except:\n",
        "            print(\"Can't do community setup, maybe there is no information in this set?\")\n",
        "            return{}\n",
        "\n",
        "    def setup_langchain_cohere(graph_analysis, community_analysis):\n",
        "        \"\"\"Sets up LangChain with the Cohere API.\"\"\"\n",
        "        try:\n",
        "            # Check if Cohere API key is set\n",
        "            cohere_api_key = os.environ.get('COHERE_API_KEY')\n",
        "            if cohere_api_key:\n",
        "                print(\"✅ Using Cohere\")\n",
        "                query_template = PromptTemplate(\n",
        "                    template=\"\"\"\n",
        "                Based on the network analysis:\n",
        "                Graph has {num_nodes} nodes and {num_edges} edges\n",
        "                Average degree: {avg_degree:.2f}\n",
        "                Max degree: {max_degree}\n",
        "                Communities detected: {num_communities}\n",
        "                Community Sizes: {community_sizes}\n",
        "                Top Products by Degree: {top_nodes_by_degree}\n",
        "                Query: {query}\n",
        "                Answer:\n",
        "                \"\"\",\n",
        "                    input_variables=[\"query\", \"num_nodes\", \"num_edges\", \"avg_degree\", \"max_degree\",\n",
        "                                    \"num_communities\", \"community_sizes\", \"top_nodes_by_degree\"]\n",
        "                )\n",
        "                cohere_llm = Cohere(cohere_api_key=cohere_api_key)\n",
        "                return LLMChain(llm=cohere_llm, prompt=query_template)\n",
        "            else:\n",
        "                # Raise error if API key is not set\n",
        "                raise ValueError(\"COHERE_API_KEY environment variable not set.\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error setting up LangChain: {e}\")\n",
        "            return None\n",
        "\n",
        "    def agentic_query(query, llm_chain, graph_analysis, community_analysis):\n",
        "        \"\"\"Processes queries about the graph using the LLM chain.\"\"\"\n",
        "        if llm_chain is None:\n",
        "            return \"LLM chain not available. Please check setup.\"\n",
        "\n",
        "        # Create a new dictionary with all parameters\n",
        "        params = {\n",
        "            \"query\": query\n",
        "        }\n",
        "        # Add graph analysis parameters\n",
        "        for key, value in graph_analysis.items():\n",
        "            if isinstance(value, (str, int, float, list, tuple, dict)) and key != \"sample_subgraph_nodes\":\n",
        "                params[key] = value\n",
        "\n",
        "        # Add community analysis parameters\n",
        "        for key, value in community_analysis.items():\n",
        "            if isinstance(value, (str, int, float, list, tuple, dict)) and key != \"node_communities\":\n",
        "                params[key] = value\n",
        "\n",
        "        # Run the chain with the prepared parameters\n",
        "        return llm_chain.run(**params)\n",
        "\n",
        "    graph_analysis = analyze_graph(amazon_graph)\n",
        "    community_analysis = detect_communities(amazon_graph, graph_analysis)\n",
        "    print(graph_analysis)\n",
        "    print(community_analysis)\n",
        "    # ----------------------------------------------------------------------------\n",
        "    # 4. Streamlit Application\n",
        "    # ----------------------------------------------------------------------------\n",
        "\n",
        "    st.title(\"Amazon Product Network Analysis\")\n",
        "\n",
        "    st.sidebar.header(\"Graph Statistics\")\n",
        "    st.sidebar.write(f\"Total Products (Nodes): {graph_analysis.get('num_nodes'):,}\")\n",
        "    st.sidebar.write(f\"Total Co-Purchase Links (Edges): {graph_analysis.get('num_edges'):,}\")\n",
        "    st.sidebar.write(f\"Average Connections per Product: {graph_analysis.get('avg_degree'):.2f}\")\n",
        "    st.sidebar.write(f\"Maximum Connections for a Product: {graph_analysis.get('max_degree')}\")\n",
        "    st.sidebar.write(f\"Largest Connected Component: {graph_analysis.get('largest_cc_percentage'):.2f}%\")\n",
        "\n",
        "    st.sidebar.header(\"Community Statistics\")\n",
        "    st.sidebar.write(f\"Number of Communities: {community_analysis.get('num_communities', 'N/A')}\")\n",
        "    st.sidebar.write(\"Top 5 Community Sizes:\")\n",
        "    if \"community_sizes\" in community_analysis:\n",
        "        for i, size in enumerate(community_analysis['community_sizes'][:5]):\n",
        "            st.sidebar.write(f\"{i+1}: \" + str(size))\n",
        "\n",
        "    # Visualization - using matplotlib for simplicity\n",
        "    st.header(\"Graph Visualization\")\n",
        "    st.write(\"Displaying a sample subgraph for visualization\")\n",
        "    if 'sample_subgraph' in graph_analysis and graph_analysis['sample_subgraph']:\n",
        "        fig, ax = plt.subplots()\n",
        "        nx.draw(graph_analysis[\"sample_subgraph\"], with_labels=True, ax=ax)\n",
        "        st.pyplot(fig)  # st.pyplot for matplotlib plots. If using plotly or other library you will use different command.\n",
        "    else:\n",
        "        st.write(\"No sample subgraph available.\")\n",
        "\n",
        "            # LLM-powered Insights Section\n",
        "    st.header(\"LLM-Powered Insights\")\n",
        "\n",
        "    llm_chain = setup_langchain_cohere(graph_analysis, community_analysis)\n",
        "\n",
        "    if llm_chain:\n",
        "        query = st.text_input(\"Enter your query about the Amazon network:\")\n",
        "        if query:\n",
        "            result = agentic_query(query, llm_chain, graph_analysis, community_analysis)\n",
        "            st.write(\"LLM Answer:\", result)\n",
        "    else:\n",
        "        st.error(\"Failed to set up the LLM Chain. Check your API key and settings.\")\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Rb8ITqDIdrBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "243e38b2-9921-48ba-e980-6941a907afca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.18.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2025.2.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.18.0-py3-none-any.whl (62.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.8 ffmpy-0.5.0 gradio-5.18.0 gradio-client-1.7.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.7 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyArango"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDvMXyBdBXdB",
        "outputId": "a10d1404-208b-4894-98be-e1755d89a9c8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyArango\n",
            "  Downloading pyArango-2.1.1.tar.gz (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from pyArango) (2.32.3)\n",
            "Collecting future (from pyArango)\n",
            "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting datetime (from pyArango)\n",
            "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->pyArango) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->pyArango) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->pyArango) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->pyArango) (2025.1.31)\n",
            "Collecting zope.interface (from datetime->pyArango)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datetime->pyArango) (2025.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface->datetime->pyArango) (75.1.0)\n",
            "Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading future-1.0.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyArango\n",
            "  Building wheel for pyArango (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyArango: filename=pyArango-2.1.1-py2.py3-none-any.whl size=53233 sha256=d15268cd63f9f5a7006f12a54ba85c3163465d09321d8f575df9f72c5e1b1ef4\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/7e/13/19e5408e459cb347796f7a51b4394976d8107a1cb68a060892\n",
            "Successfully built pyArango\n",
            "Installing collected packages: zope.interface, future, datetime, pyArango\n",
            "Successfully installed datetime-5.5 future-1.0.0 pyArango-2.1.1 zope.interface-7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "E1rwLFhmdpiJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "outputId": "8715821b-b8b5-4be1-fe59-a1cba093674a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7ef8c0f1f722fadf75.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7ef8c0f1f722fadf75.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import base64\n",
        "import os\n",
        "import requests\n",
        "import gzip\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "@dataclass\n",
        "class GraphAnalysis:\n",
        "    num_nodes: int\n",
        "    num_edges: int\n",
        "    avg_degree: float\n",
        "    max_degree: int\n",
        "    top_nodes_by_degree: List[Tuple[str, int]]\n",
        "    largest_cc_size: int\n",
        "    largest_cc_percentage: float\n",
        "    sample_subgraph: Optional[nx.DiGraph]\n",
        "    sample_subgraph_size: int\n",
        "    image: str\n",
        "\n",
        "def visualize_graph(graph: Optional[nx.DiGraph]) -> str:\n",
        "    \"\"\"Visualizes the graph (or a sample) and returns a base64 encoded image.\"\"\"\n",
        "    if graph is None:\n",
        "        return \"\"\n",
        "\n",
        "    # Always sample a subgraph for visualization to keep it simple and fast\n",
        "    if graph.number_of_nodes() > 100:\n",
        "        top_nodes = sorted(graph.degree(), key=lambda x: x[1], reverse=True)[:10]  #Top 10\n",
        "        seed_node = top_nodes[0][0]\n",
        "        sample_nodes = {seed_node}\n",
        "        frontier = {seed_node}\n",
        "        while len(sample_nodes) < 100 and frontier:\n",
        "            new_frontier = set()\n",
        "            for node in frontier:\n",
        "                neighbors = set(graph.neighbors(node))\n",
        "                new_nodes = (neighbors - sample_nodes)\n",
        "                selected_nodes = list(new_nodes)[:5]  # Limit to 5 neighbors\n",
        "                sample_nodes.update(selected_nodes)\n",
        "                new_frontier.update(selected_nodes)\n",
        "                if len(sample_nodes) >= 100:\n",
        "                    break\n",
        "            frontier = new_frontier\n",
        "        graph = graph.subgraph(list(sample_nodes))\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12, 6))  # Adjust figure size as needed\n",
        "    nx.draw(graph, with_labels=True, font_weight='bold', node_size=400, font_size=9, alpha=0.7) #Keep small\n",
        "    plt.title(\"Generated Graph (Sample)\")  # Clarify it's a sample\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close()\n",
        "    return base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "\n",
        "def analyze_graph(graph: Optional[nx.DiGraph]) -> Optional[GraphAnalysis]:\n",
        "    \"\"\"Analyzes the graph and returns metrics, including a sampled image.\"\"\"\n",
        "    if graph is None:\n",
        "        return None\n",
        "\n",
        "    analysis = {\n",
        "        \"num_nodes\": graph.number_of_nodes(),\n",
        "        \"num_edges\": graph.number_of_edges(),\n",
        "    }\n",
        "    degrees = [d for _, d in graph.degree()]\n",
        "    analysis[\"avg_degree\"] = sum(degrees) / len(degrees) if degrees else 0.0\n",
        "    analysis[\"max_degree\"] = max(degrees) if degrees else 0\n",
        "    top_nodes = sorted(graph.degree(), key=lambda x: x[1], reverse=True)[:10] #Top 10\n",
        "    analysis[\"top_nodes_by_degree\"] = top_nodes\n",
        "\n",
        "    connected_components = list(nx.weakly_connected_components(graph))\n",
        "    if connected_components:\n",
        "        largest_cc = max(connected_components, key=len)\n",
        "        analysis[\"largest_cc_size\"] = len(largest_cc)\n",
        "        analysis[\"largest_cc_percentage\"] = (len(largest_cc) / graph.number_of_nodes()) * 100\n",
        "    else:\n",
        "        analysis[\"largest_cc_size\"] = 0\n",
        "        analysis[\"largest_cc_percentage\"] = 0.0\n",
        "\n",
        "    analysis[\"sample_subgraph\"] = None  # We'll *always* create the sample now\n",
        "    analysis[\"sample_subgraph_size\"] = 0\n",
        "    analysis[\"image\"] = visualize_graph(graph)  # Get sampled image\n",
        "\n",
        "    return GraphAnalysis(**analysis)\n",
        "\n",
        "\n",
        "def download_file(url: str, filename: str) -> Optional[str]:\n",
        "    \"\"\"Downloads a file with a progress bar.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        with open(filename, 'wb') as file, tqdm(\n",
        "            desc=filename, total=total_size, unit='iB', unit_scale=True, unit_divisor=1024\n",
        "        ) as bar:\n",
        "            for data in response.iter_content(1024):\n",
        "                file.write(data)\n",
        "                bar.update(len(data))\n",
        "        print(f\"✅ Downloaded {filename}\")\n",
        "        return filename\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"⚠️ Error downloading {filename}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def parse_amazon_copurchase(gz_file: str) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Parse Amazon co-purchasing network data.\"\"\"\n",
        "    print(f\"Parsing co-purchase network from {gz_file}...\")\n",
        "    edges = []\n",
        "    try:\n",
        "        with gzip.open(gz_file, 'rt', encoding='latin1') as f:\n",
        "            for line in tqdm(f, desc=\"Reading edges\"):\n",
        "                if not line.startswith('#'):\n",
        "                    source, target = line.strip().split()\n",
        "                    edges.append((source, target))\n",
        "        print(f\"✅ Parsed {len(edges)} co-purchase edges\")\n",
        "        df = pd.DataFrame(edges, columns=['source', 'target'])\n",
        "        csv_file = gz_file.replace('.gz', '.csv')\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"✅ Saved to {csv_file}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error parsing co-purchase data: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_graph(copurchase_df: Optional[pd.DataFrame]) -> Optional[nx.DiGraph]:\n",
        "    \"\"\"Loads the graph from DataFrames.\"\"\"\n",
        "    if copurchase_df is None:\n",
        "        print(\"⚠️ Copurchase DataFrame is None.\")\n",
        "        return None\n",
        "    try:\n",
        "        graph = nx.DiGraph()\n",
        "        with tqdm(total=len(copurchase_df), desc=\"Adding edges\") as pbar:\n",
        "            for _, row in copurchase_df.iterrows():\n",
        "                graph.add_edge(str(row['source']), str(row['target']))\n",
        "                pbar.update(1)\n",
        "        print(f\"✅ Created graph: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
        "        return graph\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error loading graph: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_data(max_nodes_to_display: int = 1000) -> Dict[str, str]:\n",
        "    \"\"\"Downloads, parses, and analyzes the graph, returning results for Gradio.\"\"\"\n",
        "\n",
        "    amazon_datasets = {\n",
        "        \"copurchase\": \"http://snap.stanford.edu/data/amazon0601.txt.gz\",\n",
        "    }\n",
        "    data_dir = \"amazon_data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    copurchase_file = os.path.join(data_dir, \"amazon0601.txt.gz\")\n",
        "    copurchase_csv_file = copurchase_file.replace('.gz', '.csv')\n",
        "    if os.path.exists(copurchase_csv_file):\n",
        "        print(\"Using existing copurchase CSV.\")\n",
        "        copurchase_df = pd.read_csv(copurchase_csv_file)\n",
        "    else:\n",
        "        if not os.path.exists(copurchase_file):\n",
        "            download_file(amazon_datasets[\"copurchase\"], copurchase_file)\n",
        "        copurchase_df = parse_amazon_copurchase(copurchase_file)\n",
        "        if copurchase_df is None:\n",
        "            return {\n",
        "                \"graph_summary\": \"Error: Could not load co-purchase data.\",\n",
        "                \"graph_visualization\": \"\",\n",
        "                \"status\": \"Data loading error.\"\n",
        "            }\n",
        "\n",
        "    amazon_graph = load_graph(copurchase_df)\n",
        "    if amazon_graph is None:\n",
        "        return {\n",
        "            \"graph_summary\": \"Error: Could not create graph.\",\n",
        "            \"graph_visualization\": \"\",\n",
        "            \"status\": \"Graph creation error.\"\n",
        "        }\n",
        "\n",
        "    graph_analysis = analyze_graph(amazon_graph)\n",
        "    if graph_analysis is None:\n",
        "        return {\n",
        "            \"graph_summary\": \"Error: Graph analysis failed.\",\n",
        "            \"graph_visualization\": \"\",\n",
        "            \"status\": \"Graph analysis error.\"\n",
        "        }\n",
        "\n",
        "    # Create a concise summary for the text output\n",
        "    summary = (\n",
        "        f\"The graph has {graph_analysis.num_nodes} nodes and {graph_analysis.num_edges} edges.\\n\"\n",
        "        f\"Average degree: {graph_analysis.avg_degree:.2f}, Max degree: {graph_analysis.max_degree}.\\n\"\n",
        "        f\"Largest connected component size: {graph_analysis.largest_cc_size} \"\n",
        "        f\"({graph_analysis.largest_cc_percentage:.2f}% of nodes).\\n\"\n",
        "        f\"Top nodes by degree: {graph_analysis.top_nodes_by_degree[:5]}\" # Top 5\n",
        "\n",
        "    )\n",
        "\n",
        "    # Limit the graph displayed\n",
        "    if graph_analysis.num_nodes > max_nodes_to_display:\n",
        "      summary += f\"\\n\\nDisplaying a sample of up to {max_nodes_to_display} nodes.\"\n",
        "\n",
        "    return {\n",
        "        \"graph_summary\": summary,\n",
        "        \"graph_visualization\": graph_analysis.image,  # Always a sampled/limited image\n",
        "        \"status\": \"Graph analysis complete!\",\n",
        "    }\n",
        "\n",
        "# --- Gradio Interface Setup ---\n",
        "inputs = [\n",
        "    gr.Slider(minimum=100, maximum=10000, value=1000, step=100, label=\"Max Nodes to Display\", key=\"max_nodes_to_display\")\n",
        "]\n",
        "outputs = [\n",
        "    gr.Textbox(label=\"Graph Summary\", key=\"graph_summary\"),\n",
        "    gr.HTML(label=\"Graph Visualization\", key=\"graph_visualization\"),\n",
        "    gr.Textbox(label=\"Status\", key=\"status\"),\n",
        "]\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=process_data,\n",
        "    inputs=inputs,\n",
        "    outputs=outputs,\n",
        "    title=\"Amazon Graph Analysis (Simplified)\",\n",
        "    description=\"Analyzes the Amazon product co-purchasing network and displays a simplified graph.\",\n",
        "    allow_flagging=\"never\",  # Prevent flagging, since we don't have user input\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(debug=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "4K7aWFLWk5g1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84fbb34c-bed1-4650-9a0f-2be63b5cf534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Adding edges:  51%|█████     | 1713262/3387388 [00:54<00:47, 35002.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ijToGmoQlKRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483dd2ba-62e2-47de-9b09-ec69d63a9c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAdding edges:  55%|█████▌    | 1867907/3387388 [00:58<00:43, 34567.47it/s]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/amazon_data\"  # If using Drive\n",
        "# data_dir = \"/content/amazon_data\"  # If NOT using Drive (ephemeral)\n",
        "os.makedirs(data_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "E-_j8ClolaA6"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "COHERE_API_KEY = 'WcXnR3lxNWGwnoJmI2hq8CnCmPfAr8fRbFFacCsT'\n",
        "ARANGO_HOST = 'https://a40b6d186a3a.arangodb.cloud:8529'\n",
        "ARANGO_USERNAME = 'root'\n",
        "ARANGO_PASSWORD = '2eM5Wd4NRTrcnHQt3yfM'\n",
        "ARANGO_DB = 'arnav'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3l4Zxa6CoWQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50975bb3-754e-420c-9e29-672f26758945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Adding edges:  63%|██████▎   | 2126661/3387388 [01:06<00:36, 34128.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.18.0)\n",
            "Requirement already satisfied: langchain_cohere in /usr/local/lib/python3.11/dist-packages (0.4.2)\n",
            "Requirement already satisfied: python-arango in /usr/local/lib/python3.11/dist-packages (8.1.5)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.8)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.7.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.9.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2025.2.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: cohere<6.0,>=5.12.0 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (5.13.12)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (0.3.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (0.3.40)\n",
            "Requirement already satisfied: types-pyyaml<7.0.0.0,>=6.0.12.20240917 in /usr/local/lib/python3.11/dist-packages (from langchain_cohere) (6.0.12.20241230)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from python-arango) (2.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from python-arango) (2.32.3)\n",
            "Requirement already satisfied: requests_toolbelt in /usr/local/lib/python3.11/dist-packages (from python-arango) (1.0.0)\n",
            "Requirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from python-arango) (2.3.0)\n",
            "Requirement already satisfied: setuptools>=42 in /usr/local/lib/python3.11/dist-packages (from python-arango) (75.1.0)\n",
            "Requirement already satisfied: importlib_metadata>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from python-arango) (8.6.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (1.10.0)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (2.27.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (0.21.0)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0,>=5.12.0->langchain_cohere) (2.32.0.20241016)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.7.1->python-arango) (3.21.0)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.19)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (2.8.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.11)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_cohere) (1.33)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->python-arango) (3.4.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Adding edges:  63%|██████▎   | 2133566/3387388 [01:06<00:36, 33976.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_cohere) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.3.6)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (3.1.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_cohere) (1.0.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Adding edges:  64%|██████▍   | 2161399/3387388 [01:07<00:35, 34142.34it/s]"
          ]
        }
      ],
      "source": [
        "!pip install gradio langchain_cohere python-arango"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "SReQ2QX3oRiU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "outputId": "a30d60ac-c687-4ae3-9c01-0a57c0839eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ gradio is already installed\n",
            "✅ networkx is already installed\n",
            "✅ matplotlib is already installed\n",
            "✅ requests is already installed\n",
            "✅ tqdm is already installed\n",
            "✅ pandas is already installed\n",
            "✅ langchain is already installed\n",
            "Installing langchain-cohere...\n",
            "✅ Successfully installed langchain-cohere\n",
            "Installing python-dotenv...\n",
            "✅ Successfully installed python-dotenv\n",
            "Installing python-arango...\n",
            "✅ Successfully installed python-arango\n",
            "✅ community is already installed\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://603f17e28829461900.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://603f17e28829461900.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://a6d30a5cb643c68471.gradio.live\n",
            "Killing tunnel 127.0.0.1:7862 <> https://603f17e28829461900.gradio.live\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import base64\n",
        "import os\n",
        "import requests\n",
        "import gzip\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import pandas as pd\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_cohere import ChatCohere\n",
        "import subprocess\n",
        "import sys\n",
        "import importlib\n",
        "from arango import ArangoClient\n",
        "import time\n",
        "from arango.exceptions import ServerConnectionError, DatabaseCreateError\n",
        "import json\n",
        "\n",
        "# --- Helper Function for Installation ---\n",
        "def install_package(package_name):\n",
        "    \"\"\"Installs a package using pip, handling errors and using --break-system-packages.\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name, \"--break-system-packages\"])\n",
        "        print(f\"✅ Successfully installed {package_name}\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"❌ Failed to install {package_name}.  You may need to install it manually.\")\n",
        "        print(\"   Consider using a virtual environment or 'apt install python3-<package-name>' if available.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Unexpected error installing {package_name}: {e}\")\n",
        "        return False\n",
        "\n",
        "# --- Package Installation ---\n",
        "required_packages = [\"gradio\", \"networkx\", \"matplotlib\", \"requests\", \"tqdm\", \"pandas\",\n",
        "                     \"langchain\", \"langchain-cohere\", \"python-dotenv\", \"python-arango\",\n",
        "                     \"community\"]  # Corrected name\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "        print(f\"✅ {package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        if not install_package(package):\n",
        "            print(f\"Skipping {package} due to installation failure.\")\n",
        "\n",
        "# --- Load API Key from .env ---\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # OR use Colab secrets (recommended)\n",
        "\n",
        "\n",
        "\n",
        "if not COHERE_API_KEY:\n",
        "    print(\"⚠️ WARNING: COHERE_API_KEY not found in environment variables.  LLM features will be disabled.\")\n",
        "\n",
        "# --- Data Classes ---\n",
        "@dataclass\n",
        "class GraphAnalysis:\n",
        "    num_nodes: int\n",
        "    num_edges: int\n",
        "    avg_degree: float\n",
        "    max_degree: int\n",
        "    top_nodes_by_degree: List[Tuple[str, int]]\n",
        "    largest_cc_size: int\n",
        "    largest_cc_percentage: float\n",
        "    sample_subgraph: Optional[nx.DiGraph] = None  # Default to None\n",
        "    sample_subgraph_size: int = 0\n",
        "    image: str = \"\"\n",
        "\n",
        "@dataclass\n",
        "class CommunityAnalysis:\n",
        "  algorithm: str\n",
        "  num_communities: int\n",
        "  community_sizes: List[int]\n",
        "  top_communities: List[Tuple[int, List[str]]]\n",
        "  node_communities: Dict[str, int]\n",
        "\n",
        "# --- ArangoDB Setup and Loading Functions ---\n",
        "\n",
        "def setup_arangodb(retries=5, delay=5):\n",
        "    \"\"\"Sets up the ArangoDB connection and creates the database/collections if needed.\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            client = ArangoClient(hosts=ARANGO_HOST)\n",
        "            db = None\n",
        "\n",
        "            # Try connecting with user credentials first\n",
        "            try:\n",
        "                db = client.db(ARANGO_DB, username=ARANGO_USERNAME, password=ARANGO_PASSWORD)\n",
        "                print(f\"✅ Connected to ArangoDB: {ARANGO_DB}\")\n",
        "            except Exception:\n",
        "                # Fallback to _system as root\n",
        "                print(f\"⚠️ Initial connection to '{ARANGO_DB}' failed.  Trying _system...\")\n",
        "                sys_db = client.db('_system', username=ARANGO_USERNAME, password=ARANGO_PASSWORD)\n",
        "                if not sys_db.has_database(ARANGO_DB):\n",
        "                    sys_db.create_database(ARANGO_DB)\n",
        "                    print(f\"✅ Created database: {ARANGO_DB}\")\n",
        "                db = client.db(ARANGO_DB, username=ARANGO_USERNAME, password=ARANGO_PASSWORD) #connect\n",
        "\n",
        "            # Check for graph and create if it does not exist.\n",
        "            if db is not None:\n",
        "                if not db.has_graph(ARANGO_GRAPH_NAME):\n",
        "                    db.create_graph(ARANGO_GRAPH_NAME)\n",
        "                    print(f\"✅ Created graph: {ARANGO_GRAPH_NAME}\")\n",
        "                return db\n",
        "\n",
        "            return db\n",
        "\n",
        "        except ServerConnectionError as e:\n",
        "            print(f\"Attempt {attempt + 1}/{retries} failed: {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                print(f\"Retrying in {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error connecting to/setting up ArangoDB: {e}\")\n",
        "            return None\n",
        "\n",
        "    print(f\"❌ Error connecting to ArangoDB after multiple retries.\")\n",
        "    return None\n",
        "\n",
        "def persist_graph_to_arangodb(db, graph: nx.DiGraph, graph_name: str):\n",
        "    \"\"\"Persists the NetworkX graph to ArangoDB with a given graph name.\"\"\"\n",
        "\n",
        "    if db is None or graph is None:\n",
        "        print(\"⚠️ Cannot persist graph: Database connection or graph is None.\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        if db.has_graph(graph_name):\n",
        "            arango_graph = db.graph(graph_name)\n",
        "        else:\n",
        "            arango_graph = db.create_graph(graph_name)\n",
        "            print(f\"Using/Created ArangoDB graph: {graph_name}\")\n",
        "\n",
        "        nodes_collection_name = f\"{graph_name}_products\"\n",
        "        edges_collection_name = f\"{graph_name}_copurchase\"\n",
        "\n",
        "        # Vertex Collection\n",
        "        if not arango_graph.has_vertex_collection(nodes_collection_name):\n",
        "            products = arango_graph.create_vertex_collection(nodes_collection_name)\n",
        "            print(f\"✅ Created vertex collection: {nodes_collection_name}\")\n",
        "        else:\n",
        "            products = arango_graph.vertex_collection(nodes_collection_name)\n",
        "\n",
        "        # Edge Definition\n",
        "        if not arango_graph.has_edge_definition(edges_collection_name):\n",
        "            copurchase = arango_graph.create_edge_definition(\n",
        "                edge_collection=edges_collection_name,\n",
        "                from_vertex_collections=[nodes_collection_name],\n",
        "                to_vertex_collections=[nodes_collection_name]\n",
        "            )\n",
        "            print(f\"✅ Created edge definition: {edges_collection_name}\")\n",
        "        else:\n",
        "            copurchase = arango_graph.edge_collection(edges_collection_name)\n",
        "\n",
        "        # Insert nodes in batches\n",
        "        batch_size = 1000\n",
        "        nodes_list = list(graph.nodes(data=True))\n",
        "        print(f\"Inserting {len(nodes_list)} nodes in batches of {batch_size}...\")\n",
        "        for i in tqdm(range(0, len(nodes_list), batch_size), desc=\"Inserting nodes\"):\n",
        "            batch = nodes_list[i:i + batch_size]\n",
        "            nodes_batch = [\n",
        "                {\"_key\": str(node), **data} for node, data in batch\n",
        "            ]\n",
        "            products.insert_many(nodes_batch, overwrite_mode='update')\n",
        "\n",
        "        # Insert edges in batches\n",
        "        edges_list = list(graph.edges(data=True))\n",
        "        print(f\"Inserting {len(edges_list)} edges in batches of {batch_size}...\")\n",
        "        for i in tqdm(range(0, len(edges_list), batch_size), desc=\"Inserting edges\"):\n",
        "            batch = edges_list[i:i + batch_size]\n",
        "            edges_batch = [\n",
        "                {\n",
        "                    \"_from\": f\"{nodes_collection_name}/{str(source).replace('/', '_')}\",\n",
        "                    \"_to\": f\"{nodes_collection_name}/{str(target).replace('/', '_')}\",\n",
        "                    **data,  # Include any edge attributes\n",
        "                }\n",
        "                for source, target, data in batch\n",
        "            ]\n",
        "            copurchase.insert_many(edges_batch, overwrite_mode='update')\n",
        "\n",
        "        print(f\"✅ Graph '{graph_name}' persisted to ArangoDB.\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error persisting graph '{graph_name}' to ArangoDB: {e}\")\n",
        "        return False\n",
        "\n",
        "def load_graph_from_arangodb(db, graph_name: str) -> Optional[nx.DiGraph]:\n",
        "    \"\"\"Loads a graph from ArangoDB into NetworkX.\"\"\"\n",
        "    if db is None:\n",
        "        print(\"⚠️ Cannot load graph: Database connection is None.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        graph = nx.DiGraph(db.graph(graph_name))\n",
        "        print(f\"✅ Graph '{graph_name}' loaded from ArangoDB: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
        "        return graph\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading graph '{graph_name}' from ArangoDB: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Data Loading and Processing Functions ---\n",
        "def download_file(url, filename):\n",
        "    \"\"\"Downloads a file with a progress bar (no changes needed).\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        with open(filename, 'wb') as file, tqdm(\n",
        "            desc=filename, total=total_size, unit='iB', unit_scale=True, unit_divisor=1024\n",
        "        ) as bar:\n",
        "            for data in response.iter_content(1024):\n",
        "                file.write(data)\n",
        "                bar.update(len(data))\n",
        "        print(f\"✅ Downloaded {filename}\")\n",
        "        return filename\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"⚠️ Error downloading {filename}: {e}\")\n",
        "        return None\n",
        "def parse_amazon_metadata(gz_file):\n",
        "    \"\"\"Parse Amazon metadata (using json.loads per line).\"\"\"\n",
        "    print(f\"Parsing metadata from {gz_file}...\")\n",
        "    products = []\n",
        "    try:\n",
        "        with gzip.open(gz_file, 'rt', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(tqdm(f, desc=\"Reading lines\")):\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    try:\n",
        "                        product = json.loads(line)\n",
        "                        products.append(product)\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        print(f\"⚠️ JSONDecodeError: {e} on line {i + 1}: {line}\")\n",
        "                        continue\n",
        "\n",
        "        print(f\"✅ Parsed {len(products)} products\")\n",
        "        df = pd.DataFrame(products)\n",
        "\n",
        "        if 'ASIN' in df.columns:\n",
        "             df['ASIN'] = df['ASIN'].astype(str)\n",
        "        else:\n",
        "            print(\"⚠️ Warning: 'ASIN' column not found in metadata.\")\n",
        "            df['ASIN'] = '' #or other default\n",
        "\n",
        "        csv_file = gz_file.replace('.gz', '.csv')\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"✅ Saved to {csv_file}\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error parsing metadata file: {e}\")\n",
        "        return None\n",
        "def parse_amazon_copurchase(gz_file, max_edges=None):\n",
        "    \"\"\"Parse co-purchase data (with optional edge limit).\"\"\"\n",
        "    print(f\"Parsing co-purchase network from {gz_file}...\")\n",
        "    edges = []\n",
        "    try:\n",
        "        with gzip.open(gz_file, 'rt', encoding='latin1') as f:\n",
        "            for i, line in enumerate(tqdm(f, desc=\"Reading edges\")):\n",
        "                if not line.startswith('#'):\n",
        "                    source, target = line.strip().split()\n",
        "                    edges.append((source, target))\n",
        "                    if max_edges is not None and i >= max_edges:\n",
        "                        break\n",
        "\n",
        "        print(f\"✅ Parsed {len(edges)} co-purchase edges\")\n",
        "        df = pd.DataFrame(edges, columns=['source', 'target'])\n",
        "        csv_file = gz_file.replace('.gz', '.csv')\n",
        "        df.to_csv(csv_file, index=False)\n",
        "        print(f\"✅ Saved to {csv_file}\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error parsing co-purchase network: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_graph(copurchase_df: Optional[pd.DataFrame]) -> Optional[nx.DiGraph]:\n",
        "    \"\"\"Loads the graph from DataFrames.\"\"\"\n",
        "    if copurchase_df is None:\n",
        "        print(\"⚠️ Copurchase DataFrame is None.\")\n",
        "        return None\n",
        "    try:\n",
        "        graph = nx.DiGraph()\n",
        "        with tqdm(total=len(copurchase_df), desc=\"Adding edges\") as pbar:\n",
        "            for _, row in copurchase_df.iterrows():\n",
        "                graph.add_edge(str(row['source']), str(row['target']))\n",
        "                pbar.update(1)\n",
        "        print(f\"✅ Created graph: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
        "        return graph\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error loading graph: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Graph Analysis and Community Detection ---\n",
        "def visualize_graph(graph: Optional[nx.DiGraph]) -> str:\n",
        "    \"\"\"Visualizes the graph (or a sample) and returns a base64 encoded image.\"\"\"\n",
        "    if graph is None:\n",
        "        return \"\"\n",
        "\n",
        "    # Always sample a subgraph for visualization to keep it simple and fast\n",
        "    if graph.number_of_nodes() > 100:\n",
        "        top_nodes = sorted(graph.degree(), key=lambda x: x[1], reverse=True)[:10]  #Top 10\n",
        "        if not top_nodes: # Handle empty graph\n",
        "            return \"\"\n",
        "        seed_node = top_nodes[0][0]\n",
        "        sample_nodes = {seed_node}\n",
        "        frontier = {seed_node}\n",
        "        while len(sample_nodes) < 100 and frontier:\n",
        "            new_frontier = set()\n",
        "            for node in frontier:\n",
        "                neighbors = set(graph.neighbors(node))\n",
        "                new_nodes = (neighbors - sample_nodes)\n",
        "                selected_nodes = list(new_nodes)[:5]  # Limit to 5 neighbors\n",
        "                sample_nodes.update(selected_nodes)\n",
        "                new_frontier.update(selected_nodes)\n",
        "                if len(sample_nodes) >= 100:\n",
        "                    break\n",
        "            frontier = new_frontier\n",
        "        graph = graph.subgraph(list(sample_nodes))\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12, 6))  # Adjust figure size as needed\n",
        "    nx.draw(graph, with_labels=True, font_weight='bold', node_size=400, font_size=9, alpha=0.7) #Keep small\n",
        "    plt.title(\"Generated Graph (Sample)\")  # Clarify it's a sample\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close()\n",
        "    return base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "\n",
        "def analyze_graph(graph: Optional[nx.DiGraph]) -> Optional[GraphAnalysis]:\n",
        "    \"\"\"Analyzes the graph and returns metrics, including a sampled image.\"\"\"\n",
        "    if graph is None:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        analysis = {\n",
        "            \"num_nodes\": graph.number_of_nodes(),\n",
        "            \"num_edges\": graph.number_of_edges(),\n",
        "        }\n",
        "        degrees = [d for _, d in graph.degree()]\n",
        "        analysis[\"avg_degree\"] = sum(degrees) / len(degrees) if degrees else 0.0\n",
        "        analysis[\"max_degree\"] = max(degrees) if degrees else 0\n",
        "        top_nodes = sorted(graph.degree(), key=lambda x: x[1], reverse=True)[:10] #Top 10\n",
        "        analysis[\"top_nodes_by_degree\"] = top_nodes\n",
        "\n",
        "        connected_components = list(nx.weakly_connected_components(graph))\n",
        "        if connected_components:\n",
        "            largest_cc = max(connected_components, key=len)\n",
        "            analysis[\"largest_cc_size\"] = len(largest_cc)\n",
        "            analysis[\"largest_cc_percentage\"] = (len(largest_cc) / graph.number_of_nodes()) * 100\n",
        "        else:\n",
        "            analysis[\"largest_cc_size\"] = 0\n",
        "            analysis[\"largest_cc_percentage\"] = 0.0\n",
        "\n",
        "        analysis[\"sample_subgraph\"] = None  # We'll *always* create the sample now\n",
        "        analysis[\"sample_subgraph_size\"] = 0\n",
        "        analysis[\"image\"] = visualize_graph(graph)  # Get sampled image\n",
        "\n",
        "        return GraphAnalysis(**analysis)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during graph analysis: {e}\")\n",
        "        return None\n",
        "\n",
        "def detect_communities(G: nx.DiGraph, graph_analysis: Optional[GraphAnalysis] = None, max_nodes: int = 5000) -> Optional[CommunityAnalysis]:\n",
        "    \"\"\"Detects communities using Louvain (if available) or falls back to connected components.\"\"\"\n",
        "\n",
        "    if G is None:\n",
        "        print(\"⚠️ Input graph is None. Cannot detect communities.\")\n",
        "        return None\n",
        "    #handle exceptions:\n",
        "    try:\n",
        "        if G.number_of_nodes() > max_nodes:\n",
        "            print(f\"Graph is large ({G.number_of_nodes()} nodes), sampling for community detection...\")\n",
        "            if graph_analysis and graph_analysis.sample_subgraph:\n",
        "                subgraph = graph_analysis.sample_subgraph\n",
        "            else:\n",
        "                # Basic random sampling if we don't have a sample_subgraph\n",
        "                sampled_nodes = list(G.nodes())[:max_nodes]\n",
        "                subgraph = G.subgraph(sampled_nodes)\n",
        "        else:\n",
        "            subgraph = G\n",
        "\n",
        "        undirected_G = subgraph.to_undirected()\n",
        "\n",
        "        try:\n",
        "            # Corrected import and function call\n",
        "            import community as community_louvain\n",
        "            partition = community_louvain.best_partition(undirected_G)\n",
        "            communities: Dict[int, List[str]] = {}\n",
        "            for node, community_id in partition.items():\n",
        "                if community_id not in communities:\n",
        "                    communities[community_id] = []\n",
        "                communities[community_id].append(node)\n",
        "            sorted_communities = sorted(communities.items(), key=lambda x: len(x[1]), reverse=True)\n",
        "\n",
        "            return CommunityAnalysis(\n",
        "                algorithm=\"louvain\",\n",
        "                num_communities=len(communities),\n",
        "                community_sizes=[len(comm) for _, comm in sorted_communities[:10]],\n",
        "                top_communities=sorted_communities[:5],\n",
        "                node_communities=partition\n",
        "            )\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"⚠️ python-louvain (community) not installed. Falling back to connected components.\")\n",
        "            components = list(nx.connected_components(undirected_G))\n",
        "            sorted_components = sorted(components, key=len, reverse=True)\n",
        "            return CommunityAnalysis(\n",
        "                algorithm=\"connected_components\",\n",
        "                num_communities=len(components),\n",
        "                community_sizes=[len(comp) for comp in sorted_components[:10]],\n",
        "                top_communities=[(i, list(comp)) for i, comp in enumerate(sorted_components[:5])],\n",
        "                node_communities={node: i for i, comp in enumerate(components) for node in comp}\n",
        "            )\n",
        "    except Exception as e:\n",
        "        print(f\"Error during community detection: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- LangChain Setup ---\n",
        "\n",
        "def setup_langchain_cohere(graph_analysis: Optional[GraphAnalysis], community_analysis: Optional[CommunityAnalysis]) -> Optional[LLMChain]:\n",
        "    \"\"\"Sets up the LangChain LLMChain with Cohere, handling missing API key.\"\"\"\n",
        "\n",
        "    if not COHERE_API_KEY:\n",
        "        print(\"⚠️ COHERE_API_KEY not set.  Cannot initialize LLM.\")\n",
        "        return None\n",
        "\n",
        "    if graph_analysis is None or community_analysis is None:\n",
        "        print(\"⚠️ Graph or community analysis is None. Cannot set up LLMChain.\")\n",
        "        return None\n",
        "    #handle exceptions\n",
        "    try:\n",
        "        print(\"Setting up LangChain with Cohere...\")\n",
        "        query_template = PromptTemplate(\n",
        "            template=\"\"\"\n",
        "Based on the network analysis:\n",
        "\n",
        "Graph has {num_nodes} nodes and {num_edges} edges.\n",
        "Average degree: {avg_degree:.2f}\n",
        "Max degree: {max_degree}\n",
        "Top Products by Degree: {top_nodes_by_degree}\n",
        "Communities detected: {num_communities}\n",
        "Community Sizes: {community_sizes}\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Answer:\n",
        "\"\"\",\n",
        "            input_variables=[\"query\", \"num_nodes\", \"num_edges\", \"avg_degree\", \"max_degree\",\n",
        "                             \"num_communities\", \"community_sizes\", \"top_nodes_by_degree\"]\n",
        "        )\n",
        "\n",
        "        cohere_llm = ChatCohere(cohere_api_key=COHERE_API_KEY, model=\"command\") # Use ChatCohere\n",
        "        return LLMChain(llm=cohere_llm, prompt=query_template)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error setting up LangChain: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def agentic_query(query: str, llm_chain: Optional[LLMChain], graph_analysis: Optional[GraphAnalysis], community_analysis: Optional[CommunityAnalysis]) -> str:\n",
        "    \"\"\"Processes queries using the LLM chain.\"\"\"\n",
        "\n",
        "    if llm_chain is None:\n",
        "        return \"LLM chain not available.  Check Cohere API key and setup.\"\n",
        "\n",
        "    if graph_analysis is None or community_analysis is None:\n",
        "        return \"Graph or community analysis is missing. Cannot answer query.\"\n",
        "\n",
        "    try:\n",
        "        # Prepare the input dictionary, handling potential missing attributes\n",
        "        input_data = {\n",
        "            \"query\": query,\n",
        "            \"num_nodes\": graph_analysis.num_nodes,\n",
        "            \"num_edges\": graph_analysis.num_edges,\n",
        "            \"avg_degree\": graph_analysis.avg_degree,\n",
        "            \"max_degree\": graph_analysis.max_degree,\n",
        "            \"top_nodes_by_degree\": graph_analysis.top_nodes_by_degree,\n",
        "            \"num_communities\": community_analysis.num_communities,\n",
        "            \"community_sizes\": community_analysis.community_sizes\n",
        "\n",
        "        }\n",
        "\n",
        "        result = llm_chain.invoke(input_data) # Use .invoke() for newer LangChain\n",
        "        return result['text'] # Access the 'text' key for the response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM query: {e}\")\n",
        "        return f\"An error occurred during the query: {e}\"\n",
        "def process_data(max_nodes_to_display: int, dataset_name: str, user_query: str, persist_to_db: bool = False) -> Dict[str, str]:\n",
        "\n",
        "  # --- 1. Data Loading (Prioritize ArangoDB) ---\n",
        "    ARANGO_GRAPH_NAME = dataset_name\n",
        "    data_dir = \"amazon_data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    db = setup_arangodb()  # Connect to ArangoDB *first*\n",
        "    amazon_graph = None\n",
        "\n",
        "    if db:\n",
        "        amazon_graph = load_graph_from_arangodb(db, dataset_name)  # Load directly from DB\n",
        "        if amazon_graph:\n",
        "            print(f\"✅ Loaded graph '{dataset_name}' from ArangoDB\")\n",
        "\n",
        "    if not amazon_graph:  # If loading from ArangoDB failed, load from file\n",
        "        print(f\"Loading or downloading data for dataset: {dataset_name}...\")\n",
        "        if dataset_name == \"metadata\":\n",
        "            gz_file = os.path.join(data_dir, \"metadata.json.gz\")\n",
        "            csv_file = os.path.join(data_dir, \"metadata.csv\")\n",
        "            if not os.path.exists(gz_file):\n",
        "                download_file(amazon_datasets[dataset_name][\"url\"], gz_file)\n",
        "            df = parse_amazon_metadata(gz_file)  # No max_edges for metadata\n",
        "        elif dataset_name in amazon_datasets and amazon_datasets[dataset_name][\"type\"] == \"copurchase\":\n",
        "            gz_file = os.path.join(data_dir, os.path.basename(amazon_datasets[dataset_name][\"url\"]))\n",
        "            csv_file = gz_file.replace('.gz', '.csv')\n",
        "            if not os.path.exists(gz_file):\n",
        "                download_file(amazon_datasets[dataset_name][\"url\"], gz_file)\n",
        "            df = parse_amazon_copurchase(gz_file, max_edges=100000)  # Limit edges for testing\n",
        "        else:\n",
        "            print(f\"⚠️ Unknown dataset: {dataset_name}\")\n",
        "            return {\n",
        "                \"graph_summary\": f\"Error: Unknown dataset '{dataset_name}'.\",\n",
        "                \"graph_visualization\": \"\",\n",
        "                \"llm_response\": \"\",\n",
        "                \"status\": \"Data loading error.\"\n",
        "            }\n",
        "        if df is not None:\n",
        "            datasets[dataset_name] = {\"data\": df, \"graph\": None}\n",
        "            if dataset_name != \"metadata\":\n",
        "                amazon_graph = load_graph(df)\n",
        "                if amazon_graph is not None:\n",
        "                    datasets[dataset_name][\"graph\"] = amazon_graph\n",
        "        else:\n",
        "            datasets[dataset_name] = {\"data\": pd.DataFrame(), \"graph\": None}\n",
        "            print(f\"⚠️ Could not parse copurchase data from gz file. Co-purchase data will be unavailable.\")\n",
        "            return {\n",
        "            \"graph_summary\": f\"Error: Could not load data for '{dataset_name}'.\",\n",
        "            \"graph_visualization\": \"\",\n",
        "            \"llm_response\": \"\",\n",
        "            \"status\": \"Data loading error.\"\n",
        "            }\n",
        "    # --- 2. ArangoDB Persistence (Conditional) ---\n",
        "    if persist_to_db and db and amazon_graph:\n",
        "        persist_graph_to_arangodb(db, amazon_graph, dataset_name)\n",
        "\n",
        "\n",
        "    # --- 3. Graph Analysis ---\n",
        "    graph_analysis = analyze_graph(amazon_graph)\n",
        "    if graph_analysis is None:\n",
        "        return {\n",
        "            \"graph_summary\": \"Error: Graph analysis failed.\",\n",
        "            \"graph_visualization\": \"\",\n",
        "            \"llm_response\": \"\",\n",
        "            \"status\": \"Graph analysis error.\"\n",
        "        }\n",
        "    community_analysis = detect_communities(amazon_graph, graph_analysis)\n",
        "    if community_analysis is None:\n",
        "        return {\n",
        "            \"graph_summary\": \"Error: Community analysis failed.\",\n",
        "            \"graph_visualization\": graph_analysis.image if graph_analysis else \"\",\n",
        "            \"llm_response\": \"\",\n",
        "            \"status\": \"Community detection error.\"\n",
        "        }\n",
        "\n",
        "    # --- 4. LLM Interaction ---\n",
        "    llm_chain = setup_langchain_cohere(graph_analysis, community_analysis)\n",
        "    llm_response = agentic_query(user_query, llm_chain, graph_analysis, community_analysis) if llm_chain and user_query else \"Enter a query to get insights from the LLM.\"\n",
        "\n",
        "    summary = (\n",
        "        f\"The graph has {graph_analysis.num_nodes} nodes and {graph_analysis.num_edges} edges.\\n\"\n",
        "        f\"Average degree: {graph_analysis.avg_degree:.2f}, Max degree: {graph_analysis.max_degree}.\\n\"\n",
        "        f\"Largest connected component size: {graph_analysis.largest_cc_size} \"\n",
        "        f\"({graph_analysis.largest_cc_percentage:.2f}% of nodes).\\n\"\n",
        "        f\"Top nodes by degree: {graph_analysis.top_nodes_by_degree[:5]}\"\n",
        "    )\n",
        "\n",
        "    if graph_analysis.num_nodes > max_nodes_to_display:\n",
        "        summary += f\"\\n\\nDisplaying a sample of up to {max_nodes_to_display} nodes.\"\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"graph_summary\": summary,\n",
        "        \"graph_visualization\": graph_analysis.image,\n",
        "        \"llm_response\": llm_response,\n",
        "        \"status\": \"Graph analysis complete!\",\n",
        "    }\n",
        "# --- Dataset Definitions ---\n",
        "amazon_datasets = {\n",
        "    \"metadata\": {\n",
        "        \"url\": \"http://snap.stanford.edu/data/amazon/productGraph/metadata.json.gz\",\n",
        "        \"type\": \"metadata\"\n",
        "    },\n",
        "    \"amazon0302\": {\n",
        "        \"url\": \"http://snap.stanford.edu/data/amazon0302.txt.gz\",\n",
        "        \"type\": \"copurchase\"\n",
        "    },\n",
        "    \"amazon0312\": {\n",
        "        \"url\": \"http://snap.stanford.edu/data/amazon0312.txt.gz\",\n",
        "        \"type\": \"copurchase\"\n",
        "    },\n",
        "    \"amazon0505\": {\n",
        "        \"url\": \"http://snap.stanford.edu/data/amazon0505.txt.gz\",\n",
        "        \"type\": \"copurchase\"\n",
        "    },\n",
        "    \"amazon0601\": {\n",
        "        \"url\": \"http://snap.stanford.edu/data/amazon0601.txt.gz\",\n",
        "        \"type\": \"copurchase\"\n",
        "    },\n",
        "}\n",
        "\n",
        "# --- Gradio Interface Setup ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize datasets dictionary outside the main function\n",
        "    datasets = {name: {\"data\": None, \"graph\": None} for name in amazon_datasets}\n",
        "\n",
        "    with gr.Blocks() as iface:\n",
        "      with gr.Row():\n",
        "        with gr.Column():\n",
        "          max_nodes_to_display = gr.Slider(minimum=100, maximum=10000, value=1000, step=100, label=\"Max Nodes to Display\")\n",
        "          dataset_dropdown = gr.Dropdown(choices=list(amazon_datasets.keys()), label=\"Select Dataset\", value=\"amazon0601\")\n",
        "          user_query = gr.Textbox(label=\"Ask a question about the graph:\")\n",
        "          persist_to_db_checkbox = gr.Checkbox(label=\"Persist graph to ArangoDB\", value=False)\n",
        "          run_button = gr.Button(\"Analyze and Query\")\n",
        "        with gr.Column():\n",
        "          graph_summary = gr.Textbox(label=\"Graph Summary\")\n",
        "          graph_visualization = gr.Image(label=\"Graph Visualization\")\n",
        "          llm_response = gr.Textbox(label=\"LLM Response\")\n",
        "          status = gr.Textbox(label=\"Status\")\n",
        "\n",
        "      run_button.click(\n",
        "          fn=process_data,\n",
        "          inputs=[max_nodes_to_display, dataset_dropdown, user_query, persist_to_db_checkbox],\n",
        "          outputs=[graph_summary, graph_visualization, llm_response, status]\n",
        "      )\n",
        "    iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit pyngrok"
      ],
      "metadata": {
        "id": "jePexNkdD6hJ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import base64\n",
        "import os\n",
        "import requests\n",
        "import gzip\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import pandas as pd\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_cohere import ChatCohere\n",
        "import subprocess\n",
        "import sys\n",
        "import importlib\n",
        "from arango import ArangoClient\n",
        "import time\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# --- Helper Function for Installation ---\n",
        "def install_package(package_name):\n",
        "    \"\"\"Installs a package using pip.\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name, \"--break-system-packages\"])\n",
        "        print(f\"✅ Successfully installed {package_name}\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"❌ Failed to install {package_name}.  You may need to install it manually.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Unexpected error installing {package_name}: {e}\")\n",
        "        return False\n",
        "\n",
        "# --- Package Installation ---\n",
        "required_packages = [\"streamlit\", \"networkx\", \"matplotlib\", \"requests\", \"tqdm\", \"pandas\",\n",
        "                     \"langchain\", \"langchain-cohere\", \"python-dotenv\", \"python-arango\",\n",
        "                     \"community\"]\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "        print(f\"✅ {package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        install_package(package)\n",
        "\n",
        "\n",
        "if not COHERE_API_KEY:\n",
        "    st.warning(\"COHERE_API_KEY not found. LLM features will be disabled.\")\n",
        "\n",
        "# --- Data Classes ---\n",
        "@dataclass\n",
        "class GraphAnalysis:\n",
        "    num_nodes: int\n",
        "    num_edges: int\n",
        "    avg_degree: float\n",
        "    max_degree: int\n",
        "    top_nodes_by_degree: List[Tuple[str, int]]\n",
        "    largest_cc_size: int\n",
        "    largest_cc_percentage: float\n",
        "    sample_subgraph: Optional[nx.DiGraph] = None\n",
        "    image: str = \"\"\n",
        "\n",
        "@dataclass\n",
        "class CommunityAnalysis:\n",
        "    algorithm: str\n",
        "    num_communities: int\n",
        "    community_sizes: List[int]\n",
        "    top_communities: List[Tuple[int, List[str]]]\n",
        "    node_communities: Dict[str, int]\n",
        "\n",
        "# --- ArangoDB Setup and Loading Functions ---\n",
        "def setup_arangodb(retries=5, delay=5):\n",
        "    \"\"\"Sets up ArangoDB connection and creates database/collections.\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            client = ArangoClient(hosts=ARANGO_HOST)\n",
        "            db = None\n",
        "            try:\n",
        "                db = client.db(ARANGO_DB, username=ARANGO_USERNAME, password=ARANGO_PASSWORD)\n",
        "                st.success(f\"Connected to ArangoDB: {ARANGO_DB}\")\n",
        "            except Exception:\n",
        "                st.warning(f\"Initial connection to '{ARANGO_DB}' failed.  Trying _system...\")\n",
        "                sys_db = client.db('_system', username=ARANGO_USERNAME, password=ARANGO_PASSWORD)\n",
        "                if not sys_db.has_database(ARANGO_DB):\n",
        "                    sys_db.create_database(ARANGO_DB)\n",
        "                    st.success(f\"Created database: {ARANGO_DB}\")\n",
        "                db = client.db(ARANGO_DB, username=ARANGO_USERNAME, password=ARANGO_PASSWORD)\n",
        "\n",
        "            if db and not db.has_graph(ARANGO_GRAPH_NAME):\n",
        "                db.create_graph(ARANGO_GRAPH_NAME)\n",
        "                st.success(f\"Created graph: {ARANGO_GRAPH_NAME}\")\n",
        "            return db\n",
        "\n",
        "        except ServerConnectionError as e:\n",
        "            st.error(f\"Attempt {attempt + 1}/{retries} failed: {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                time.sleep(delay)\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error connecting to/setting up ArangoDB: {e}\")\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def persist_graph_to_arangodb(db, graph: nx.DiGraph, graph_name: str):\n",
        "    \"\"\"Persists the NetworkX graph to ArangoDB.\"\"\"\n",
        "    if not db or not graph:\n",
        "        st.warning(\"Cannot persist graph: Database/graph is None.\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        arango_graph = db.graph(graph_name) if db.has_graph(graph_name) else db.create_graph(graph_name)\n",
        "        nodes_collection_name = f\"{graph_name}_products\"\n",
        "        edges_collection_name = f\"{graph_name}_copurchase\"\n",
        "\n",
        "        products = arango_graph.create_vertex_collection(nodes_collection_name) if not arango_graph.has_vertex_collection(nodes_collection_name) else arango_graph.vertex_collection(nodes_collection_name)\n",
        "        copurchase = arango_graph.create_edge_definition(\n",
        "            edge_collection=edges_collection_name,\n",
        "            from_vertex_collections=[nodes_collection_name],\n",
        "            to_vertex_collections=[nodes_collection_name]\n",
        "        ) if not arango_graph.has_edge_definition(edges_collection_name) else arango_graph.edge_collection(edges_collection_name)\n",
        "\n",
        "        batch_size = 1000\n",
        "        nodes_list = list(graph.nodes(data=True))\n",
        "        with st.spinner(\"Inserting nodes...\"):\n",
        "          for i in tqdm(range(0, len(nodes_list), batch_size), desc=\"Inserting nodes\"):\n",
        "            batch = nodes_list[i:i + batch_size]\n",
        "            products.insert_many([{\"_key\": str(node), **data} for node, data in batch], overwrite_mode='update')\n",
        "\n",
        "        edges_list = list(graph.edges(data=True))\n",
        "        with st.spinner(\"Inserting edges...\"):\n",
        "          for i in tqdm(range(0, len(edges_list), batch_size), desc=\"Inserting edges\"):\n",
        "            batch = edges_list[i:i + batch_size]\n",
        "            edges_batch = [\n",
        "                {\"_from\": f\"{nodes_collection_name}/{str(source)}\", \"_to\": f\"{nodes_collection_name}/{str(target)}\", **data}\n",
        "                for source, target, data in batch\n",
        "            ]\n",
        "            copurchase.insert_many(edges_batch, overwrite_mode='update')\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error persisting graph: {e}\")\n",
        "        return False\n",
        "\n",
        "def load_graph_from_arangodb(db, graph_name: str) -> Optional[nx.DiGraph]:\n",
        "    \"\"\"Loads a graph from ArangoDB.\"\"\"\n",
        "    if not db:\n",
        "        st.warning(\"Cannot load graph: Database is None.\")\n",
        "        return None\n",
        "    try:\n",
        "        graph = nx.DiGraph(db.graph(graph_name))\n",
        "        st.success(f\"Graph '{graph_name}' loaded: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
        "        return graph\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading graph: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Data Loading and Processing ---\n",
        "def download_file(url, filename):\n",
        "    \"\"\"Downloads a file with a progress bar.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "        with open(filename, 'wb') as file, st.spinner(f\"Downloading {filename}...\"):\n",
        "            with tqdm(desc=filename, total=total_size, unit='iB', unit_scale=True, unit_divisor=1024) as bar:\n",
        "                for data in response.iter_content(1024):\n",
        "                    file.write(data)\n",
        "                    bar.update(len(data))\n",
        "        return filename\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        st.error(f\"Error downloading: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_amazon_metadata(gz_file):\n",
        "    \"\"\"Parses Amazon metadata from a gzipped JSON file.\"\"\"\n",
        "    products = []\n",
        "    try:\n",
        "        with gzip.open(gz_file, 'rt', encoding='utf-8') as f:\n",
        "          with st.spinner(\"Reading Metadata...\"):\n",
        "            for line in tqdm(f, desc=\"Reading metadata\"):\n",
        "                if line.strip():\n",
        "                    try:\n",
        "                        products.append(json.loads(line.strip()))\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        st.warning(f\"JSONDecodeError: {e} in line: {line.strip()}\")\n",
        "        df = pd.DataFrame(products)\n",
        "        df['ASIN'] = df['ASIN'].astype(str)\n",
        "        return df.to_csv(gz_file.replace('.gz', '.csv'), index=False)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error parsing metadata: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_amazon_copurchase(gz_file, max_edges=None):\n",
        "    \"\"\"Parses Amazon co-purchase data.\"\"\"\n",
        "    edges = []\n",
        "    try:\n",
        "        with gzip.open(gz_file, 'rt', encoding='latin1') as f:\n",
        "            with st.spinner(\"Reading Copurchase data...\"):\n",
        "              for i, line in enumerate(tqdm(f, desc=\"Reading copurchase\")):\n",
        "                if not line.startswith('#'):\n",
        "                    source, target = line.strip().split()\n",
        "                    edges.append((source, target))\n",
        "                    if max_edges and i >= max_edges:\n",
        "                        break\n",
        "        df = pd.DataFrame(edges, columns=['source', 'target'])\n",
        "        return df.to_csv(gz_file.replace('.gz', '.csv'), index=False)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error parsing copurchase: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_graph(copurchase_file: str) -> Optional[nx.DiGraph]:\n",
        "    \"\"\"Loads graph from copurchase CSV.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(copurchase_file)\n",
        "        graph = nx.DiGraph()\n",
        "        with st.spinner(\"Creating Graph...\"):\n",
        "          for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Adding edges\"):\n",
        "            graph.add_edge(str(row['source']), str(row['target']))\n",
        "        st.success(f\"Graph created: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
        "        return graph\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading graph: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Graph Analysis and Community Detection ---\n",
        "def visualize_graph(graph: Optional[nx.DiGraph]) -> str:\n",
        "    \"\"\"Visualizes a sample of the graph.\"\"\"\n",
        "    if not graph: return \"\"\n",
        "    if graph.number_of_nodes() > 100:\n",
        "        top_nodes = sorted(graph.degree(), key=lambda x: x[1], reverse=True)[:10]\n",
        "        if not top_nodes: return \"\"\n",
        "        seed_node = top_nodes[0][0]\n",
        "        sample_nodes, frontier = {seed_node}, {seed_node}\n",
        "        while len(sample_nodes) < 100 and frontier:\n",
        "            new_frontier = set()\n",
        "            for node in frontier:\n",
        "                neighbors = set(graph.neighbors(node)) - sample_nodes\n",
        "                selected = list(neighbors)[:5]\n",
        "                sample_nodes.update(selected)\n",
        "                new_frontier.update(selected)\n",
        "            frontier = new_frontier\n",
        "        graph = graph.subgraph(list(sample_nodes))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    nx.draw(graph, with_labels=True, node_size=400, font_size=9, alpha=0.7)\n",
        "    plt.title(\"Graph Sample\")\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close()\n",
        "    return base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "\n",
        "def analyze_graph(graph: Optional[nx.DiGraph]) -> Optional[GraphAnalysis]:\n",
        "    \"\"\"Analyzes the graph.\"\"\"\n",
        "    if not graph: return None\n",
        "    try:\n",
        "        degrees = [d for _, d in graph.degree()]\n",
        "        analysis = {\n",
        "            \"num_nodes\": graph.number_of_nodes(),\n",
        "            \"num_edges\": graph.number_of_edges(),\n",
        "            \"avg_degree\": sum(degrees) / len(degrees) if degrees else 0.0,\n",
        "            \"max_degree\": max(degrees) if degrees else 0,\n",
        "            \"top_nodes_by_degree\": sorted(graph.degree(), key=lambda x: x[1], reverse=True)[:10],\n",
        "            \"largest_cc_size\": 0,\n",
        "            \"largest_cc_percentage\": 0.0,\n",
        "            \"image\": visualize_graph(graph)\n",
        "        }\n",
        "        if cc := list(nx.weakly_connected_components(graph)):\n",
        "            largest_cc = max(cc, key=len)\n",
        "            analysis[\"largest_cc_size\"] = len(largest_cc)\n",
        "            analysis[\"largest_cc_percentage\"] = (len(largest_cc) / graph.number_of_nodes()) * 100\n",
        "        return GraphAnalysis(**analysis)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error during graph analysis: {e}\")\n",
        "        return None\n",
        "\n",
        "def detect_communities(G: nx.DiGraph, max_nodes: int = 5000) -> Optional[CommunityAnalysis]:\n",
        "    \"\"\"Detects communities (Louvain or connected components).\"\"\"\n",
        "    if not G: return None\n",
        "    try:\n",
        "        subgraph = G.subgraph(list(G.nodes())[:max_nodes]) if G.number_of_nodes() > max_nodes else G\n",
        "        undirected_G = subgraph.to_undirected()\n",
        "\n",
        "        try:\n",
        "            import community as community_louvain\n",
        "            partition = community_louvain.best_partition(undirected_G)\n",
        "            communities = {}\n",
        "            for node, cid in partition.items():\n",
        "                communities.setdefault(cid, []).append(node)\n",
        "            sorted_communities = sorted(communities.items(), key=lambda x: len(x[1]), reverse=True)\n",
        "            return CommunityAnalysis(\n",
        "                algorithm=\"louvain\", num_communities=len(communities),\n",
        "                community_sizes=[len(c) for _, c in sorted_communities[:10]],\n",
        "                top_communities=sorted_communities[:5], node_communities=partition\n",
        "            )\n",
        "        except ImportError:\n",
        "            st.warning(\"python-louvain not installed. Using connected components.\")\n",
        "            components = list(nx.connected_components(undirected_G))\n",
        "            sorted_components = sorted(components, key=len, reverse=True)\n",
        "            return CommunityAnalysis(\n",
        "                algorithm=\"connected_components\", num_communities=len(components),\n",
        "                community_sizes=[len(c) for c in sorted_components[:10]],\n",
        "                top_communities=[(i, list(c)) for i, c in enumerate(sorted_components[:5])],\n",
        "                node_communities={n: i for i, comp in enumerate(components) for n in comp}\n",
        "            )\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error during community detection: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- LangChain Setup ---\n",
        "def setup_langchain_cohere(graph_analysis: Optional[GraphAnalysis], community_analysis: Optional[CommunityAnalysis]) -> Optional[LLMChain]:\n",
        "    \"\"\"Sets up LangChain with Cohere.\"\"\"\n",
        "    if not COHERE_API_KEY or not graph_analysis or not community_analysis:\n",
        "        st.warning(\"Cannot initialize LLM: Missing key or analysis.\")\n",
        "        return None\n",
        "    try:\n",
        "        template = \"\"\"Based on the network analysis:\n",
        "Graph: {num_nodes} nodes, {num_edges} edges. Avg degree: {avg_degree:.2f}, Max: {max_degree}.\n",
        "Top nodes: {top_nodes_by_degree}. Communities: {num_communities}, Sizes: {community_sizes}.\n",
        "Query: {query}  Answer:\"\"\"\n",
        "        prompt = PromptTemplate(template=template, input_variables=[\"query\", \"num_nodes\", \"num_edges\", \"avg_degree\",\n",
        "                                                                \"max_degree\", \"num_communities\", \"community_sizes\",\n",
        "                                                                \"top_nodes_by_degree\"])\n",
        "        return LLMChain(llm=ChatCohere(cohere_api_key=COHERE_API_KEY, model=\"command\"), prompt=prompt)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error setting up LangChain: {e}\")\n",
        "        return None\n",
        "\n",
        "def agentic_query(query: str, llm_chain: Optional[LLMChain], graph_analysis: Optional[GraphAnalysis], community_analysis: Optional[CommunityAnalysis]) -> str:\n",
        "    \"\"\"Queries the LLM.\"\"\"\n",
        "    if not llm_chain: return \"LLM not available.\"\n",
        "    if not graph_analysis or not community_analysis: return \"Analysis missing.\"\n",
        "    try:\n",
        "        input_data = {\n",
        "            \"query\": query, \"num_nodes\": graph_analysis.num_nodes, \"num_edges\": graph_analysis.num_edges,\n",
        "            \"avg_degree\": graph_analysis.avg_degree, \"max_degree\": graph_analysis.max_degree,\n",
        "            \"top_nodes_by_degree\": graph_analysis.top_nodes_by_degree,\n",
        "            \"num_communities\": community_analysis.num_communities, \"community_sizes\": community_analysis.community_sizes\n",
        "        }\n",
        "        return llm_chain.invoke(input_data)['text']\n",
        "    except Exception as e:\n",
        "        return f\"Error during query: {e}\"\n",
        "\n",
        "# --- Data Processing and Streamlit Interface ---\n",
        "def process_data(max_nodes_to_display: int, dataset_name: str, user_query: str, persist_to_db: bool) -> Dict[str, str]:\n",
        "    \"\"\"Main data processing function.\"\"\"\n",
        "    data_dir = \"amazon_data\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    db = setup_arangodb()\n",
        "    amazon_graph = load_graph_from_arangodb(db, dataset_name) if db else None\n",
        "\n",
        "    if not amazon_graph:\n",
        "        if dataset_name == \"metadata\":\n",
        "            gz_file = os.path.join(data_dir, \"metadata.json.gz\")\n",
        "            csv_file = os.path.join(data_dir, \"metadata.csv\")\n",
        "            download_file(amazon_datasets[dataset_name][\"url\"], gz_file) if not os.path.exists(gz_file) else None\n",
        "            parse_amazon_metadata(gz_file)\n",
        "        elif dataset_name in amazon_datasets:\n",
        "            gz_file = os.path.join(data_dir, os.path.basename(amazon_datasets[dataset_name][\"url\"]))\n",
        "            csv_file = gz_file.replace('.gz', '.csv')\n",
        "            download_file(amazon_datasets[dataset_name][\"url\"], gz_file) if not os.path.exists(gz_file) else None\n",
        "            parse_amazon_copurchase(gz_file, max_edges=100000)\n",
        "            amazon_graph = load_graph(csv_file)\n",
        "        else:\n",
        "            return {\"graph_summary\": f\"Error: Unknown dataset '{dataset_name}'.\", \"image\": \"\", \"llm_response\": \"\", \"status\": \"Data loading error.\"}\n",
        "    if persist_to_db and db and amazon_graph:\n",
        "        persist_graph_to_arangodb(db, amazon_graph, dataset_name)\n",
        "\n",
        "    graph_analysis = analyze_graph(amazon_graph)\n",
        "    if not graph_analysis:\n",
        "        return {\"graph_summary\": \"Error: Graph analysis failed.\", \"image\": \"\", \"llm_response\": \"\", \"status\": \"Analysis error.\"}\n",
        "    community_analysis = detect_communities(amazon_graph, graph_analysis)\n",
        "    if not community_analysis:\n",
        "         return {\"graph_summary\": \"Error: Community analysis failed.\", \"image\": graph_analysis.image, \"llm_response\": \"\", \"status\": \"Analysis error.\"}\n",
        "\n",
        "    llm_chain = setup_langchain_cohere(graph_analysis, community_analysis)\n",
        "    llm_response = agentic_query(user_query, llm_chain, graph_analysis, community_analysis) if llm_chain and user_query else \"Enter a query.\"\n",
        "    summary = (f\"Nodes: {graph_analysis.num_nodes}, Edges: {graph_analysis.num_edges}.  Avg degree: {graph_analysis.avg_degree:.2f}, \"\n",
        "               f\"Max degree: {graph_analysis.max_degree}. Largest CC: {graph_analysis.largest_cc_size} ({graph_analysis.largest_cc_percentage:.2f}%). \"\n",
        "               f\"Top nodes: {graph_analysis.top_nodes_by_degree[:5]}\")\n",
        "    if graph_analysis.num_nodes > max_nodes_to_display:\n",
        "        summary += f\"\\nDisplaying sample (up to {max_nodes_to_display} nodes).\"\n",
        "\n",
        "    return {\"graph_summary\": summary, \"image\": graph_analysis.image, \"llm_response\": llm_response, \"status\": \"Complete!\"}\n",
        "\n",
        "amazon_datasets = {\n",
        "    \"metadata\": {\"url\": \"http://snap.stanford.edu/data/amazon/productGraph/metadata.json.gz\", \"type\": \"metadata\"},\n",
        "    \"amazon0302\": {\"url\": \"http://snap.stanford.edu/data/amazon0302.txt.gz\", \"type\": \"copurchase\"},\n",
        "    \"amazon0312\": {\"url\": \"http://snap.stanford.edu/data/amazon0312.txt.gz\", \"type\": \"copurchase\"},\n",
        "    \"amazon0505\": {\"url\": \"http://snap.stanford.edu/data/amazon0505.txt.gz\", \"type\": \"copurchase\"},\n",
        "    \"amazon0601\": {\"url\": \"http://snap.stanford.edu/data/amazon0601.txt.gz\", \"type\": \"copurchase\"},\n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    st.set_page_config(layout=\"wide\")  # Use wide layout\n",
        "    st.title(\"Amazon Co-purchase Network Analysis\")\n",
        "\n",
        "    with st.sidebar:\n",
        "        st.header(\"Settings\")\n",
        "        max_nodes_to_display = st.slider(\"Max Nodes to Display\", 100, 10000, value=1000, step=100)\n",
        "        dataset_name = st.selectbox(\"Select Dataset\", list(amazon_datasets.keys()), index=list(amazon_datasets.keys()).index(\"amazon0601\"))  # Correct default selection\n",
        "        user_query = st.text_input(\"Ask a question about the graph:\")\n",
        "        persist_to_db = st.checkbox(\"Persist graph to ArangoDB\", value=False)\n",
        "        if st.button(\"Analyze\"):\n",
        "            with st.spinner(\"Analyzing...\"):\n",
        "                results = process_data(max_nodes_to_display, dataset_name, user_query, persist_to_db)\n",
        "            st.session_state['results'] = results  # Store results in session state\n",
        "\n",
        "    # Display results (if available)\n",
        "    if 'results' in st.session_state:\n",
        "        results = st.session_state['results']\n",
        "        col1, col2 = st.columns(2)  # Two columns for better layout\n",
        "\n",
        "        with col1:\n",
        "            st.subheader(\"Graph Summary\")\n",
        "            st.text(results['graph_summary'])\n",
        "\n",
        "            st.subheader(\"LLM Response\")\n",
        "            st.text(results['llm_response'])\n",
        "            st.write(f\"Status: {results['status']}\")\n",
        "\n",
        "\n",
        "        with col2:\n",
        "            st.subheader(\"Graph Visualization\")\n",
        "            st.image(\"data:image/png;base64,\" + results['image'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwDq0EO4Dklc",
        "outputId": "5c8504ef-b891-41cd-9969-a5df71921b10"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ streamlit is already installed\n",
            "✅ networkx is already installed\n",
            "✅ matplotlib is already installed\n",
            "✅ requests is already installed\n",
            "✅ tqdm is already installed\n",
            "✅ pandas is already installed\n",
            "✅ langchain is already installed\n",
            "Installing langchain-cohere...\n",
            "✅ Successfully installed langchain-cohere\n",
            "Installing python-dotenv...\n",
            "✅ Successfully installed python-dotenv\n",
            "Installing python-arango...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-02-26 07:00:52.502 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.503 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.503 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.504 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.504 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.504 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.505 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.505 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.506 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.506 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.507 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.507 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.507 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.508 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.508 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.509 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.509 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.509 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.510 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.510 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.512 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.513 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.514 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.514 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.515 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.515 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.515 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.516 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.518 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.518 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.519 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.519 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-26 07:00:52.519 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully installed python-arango\n",
            "✅ community is already installed\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}